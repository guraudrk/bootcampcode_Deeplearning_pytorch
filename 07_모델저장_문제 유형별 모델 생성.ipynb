{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이진 분류나, 다중 분류나, 상황에 따라 쓰는 함수들이 다르다.\n",
    "\n",
    "#다중분류는 CrossEntropyLoss, NLLLoss, LogSoftmax 등이 있다.\n",
    "#CrossEntropyLoss: NLLLoss + LogSoftmax\n",
    "#NLLLoss => 정답에 원핫 인코딩 후 cross entropy 계산.\n",
    "#LogSoftmax => 모델 추정 결과에 softmax를 처리한 뒤 cross entropy를 계산한다.\n",
    "#다중 분류는 class의 수가 얼마나 되는지 파악해서, 그것을 토대로 처리를 하면 된다잉.\n",
    "\n",
    "#이진 분류\n",
    "# output layer의 출력 unit 개수가 1개이다.(positive일 확률을 구하는 경우가 높다.)\n",
    "# 출력의 activation 함수: Sigmoid(Logistic) 함수\n",
    "# loss 함수 : Binary crossentropy(BCELoss)\n",
    "# MSE,RMSE, R square(평균으로 예측하는 것 대비 얼마나 성능이 좋은지에 관한 점수.)\n",
    "\n",
    "\n",
    "#분류의 평가지표(각자가 약간은 미묘하게 다르다. 그러니 이 정의를 잘 파악하도록 하자.)\n",
    "#정확도(Accuracy)   - 맞은 것의 개수 / 전체개수\n",
    "#재현율/민감도(Recall) - positive중에 맞은 것의 개수 / positive 의 개수 (이는 이진분류평가지표이다.) \n",
    "#정밀도(Precision) - 모델이 positive로 예측한 것 중 맞은 것의 개수 / 모델이 positive로 예측한 전체의 개수\n",
    "#재현율과 정밀도의 차이는 '예측'의 차이라고 생각을 할 수 있다.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 저장\n",
    "\n",
    "- 학습한 모델을 저장장치에 파일로 저장하고 나중에 불러와 사용(추가 학습, 예측 서비스) 할 수 있도록 한다. \n",
    "- 파이토치는 모델의 파라미터만 저장하는 방법과 모델 구조와 파라미터 모두를 저장하는 두가지 방식을 제공한다.\n",
    "- 저장 함수\n",
    "    - `torch.save(저장할 객체, 저장경로)`\n",
    "- 보통 저장파일의 확장자는 `pt`나 `pth` 를 지정한다.\n",
    "\n",
    "## 모델 전체 저장하기 및 불러오기\n",
    "\n",
    "- 저장하기\n",
    "    - `torch.save(model, 저장경로)`\n",
    "- 불러오기\n",
    "    - `load_model = torch.load(저장경로)`\n",
    "- 저장시 **pickle**을 이용해 직렬화하기 때문에 불어오는 실행환경에도 모델을 저장할 때 사용한 클래스가 있어야 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 모델의 파라미터만 저장\n",
    "- 모델을 구성하는 파라미터만 저장한다.\n",
    "- 모델의 구조는 저장하지 않기 때문에 불러올 때 **모델을 먼저 생성하고 생성한 모델에 불러온 파라미터를 덮어씌운다.**\n",
    "- 모델의 파라미터는 **state_dict** 형식으로 저장한다.\n",
    "\n",
    "### state_dict\n",
    "- 모델의 파라미터 Tensor들을 레이어 단위별로 나누어 저장한 Ordered Dictionary (OrderedDict)\n",
    "- `모델객체.state_dict()` 메소드를 이용해 조회한다.\n",
    "- 모델의 state_dict을 조회 후 저장한다.\n",
    "    - `torch.save(model.state_dict(), \"저장경로\")`\n",
    "- 생성된 모델에 읽어온 state_dict를 덮어씌운다.\n",
    "    - `new_model.load_state_dict(torch.load(\"state_dict저장경로\"))`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#한마디로, 저장할 것을 dict형식으로 저장을 하라는 이야기이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint를 저장 및 불러오기\n",
    "- 학습이 끝나지 않은 모델을 저장 후 나중에 이어서 학습시킬 경우에는 모델의 구조, 파라미터 뿐만 아니라 optimizer, loss 함수등 학습에 필요한 객체들을 저장해야 한다.\n",
    "- Dictionary에 필요한 요소들을 key-value 쌍으로 저장후 `torch.save()`를 이용해 저장한다.(이런 것을 checkpoint 라고 한다.)\n",
    "```python\n",
    "# 저장\n",
    "torch.save({\n",
    "    'epoch':epoch,\n",
    "    'model_state_dict':model.state_dict(),\n",
    "    'optimizer_state_dict':optimizer.state_dict(),\n",
    "    'loss':train_loss\n",
    "}, \"저장경로\")\n",
    "\n",
    "# 불러오기\n",
    "# 모델과 optimizer를 새롭게 생성하고, 저장한 저장경로의 것을 load해 오면 된다.\n",
    "model = MyModel()\n",
    "optimizer = optim.Adam(model.parameter(),lr=0.001)\n",
    "\n",
    "checkpoint = torch.load(\"저장경로\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "#### 이어학습\n",
    "model.train()\n",
    "#### 추론\n",
    "model.eval()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "#단순하게 생각해서, 객체 안에 모듈이 들어가 있다고 생각하면 된다.\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr = nn.Linear(784,64)\n",
    "        self.out = nn.Linear(64,10)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #이런 흐름에 따라서 데이터가 정리가 된다.\n",
    "    def forward(self,X):\n",
    "        X = torch.flatten(X,start_dim=1)\n",
    "        X = self.lr(X) #self.lr에 값을 입력하고, 그 값을 다시 가져갈 것이다.\n",
    "        X = relu(X)\n",
    "        X = out(X)\n",
    "        return X\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_model = Network() #모델 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '경로/sample_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    454\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 455\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ascii'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    456\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mUnicodeEncodeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode characters in position 0-1: ordinal not in range(128)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-554e1043f08c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#모델 구조 + 파라미터\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"경로/sample_model.pth\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    619\u001b[0m             \u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_disable_byteorder_record\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    490\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    458\u001b[0m             \u001b[1;31m# For filenames with non-ascii characters, we rely on Python\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m             \u001b[1;31m# for writing out the file.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 460\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_stream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m             \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_stream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '경로/sample_model.pth'"
     ]
    }
   ],
   "source": [
    "#모델 구조 + 파라미터\n",
    "torch.save(sample_model,\"경로/sample_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['lr.weight', 'lr.bias', 'out.weight', 'out.bias'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 모델의 파라미터만 저장 -> model.state_dict(): 파라미터들만 조회한다.\n",
    "\n",
    "\n",
    "\n",
    "state_dict  =sample_model.state_dict()\n",
    "print(type(state_dict)) #orderedDict : 순서를 유지하는 dictionary\n",
    "state_dict.keys() #키값들만 조회\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 784]), torch.Size([64]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#다양한 값들을 보도록 하자.\n",
    "lr_weight = state_dict['lr.weight']\n",
    "lr_bias = state_dict['lr.bias']\n",
    "lr_weight.shape, lr_bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 64]), torch.Size([10]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_weight = state_dict['out.weight']\n",
    "lr_bias = state_dict['out.bias']\n",
    "lr_weight.shape, lr_bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0353,  0.0067,  0.0639, -0.1071,  0.0629,  0.0887, -0.0594,  0.1086,\n",
       "        -0.0553,  0.0206])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_bias #값을 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 파라미터 저장\n",
    "#주의: 여러번 저장하면안된다. 오류난다.\n",
    "import os\n",
    "#os.makedirs('models/sample')\n",
    "\n",
    "\n",
    "#torch.save(state_dict, \"models/sample/sample_state_dict.pth\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['lr.weight', 'lr.bias', 'out.weight', 'out.bias'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##파라미터 로드\n",
    "\n",
    "\n",
    "\n",
    "load_state_dict = torch.load(\"models/sample/sample_state_dict.pth\")\n",
    "load_state_dict.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1146,  0.0596, -0.0638, -0.0289,  0.1136,  0.1118,  0.0705,  0.0064,\n",
       "         0.0513,  0.0966])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_state_dict['out.bias'] #값을 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#새로 모델객체를 생성한다. \n",
    "\n",
    "new_model = Network()\n",
    "#파라미터들을 매칭한다.\n",
    "new_model.load_state_dict(load_state_dict) #매칭! 만약 잘 되면 매칭이 잘 되었다고 결과가 뜬다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 문제 유형별 MLP 네트워크\n",
    "- MLP(Multi Layer Perceptron)\n",
    "    - Fully Connected Layer로 구성된 네트워크"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Regression(회귀)\n",
    "\n",
    "\n",
    "- 그냥 값을 예측하는 것이다.예측할 값이 정해져 있지 않은 경우이다. => 연속형 값을 추론하는 경우이다. 정답이 될 수 있는 경우는 무한대이다.\n",
    "- 정답이 될 수 있는 것이 정해져 있는 것과는 정 반대이다.\n",
    "\n",
    "## Boston Housing Dataset\n",
    "보스턴 주택가격 dataset은 다음과 같은 속성을 바탕으로 해당 타운 주택 가격의 중앙값을 예측하는 문제.\n",
    "- CRIM: 범죄율\n",
    "- ZN: 25,000 평방피트당 주거지역 비율\n",
    "- INDUS: 비소매 상업지구 비율\n",
    "- CHAS: 찰스강에 인접해 있는지 여부(인접:1, 아니면:0)\n",
    "- NOX: 일산화질소 농도(단위: 0.1ppm)\n",
    "- RM: 주택당 방의 수\n",
    "- AGE: 1940년 이전에 건설된 주택의 비율\n",
    "- DIS: 5개의 보스턴 직업고용센터와의 거리(가중 평균)\n",
    "- RAD: 고속도로 접근성\n",
    "- TAX: 재산세율\n",
    "- PTRATIO: 학생/교사 비율\n",
    "- B: 흑인 비율\n",
    "- LSTAT: 하위 계층 비율\n",
    "<br><br>\n",
    "- **Target**\n",
    "    - MEDV: 타운의 주택가격 중앙값(단위: 1,000달러)---우리나라의 주택 평균 가격 정도."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.22.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.0.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#import 구문 따로 설정하기.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "import torchinfo\n",
    "\n",
    "\n",
    "#이렇게 import 하는 구문을 따로 두는 것이 중요하다.\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset, DataLoader 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 14)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   CRIM     506 non-null    float64\n",
      " 1   ZN       506 non-null    float64\n",
      " 2   INDUS    506 non-null    float64\n",
      " 3   CHAS     506 non-null    float64\n",
      " 4   NOX      506 non-null    float64\n",
      " 5   RM       506 non-null    float64\n",
      " 6   AGE      506 non-null    float64\n",
      " 7   DIS      506 non-null    float64\n",
      " 8   RAD      506 non-null    float64\n",
      " 9   TAX      506 non-null    float64\n",
      " 10  PTRATIO  506 non-null    float64\n",
      " 11  B        506 non-null    float64\n",
      " 12  LSTAT    506 non-null    float64\n",
      " 13  MEDV     506 non-null    float64\n",
      "dtypes: float64(14)\n",
      "memory usage: 55.5 KB\n"
     ]
    }
   ],
   "source": [
    "#데이터를 읽는다.\n",
    "\n",
    "boston =  pd.read_csv('data/boston_hosing.csv') #이름을 유의하자....\n",
    "print(boston.shape)\n",
    "boston.info()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.head() #맨 위의 5개를 출력해볼까?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24. ],\n",
       "       [21.6],\n",
       "       [34.7],\n",
       "       [33.4],\n",
       "       [36.2],\n",
       "       [28.7],\n",
       "       [22.9],\n",
       "       [27.1],\n",
       "       [16.5],\n",
       "       [18.9],\n",
       "       [15. ],\n",
       "       [18.9],\n",
       "       [21.7],\n",
       "       [20.4],\n",
       "       [18.2],\n",
       "       [19.9],\n",
       "       [23.1],\n",
       "       [17.5],\n",
       "       [20.2],\n",
       "       [18.2],\n",
       "       [13.6],\n",
       "       [19.6],\n",
       "       [15.2],\n",
       "       [14.5],\n",
       "       [15.6],\n",
       "       [13.9],\n",
       "       [16.6],\n",
       "       [14.8],\n",
       "       [18.4],\n",
       "       [21. ],\n",
       "       [12.7],\n",
       "       [14.5],\n",
       "       [13.2],\n",
       "       [13.1],\n",
       "       [13.5],\n",
       "       [18.9],\n",
       "       [20. ],\n",
       "       [21. ],\n",
       "       [24.7],\n",
       "       [30.8],\n",
       "       [34.9],\n",
       "       [26.6],\n",
       "       [25.3],\n",
       "       [24.7],\n",
       "       [21.2],\n",
       "       [19.3],\n",
       "       [20. ],\n",
       "       [16.6],\n",
       "       [14.4],\n",
       "       [19.4],\n",
       "       [19.7],\n",
       "       [20.5],\n",
       "       [25. ],\n",
       "       [23.4],\n",
       "       [18.9],\n",
       "       [35.4],\n",
       "       [24.7],\n",
       "       [31.6],\n",
       "       [23.3],\n",
       "       [19.6],\n",
       "       [18.7],\n",
       "       [16. ],\n",
       "       [22.2],\n",
       "       [25. ],\n",
       "       [33. ],\n",
       "       [23.5],\n",
       "       [19.4],\n",
       "       [22. ],\n",
       "       [17.4],\n",
       "       [20.9],\n",
       "       [24.2],\n",
       "       [21.7],\n",
       "       [22.8],\n",
       "       [23.4],\n",
       "       [24.1],\n",
       "       [21.4],\n",
       "       [20. ],\n",
       "       [20.8],\n",
       "       [21.2],\n",
       "       [20.3],\n",
       "       [28. ],\n",
       "       [23.9],\n",
       "       [24.8],\n",
       "       [22.9],\n",
       "       [23.9],\n",
       "       [26.6],\n",
       "       [22.5],\n",
       "       [22.2],\n",
       "       [23.6],\n",
       "       [28.7],\n",
       "       [22.6],\n",
       "       [22. ],\n",
       "       [22.9],\n",
       "       [25. ],\n",
       "       [20.6],\n",
       "       [28.4],\n",
       "       [21.4],\n",
       "       [38.7],\n",
       "       [43.8],\n",
       "       [33.2],\n",
       "       [27.5],\n",
       "       [26.5],\n",
       "       [18.6],\n",
       "       [19.3],\n",
       "       [20.1],\n",
       "       [19.5],\n",
       "       [19.5],\n",
       "       [20.4],\n",
       "       [19.8],\n",
       "       [19.4],\n",
       "       [21.7],\n",
       "       [22.8],\n",
       "       [18.8],\n",
       "       [18.7],\n",
       "       [18.5],\n",
       "       [18.3],\n",
       "       [21.2],\n",
       "       [19.2],\n",
       "       [20.4],\n",
       "       [19.3],\n",
       "       [22. ],\n",
       "       [20.3],\n",
       "       [20.5],\n",
       "       [17.3],\n",
       "       [18.8],\n",
       "       [21.4],\n",
       "       [15.7],\n",
       "       [16.2],\n",
       "       [18. ],\n",
       "       [14.3],\n",
       "       [19.2],\n",
       "       [19.6],\n",
       "       [23. ],\n",
       "       [18.4],\n",
       "       [15.6],\n",
       "       [18.1],\n",
       "       [17.4],\n",
       "       [17.1],\n",
       "       [13.3],\n",
       "       [17.8],\n",
       "       [14. ],\n",
       "       [14.4],\n",
       "       [13.4],\n",
       "       [15.6],\n",
       "       [11.8],\n",
       "       [13.8],\n",
       "       [15.6],\n",
       "       [14.6],\n",
       "       [17.8],\n",
       "       [15.4],\n",
       "       [21.5],\n",
       "       [19.6],\n",
       "       [15.3],\n",
       "       [19.4],\n",
       "       [17. ],\n",
       "       [15.6],\n",
       "       [13.1],\n",
       "       [41.3],\n",
       "       [24.3],\n",
       "       [23.3],\n",
       "       [27. ],\n",
       "       [50. ],\n",
       "       [50. ],\n",
       "       [50. ],\n",
       "       [22.7],\n",
       "       [25. ],\n",
       "       [50. ],\n",
       "       [23.8],\n",
       "       [23.8],\n",
       "       [22.3],\n",
       "       [17.4],\n",
       "       [19.1],\n",
       "       [23.1],\n",
       "       [23.6],\n",
       "       [22.6],\n",
       "       [29.4],\n",
       "       [23.2],\n",
       "       [24.6],\n",
       "       [29.9],\n",
       "       [37.2],\n",
       "       [39.8],\n",
       "       [36.2],\n",
       "       [37.9],\n",
       "       [32.5],\n",
       "       [26.4],\n",
       "       [29.6],\n",
       "       [50. ],\n",
       "       [32. ],\n",
       "       [29.8],\n",
       "       [34.9],\n",
       "       [37. ],\n",
       "       [30.5],\n",
       "       [36.4],\n",
       "       [31.1],\n",
       "       [29.1],\n",
       "       [50. ],\n",
       "       [33.3],\n",
       "       [30.3],\n",
       "       [34.6],\n",
       "       [34.9],\n",
       "       [32.9],\n",
       "       [24.1],\n",
       "       [42.3],\n",
       "       [48.5],\n",
       "       [50. ],\n",
       "       [22.6],\n",
       "       [24.4],\n",
       "       [22.5],\n",
       "       [24.4],\n",
       "       [20. ],\n",
       "       [21.7],\n",
       "       [19.3],\n",
       "       [22.4],\n",
       "       [28.1],\n",
       "       [23.7],\n",
       "       [25. ],\n",
       "       [23.3],\n",
       "       [28.7],\n",
       "       [21.5],\n",
       "       [23. ],\n",
       "       [26.7],\n",
       "       [21.7],\n",
       "       [27.5],\n",
       "       [30.1],\n",
       "       [44.8],\n",
       "       [50. ],\n",
       "       [37.6],\n",
       "       [31.6],\n",
       "       [46.7],\n",
       "       [31.5],\n",
       "       [24.3],\n",
       "       [31.7],\n",
       "       [41.7],\n",
       "       [48.3],\n",
       "       [29. ],\n",
       "       [24. ],\n",
       "       [25.1],\n",
       "       [31.5],\n",
       "       [23.7],\n",
       "       [23.3],\n",
       "       [22. ],\n",
       "       [20.1],\n",
       "       [22.2],\n",
       "       [23.7],\n",
       "       [17.6],\n",
       "       [18.5],\n",
       "       [24.3],\n",
       "       [20.5],\n",
       "       [24.5],\n",
       "       [26.2],\n",
       "       [24.4],\n",
       "       [24.8],\n",
       "       [29.6],\n",
       "       [42.8],\n",
       "       [21.9],\n",
       "       [20.9],\n",
       "       [44. ],\n",
       "       [50. ],\n",
       "       [36. ],\n",
       "       [30.1],\n",
       "       [33.8],\n",
       "       [43.1],\n",
       "       [48.8],\n",
       "       [31. ],\n",
       "       [36.5],\n",
       "       [22.8],\n",
       "       [30.7],\n",
       "       [50. ],\n",
       "       [43.5],\n",
       "       [20.7],\n",
       "       [21.1],\n",
       "       [25.2],\n",
       "       [24.4],\n",
       "       [35.2],\n",
       "       [32.4],\n",
       "       [32. ],\n",
       "       [33.2],\n",
       "       [33.1],\n",
       "       [29.1],\n",
       "       [35.1],\n",
       "       [45.4],\n",
       "       [35.4],\n",
       "       [46. ],\n",
       "       [50. ],\n",
       "       [32.2],\n",
       "       [22. ],\n",
       "       [20.1],\n",
       "       [23.2],\n",
       "       [22.3],\n",
       "       [24.8],\n",
       "       [28.5],\n",
       "       [37.3],\n",
       "       [27.9],\n",
       "       [23.9],\n",
       "       [21.7],\n",
       "       [28.6],\n",
       "       [27.1],\n",
       "       [20.3],\n",
       "       [22.5],\n",
       "       [29. ],\n",
       "       [24.8],\n",
       "       [22. ],\n",
       "       [26.4],\n",
       "       [33.1],\n",
       "       [36.1],\n",
       "       [28.4],\n",
       "       [33.4],\n",
       "       [28.2],\n",
       "       [22.8],\n",
       "       [20.3],\n",
       "       [16.1],\n",
       "       [22.1],\n",
       "       [19.4],\n",
       "       [21.6],\n",
       "       [23.8],\n",
       "       [16.2],\n",
       "       [17.8],\n",
       "       [19.8],\n",
       "       [23.1],\n",
       "       [21. ],\n",
       "       [23.8],\n",
       "       [23.1],\n",
       "       [20.4],\n",
       "       [18.5],\n",
       "       [25. ],\n",
       "       [24.6],\n",
       "       [23. ],\n",
       "       [22.2],\n",
       "       [19.3],\n",
       "       [22.6],\n",
       "       [19.8],\n",
       "       [17.1],\n",
       "       [19.4],\n",
       "       [22.2],\n",
       "       [20.7],\n",
       "       [21.1],\n",
       "       [19.5],\n",
       "       [18.5],\n",
       "       [20.6],\n",
       "       [19. ],\n",
       "       [18.7],\n",
       "       [32.7],\n",
       "       [16.5],\n",
       "       [23.9],\n",
       "       [31.2],\n",
       "       [17.5],\n",
       "       [17.2],\n",
       "       [23.1],\n",
       "       [24.5],\n",
       "       [26.6],\n",
       "       [22.9],\n",
       "       [24.1],\n",
       "       [18.6],\n",
       "       [30.1],\n",
       "       [18.2],\n",
       "       [20.6],\n",
       "       [17.8],\n",
       "       [21.7],\n",
       "       [22.7],\n",
       "       [22.6],\n",
       "       [25. ],\n",
       "       [19.9],\n",
       "       [20.8],\n",
       "       [16.8],\n",
       "       [21.9],\n",
       "       [27.5],\n",
       "       [21.9],\n",
       "       [23.1],\n",
       "       [50. ],\n",
       "       [50. ],\n",
       "       [50. ],\n",
       "       [50. ],\n",
       "       [50. ],\n",
       "       [13.8],\n",
       "       [13.8],\n",
       "       [15. ],\n",
       "       [13.9],\n",
       "       [13.3],\n",
       "       [13.1],\n",
       "       [10.2],\n",
       "       [10.4],\n",
       "       [10.9],\n",
       "       [11.3],\n",
       "       [12.3],\n",
       "       [ 8.8],\n",
       "       [ 7.2],\n",
       "       [10.5],\n",
       "       [ 7.4],\n",
       "       [10.2],\n",
       "       [11.5],\n",
       "       [15.1],\n",
       "       [23.2],\n",
       "       [ 9.7],\n",
       "       [13.8],\n",
       "       [12.7],\n",
       "       [13.1],\n",
       "       [12.5],\n",
       "       [ 8.5],\n",
       "       [ 5. ],\n",
       "       [ 6.3],\n",
       "       [ 5.6],\n",
       "       [ 7.2],\n",
       "       [12.1],\n",
       "       [ 8.3],\n",
       "       [ 8.5],\n",
       "       [ 5. ],\n",
       "       [11.9],\n",
       "       [27.9],\n",
       "       [17.2],\n",
       "       [27.5],\n",
       "       [15. ],\n",
       "       [17.2],\n",
       "       [17.9],\n",
       "       [16.3],\n",
       "       [ 7. ],\n",
       "       [ 7.2],\n",
       "       [ 7.5],\n",
       "       [10.4],\n",
       "       [ 8.8],\n",
       "       [ 8.4],\n",
       "       [16.7],\n",
       "       [14.2],\n",
       "       [20.8],\n",
       "       [13.4],\n",
       "       [11.7],\n",
       "       [ 8.3],\n",
       "       [10.2],\n",
       "       [10.9],\n",
       "       [11. ],\n",
       "       [ 9.5],\n",
       "       [14.5],\n",
       "       [14.1],\n",
       "       [16.1],\n",
       "       [14.3],\n",
       "       [11.7],\n",
       "       [13.4],\n",
       "       [ 9.6],\n",
       "       [ 8.7],\n",
       "       [ 8.4],\n",
       "       [12.8],\n",
       "       [10.5],\n",
       "       [17.1],\n",
       "       [18.4],\n",
       "       [15.4],\n",
       "       [10.8],\n",
       "       [11.8],\n",
       "       [14.9],\n",
       "       [12.6],\n",
       "       [14.1],\n",
       "       [13. ],\n",
       "       [13.4],\n",
       "       [15.2],\n",
       "       [16.1],\n",
       "       [17.8],\n",
       "       [14.9],\n",
       "       [14.1],\n",
       "       [12.7],\n",
       "       [13.5],\n",
       "       [14.9],\n",
       "       [20. ],\n",
       "       [16.4],\n",
       "       [17.7],\n",
       "       [19.5],\n",
       "       [20.2],\n",
       "       [21.4],\n",
       "       [19.9],\n",
       "       [19. ],\n",
       "       [19.1],\n",
       "       [19.1],\n",
       "       [20.1],\n",
       "       [19.9],\n",
       "       [19.6],\n",
       "       [23.2],\n",
       "       [29.8],\n",
       "       [13.8],\n",
       "       [13.3],\n",
       "       [16.7],\n",
       "       [12. ],\n",
       "       [14.6],\n",
       "       [21.4],\n",
       "       [23. ],\n",
       "       [23.7],\n",
       "       [25. ],\n",
       "       [21.8],\n",
       "       [20.6],\n",
       "       [21.2],\n",
       "       [19.1],\n",
       "       [20.6],\n",
       "       [15.2],\n",
       "       [ 7. ],\n",
       "       [ 8.1],\n",
       "       [13.6],\n",
       "       [20.1],\n",
       "       [21.8],\n",
       "       [24.5],\n",
       "       [23.1],\n",
       "       [19.7],\n",
       "       [18.3],\n",
       "       [21.2],\n",
       "       [17.5],\n",
       "       [16.8],\n",
       "       [22.4],\n",
       "       [20.6],\n",
       "       [23.9],\n",
       "       [22. ],\n",
       "       [11.9]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x(input data,features),y(output data,target,label)을 분리한다.\n",
    "\n",
    "\n",
    "\n",
    "#왜 MEDV를 기준으로 target 을 설정했는지 잘 생각해보면 안다.\n",
    "X_boston = boston.drop(columns=\"MEDV\").values #컬럼을 제거한다.\n",
    "y_boston = boston['MEDV'].to_frame().values #to_frame을 통해 dataframe으로 데이터를 가져 올 수 있다. values 메소드도 잘 파악할 것.\n",
    "\n",
    "y_boston #shape을 통해 데이터가 총 몇개인지 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13) (102, 13) (404, 1) (102, 1)\n"
     ]
    }
   ],
   "source": [
    "##trainset/testset을 분리\n",
    "#8:2로 데이터를 분리한다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_boston,\n",
    "                                                    y_boston,\n",
    "                                                    test_size=0.2,\n",
    "                                                   random_state=0\n",
    "                                                   )#이 함수는 scikit-learn에 있다. \n",
    "#random_state는 seed 값을 고정시켜서 섞이는 순서를 동일하게 한다.\n",
    "\n",
    "#회귀(정답이 연속형-다 다른 값)는 stratify=y를 설정하지 않는다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#설정한 대로 데이터가 잘 구분이 된 것을 알 수 있다.\n",
    "#숫자들을 잘 보면, 대강 8:2로 데이터들이 나뉘어져 있다는 것을 볼 수 있다.\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature scaling =>컬럼들의 scaling을 맞춰주는 작업이다.\n",
    "\n",
    "#단순히 값의 크기에 따라 모델링이 되지 않도록 처리한다. 약간 다듬는 과정이라고 보면 된다..\n",
    "\n",
    "\n",
    "#예시: 200과 2가 있다면 당연히 200이 더 크다. 근데 200CM랑 2KM는?\n",
    "#이런 예시가 있기 때문에, 다듬는 과정이 필요하다고 볼 수 있다.\n",
    "\n",
    "#뭐 0부터 255까지의 범위를 0~1까지로 조정하는 것을 feature scaling의 예시라고 볼 수 있다.\n",
    "\n",
    "\n",
    "#StandardScaler => 모든 컬럼의 척도를 평균 0, 표준편차 1로 맞춘다. 뭐 간단하게 조정하는 것이다.\n",
    "#Feature sacling은 train set의 평균과 표준편차를 이용해 train set/test set의 값들에 적용한다.\n",
    "\n",
    "\n",
    "X_train_mean = X_train.mean(axis=0) #컬럼별 평균. axis를 잘 보도록하자.\n",
    "X_train_std = X_train.std(axis=0) #컬럼별 표준편차.\n",
    "\n",
    "\n",
    "X_train_scaled_raw = (X_train - X_train_mean)/X_train_std\n",
    "#각원소값-평균/표준편차=표준점수\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.33174316e-16, -6.23813432e-17,  2.69916103e-15, -3.95723058e-17,\n",
       "       -9.89857261e-16, -1.47571724e-15,  8.65918998e-16,  2.00884414e-16,\n",
       "       -1.18716917e-16,  2.91296140e-17,  1.65098958e-14,  7.99759049e-15,\n",
       "       -1.85852434e-15])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled_raw.mean(axis=0) #평균이 거의 0이라고 볼 수 있다. 10의 -14승 정도이니 ㅋㅋ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled_raw.std(axis=0) #표편이 모두 1인 것을 목격할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sklearn을 이용해 Standard Scaling 처리\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train) #어떻게 변환할지 학습을 한다. ->평균과 표준편차를 계산한다.\n",
    "X_train_scaled = scaler.transform(X_train) #변환.\n",
    "X_test_scaled = scaler.transform(X_test) #X_train의 평균/표준편차 기준으로 testset도 변환한다.(모델을 좀 더 정확히 평가하기 위함이다.)\n",
    "\n",
    "\n",
    "# 메소드를 이용해서 더 간단히 코딩을 한 버전이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mean = X_train_scaled.mean(axis=0) #컬럼별 평균. axis를 잘 보도록하자.\n",
    "X_train_std = X_train_scaled.std(axis=0) #컬럼별 표준편차.\n",
    "X_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train set: 모델을 학습한다. ===>이전 데이터를 대표하는 샘플이다.\n",
    "#test set: 모델을 평가한다. ==>앞으로 예측할 데이터를 대표하는 샘플이다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-39-6a0baa4ea347>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train,dtype=torch.float32)\n",
      "<ipython-input-39-6a0baa4ea347>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test = torch.tensor(y_test,dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "#X,y : ndarray => torch.Tensor로 변환\n",
    "\n",
    "X_train_scaled = torch.tensor(X_train_scaled,dtype=torch.float32)\n",
    "X_test_scaled = torch.tensor(X_test_scaled,dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train,dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset의 데이터 개수: 404 102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([-0.3726, -0.4996, -0.7049,  3.6645, -0.4249,  0.9357,  0.6937, -0.4372,\n",
       "         -0.1622, -0.5617, -0.4846,  0.3717, -0.4110]),\n",
       " tensor([26.7000]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dataset 생성\n",
    "boston_trainset = TensorDataset(X_train_scaled,y_train)\n",
    "boston_testset = TensorDataset(X_test_scaled,y_test)\n",
    "\n",
    "print(\"Dataset의 데이터 개수:\",len(boston_trainset),len(boston_testset))\n",
    "boston_trainset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch당 step수: 2 1\n"
     ]
    }
   ],
   "source": [
    "#DataLoader 생성\n",
    "\n",
    "boston_trainloader = DataLoader(boston_trainset,batch_size=200,shuffle=True,drop_last=True) #drop_last를 통해 '몫'이 아닌 '나머지'들을 정리해 낼 수 있다.\n",
    "\n",
    "boston_testloader = DataLoader(boston_testset,batch_size=len(boston_testset)) #나중에 사이즈가 바뀌더라도 잘 할 수 있도록 len으로 설정한다.\n",
    "\n",
    "\n",
    "print(\"epoch당 step수:\",len(boston_trainloader),len(boston_testloader)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset이랑 dataloader를 생성했으니, 이제 모델을 정의해야 한다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BostonModel(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        #nn.Module의 __init__() 실행 =>초기화\n",
    "        super().__init__()\n",
    "        #forward propagation(예측) 할 때 필요한 layer들을 생성한다.\n",
    "        \n",
    "        \n",
    "        self.lr1 = nn.Linear(in_features=13,out_features=32) #13은 우리가 하는 예시의 데이터의 수이다.\n",
    "        #입력이 13이고 출력이 32라는 것은 weight를 13*32짜리를 만들겠다는 것이다.\n",
    "        self.lr2 = nn.Linear(32,16) #들어가는 것이 32이니까 나가는 것을 약수인 16으로 설정.(사실 무슨 값으로 해도 무방하긴 하다.)\n",
    "        ##lr3을 출력 layer로 만든다. out_features를 지정해야 하는데, 이는 모델이 출력해야 할 값의 개수에 맞춰준다.\n",
    "        self.lr3 = nn.Linear(16,1) # 최종적으로 1개를 출력한다. 집값 하나를 예측해야 하기 때문이다.\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,X):\n",
    "        out = self.lr1(X) #선형\n",
    "        out = nn.ReLU()(out) #비선형 #lr1을 한 다음 relu함수에 이를 집어 넣는다. relu객체를 생성한 다음, 그 안에다 넣는 것이다.\n",
    "        out = self.lr2(out)#선형\n",
    "        out = nn.ReLU()(out)#비선형 #다시 한번 relu를 쓴다.\n",
    "        \n",
    "        out=self.lr3(out) #출력 레이어(이 값이 모델의 예측값이 된다.)\n",
    "        #회귀의 출력결과에는 activation 함수를 정의하지 않는다. \n",
    "        # 예외: 출력값의 범위가 정해져 있고 그 범위값을 출력하는 함수가 있는 경우에는 예외이다.\n",
    "        # 범위: 0~1 -> logistic (nn.sigmoid())\n",
    "        #        -1~1 -> tanh(nn.Tanh())\n",
    "        \n",
    "        \n",
    "        return out #out을 최종적으로 return 한다.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BostonModel(\n",
      "  (lr1): Linear(in_features=13, out_features=32, bias=True)\n",
      "  (lr2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (lr3): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#모델 생성\n",
    "\n",
    "boston_model = BostonModel()\n",
    "\n",
    "#모델 구조 확인\n",
    "\n",
    "print(boston_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BostonModel                              [100, 1]                  --\n",
       "├─Linear: 1-1                            [100, 32]                 448\n",
       "├─Linear: 1-2                            [100, 16]                 528\n",
       "├─Linear: 1-3                            [100, 1]                  17\n",
       "==========================================================================================\n",
       "Total params: 993\n",
       "Trainable params: 993\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.10\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.04\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.05\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모델 구조 확인\n",
    "#모델, 입력데이터의 shape(batch_size,features)\n",
    "torchinfo.summary(boston_model,(100,13))\n",
    "\n",
    "\n",
    "#param은 weight와 bias의 개수를 합친 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습(train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습 + 검증\n",
    "\n",
    "\n",
    "\n",
    "boston_model = boston_model.to(device) #모델: 1. 생성 2. device를 설정\n",
    "#loss 함수 정의 - 회귀: mse\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "#하이퍼파라미터(우리가 설정함.) 정의\n",
    "LR=0.001\n",
    "N_EPOCH =1000\n",
    "\n",
    "#optimizer를 정의한다.\n",
    "optimizer = torch.optim.RMSprop(boston_model.parameters(),lr=LR)   #torch.optim 모듈에 최적화알고리즘들이 정의되어 있다. (모델의 파라미터, 학습률)\n",
    "#최적화할 대상이 boston_model의 파라미터들임을 알려준다.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#에폭별 학습 결과를 저장할 리스트\n",
    "##train loss와 validation loss를 저장한다. (valid는 학습 중에 성능을 체크하는 것이기 때문이다.)\n",
    "\n",
    "\n",
    "train_loss_list =[]\n",
    "valid_loss_list =[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1000]  train loss: 6.2860,valid loss: 21.0379\n",
      "[2/1000]  train loss: 4.6088,valid loss: 19.0763\n",
      "[3/1000]  train loss: 3.8892,valid loss: 18.8754\n",
      "[4/1000]  train loss: 3.6911,valid loss: 18.4967\n",
      "[5/1000]  train loss: 3.6581,valid loss: 18.5264\n",
      "[6/1000]  train loss: 3.5510,valid loss: 17.8601\n",
      "[7/1000]  train loss: 3.7504,valid loss: 18.6694\n",
      "[8/1000]  train loss: 3.6733,valid loss: 18.0838\n",
      "[9/1000]  train loss: 3.6567,valid loss: 17.7966\n",
      "[10/1000]  train loss: 3.6592,valid loss: 17.9446\n",
      "[11/1000]  train loss: 3.5374,valid loss: 18.3578\n",
      "[12/1000]  train loss: 3.5152,valid loss: 18.2789\n",
      "[13/1000]  train loss: 3.4969,valid loss: 18.1138\n",
      "[14/1000]  train loss: 3.5304,valid loss: 18.2971\n",
      "[15/1000]  train loss: 3.6085,valid loss: 17.9911\n",
      "[16/1000]  train loss: 3.5844,valid loss: 18.1177\n",
      "[17/1000]  train loss: 3.5072,valid loss: 18.2706\n",
      "[18/1000]  train loss: 3.4079,valid loss: 18.3489\n",
      "[19/1000]  train loss: 3.5051,valid loss: 18.2322\n",
      "[20/1000]  train loss: 3.4913,valid loss: 18.4439\n",
      "[21/1000]  train loss: 3.4103,valid loss: 18.0909\n",
      "[22/1000]  train loss: 3.5702,valid loss: 17.9790\n",
      "[23/1000]  train loss: 3.5266,valid loss: 18.1705\n",
      "[24/1000]  train loss: 3.4773,valid loss: 18.1047\n",
      "[25/1000]  train loss: 3.4610,valid loss: 18.3216\n",
      "[26/1000]  train loss: 3.4796,valid loss: 18.5716\n",
      "[27/1000]  train loss: 3.4013,valid loss: 18.5511\n",
      "[28/1000]  train loss: 3.4536,valid loss: 18.0133\n",
      "[29/1000]  train loss: 3.4506,valid loss: 18.0801\n",
      "[30/1000]  train loss: 3.4226,valid loss: 18.2632\n",
      "[31/1000]  train loss: 3.3120,valid loss: 18.3152\n",
      "[32/1000]  train loss: 3.4395,valid loss: 18.3702\n",
      "[33/1000]  train loss: 3.3355,valid loss: 18.0632\n",
      "[34/1000]  train loss: 3.4297,valid loss: 18.1020\n",
      "[35/1000]  train loss: 3.4109,valid loss: 18.3704\n",
      "[36/1000]  train loss: 3.4748,valid loss: 18.5322\n",
      "[37/1000]  train loss: 3.4339,valid loss: 18.2109\n",
      "[38/1000]  train loss: 3.4136,valid loss: 18.1156\n",
      "[39/1000]  train loss: 3.4500,valid loss: 17.9289\n",
      "[40/1000]  train loss: 3.4260,valid loss: 18.4551\n",
      "[41/1000]  train loss: 3.5144,valid loss: 18.3654\n",
      "[42/1000]  train loss: 3.3535,valid loss: 18.1928\n",
      "[43/1000]  train loss: 3.5239,valid loss: 18.2223\n",
      "[44/1000]  train loss: 3.3940,valid loss: 18.5714\n",
      "[45/1000]  train loss: 3.4284,valid loss: 18.4467\n",
      "[46/1000]  train loss: 3.3625,valid loss: 18.1620\n",
      "[47/1000]  train loss: 3.3520,valid loss: 18.1502\n",
      "[48/1000]  train loss: 3.4290,valid loss: 18.0591\n",
      "[49/1000]  train loss: 3.4216,valid loss: 18.0532\n",
      "[50/1000]  train loss: 3.4968,valid loss: 18.3758\n",
      "[51/1000]  train loss: 3.5353,valid loss: 18.5904\n",
      "[52/1000]  train loss: 3.4286,valid loss: 18.1158\n",
      "[53/1000]  train loss: 3.4037,valid loss: 18.5879\n",
      "[54/1000]  train loss: 3.4467,valid loss: 18.5898\n",
      "[55/1000]  train loss: 3.3113,valid loss: 18.2668\n",
      "[56/1000]  train loss: 3.3464,valid loss: 18.4723\n",
      "[57/1000]  train loss: 3.3282,valid loss: 18.5926\n",
      "[58/1000]  train loss: 3.2900,valid loss: 18.2566\n",
      "[59/1000]  train loss: 3.3182,valid loss: 18.1764\n",
      "[60/1000]  train loss: 3.3090,valid loss: 18.5651\n",
      "[61/1000]  train loss: 3.3634,valid loss: 18.6735\n",
      "[62/1000]  train loss: 3.3249,valid loss: 18.3241\n",
      "[63/1000]  train loss: 3.2206,valid loss: 18.4826\n",
      "[64/1000]  train loss: 3.2983,valid loss: 18.2832\n",
      "[65/1000]  train loss: 3.3166,valid loss: 18.3907\n",
      "[66/1000]  train loss: 3.3671,valid loss: 18.6832\n",
      "[67/1000]  train loss: 3.3975,valid loss: 18.6130\n",
      "[68/1000]  train loss: 3.2806,valid loss: 18.4433\n",
      "[69/1000]  train loss: 3.2863,valid loss: 18.3030\n",
      "[70/1000]  train loss: 3.2853,valid loss: 18.6194\n",
      "[71/1000]  train loss: 3.3679,valid loss: 18.3770\n",
      "[72/1000]  train loss: 3.3550,valid loss: 18.4466\n",
      "[73/1000]  train loss: 3.2502,valid loss: 18.4183\n",
      "[74/1000]  train loss: 3.2722,valid loss: 18.6032\n",
      "[75/1000]  train loss: 3.3288,valid loss: 18.5303\n",
      "[76/1000]  train loss: 3.3395,valid loss: 18.5504\n",
      "[77/1000]  train loss: 3.2726,valid loss: 18.6046\n",
      "[78/1000]  train loss: 3.2924,valid loss: 18.7124\n",
      "[79/1000]  train loss: 3.2828,valid loss: 18.4438\n",
      "[80/1000]  train loss: 3.2709,valid loss: 18.3399\n",
      "[81/1000]  train loss: 3.4098,valid loss: 18.2861\n",
      "[82/1000]  train loss: 3.2757,valid loss: 18.4657\n",
      "[83/1000]  train loss: 3.2363,valid loss: 18.5189\n",
      "[84/1000]  train loss: 3.2493,valid loss: 18.4165\n",
      "[85/1000]  train loss: 3.1954,valid loss: 18.5219\n",
      "[86/1000]  train loss: 3.1035,valid loss: 18.6103\n",
      "[87/1000]  train loss: 3.2715,valid loss: 18.2963\n",
      "[88/1000]  train loss: 3.2063,valid loss: 18.6214\n",
      "[89/1000]  train loss: 3.2910,valid loss: 18.7201\n",
      "[90/1000]  train loss: 3.2738,valid loss: 19.0378\n",
      "[91/1000]  train loss: 3.2031,valid loss: 18.5273\n",
      "[92/1000]  train loss: 3.2449,valid loss: 18.4834\n",
      "[93/1000]  train loss: 3.2373,valid loss: 18.3436\n",
      "[94/1000]  train loss: 3.2191,valid loss: 18.4744\n",
      "[95/1000]  train loss: 3.2314,valid loss: 18.7649\n",
      "[96/1000]  train loss: 3.3025,valid loss: 19.1865\n",
      "[97/1000]  train loss: 3.2264,valid loss: 18.5846\n",
      "[98/1000]  train loss: 3.1624,valid loss: 18.4359\n",
      "[99/1000]  train loss: 3.2280,valid loss: 18.4260\n",
      "[100/1000]  train loss: 3.1465,valid loss: 18.4601\n",
      "[101/1000]  train loss: 3.2127,valid loss: 18.5667\n",
      "[102/1000]  train loss: 3.1613,valid loss: 18.5912\n",
      "[103/1000]  train loss: 3.2006,valid loss: 18.7809\n",
      "[104/1000]  train loss: 3.1549,valid loss: 18.8592\n",
      "[105/1000]  train loss: 3.1871,valid loss: 18.6316\n",
      "[106/1000]  train loss: 3.1227,valid loss: 18.9879\n",
      "[107/1000]  train loss: 3.2409,valid loss: 18.7560\n",
      "[108/1000]  train loss: 3.4083,valid loss: 18.6305\n",
      "[109/1000]  train loss: 3.1705,valid loss: 18.5477\n",
      "[110/1000]  train loss: 3.2487,valid loss: 18.6780\n",
      "[111/1000]  train loss: 3.2199,valid loss: 18.5601\n",
      "[112/1000]  train loss: 3.3274,valid loss: 18.6439\n",
      "[113/1000]  train loss: 3.3545,valid loss: 18.5932\n",
      "[114/1000]  train loss: 3.1835,valid loss: 18.4862\n",
      "[115/1000]  train loss: 3.2108,valid loss: 18.6352\n",
      "[116/1000]  train loss: 3.1713,valid loss: 18.6899\n",
      "[117/1000]  train loss: 3.1472,valid loss: 18.6594\n",
      "[118/1000]  train loss: 3.0805,valid loss: 18.5905\n",
      "[119/1000]  train loss: 3.1234,valid loss: 18.9801\n",
      "[120/1000]  train loss: 3.1385,valid loss: 18.4853\n",
      "[121/1000]  train loss: 3.1630,valid loss: 18.5480\n",
      "[122/1000]  train loss: 3.2312,valid loss: 18.5821\n",
      "[123/1000]  train loss: 3.1206,valid loss: 18.9707\n",
      "[124/1000]  train loss: 3.1207,valid loss: 18.7388\n",
      "[125/1000]  train loss: 3.1444,valid loss: 18.7288\n",
      "[126/1000]  train loss: 3.1420,valid loss: 18.7332\n",
      "[127/1000]  train loss: 3.1480,valid loss: 19.0428\n",
      "[128/1000]  train loss: 3.1600,valid loss: 18.5920\n",
      "[129/1000]  train loss: 3.1447,valid loss: 18.8432\n",
      "[130/1000]  train loss: 3.0960,valid loss: 18.7589\n",
      "[131/1000]  train loss: 3.1268,valid loss: 18.8078\n",
      "[132/1000]  train loss: 3.2432,valid loss: 18.7400\n",
      "[133/1000]  train loss: 3.1495,valid loss: 18.6523\n",
      "[134/1000]  train loss: 3.0933,valid loss: 18.7628\n",
      "[135/1000]  train loss: 3.0921,valid loss: 18.8687\n",
      "[136/1000]  train loss: 3.0395,valid loss: 18.8011\n",
      "[137/1000]  train loss: 3.1117,valid loss: 18.8810\n",
      "[138/1000]  train loss: 3.1150,valid loss: 19.0865\n",
      "[139/1000]  train loss: 3.1673,valid loss: 18.4672\n",
      "[140/1000]  train loss: 3.1045,valid loss: 18.9658\n",
      "[141/1000]  train loss: 3.1526,valid loss: 18.5491\n",
      "[142/1000]  train loss: 3.1843,valid loss: 18.6174\n",
      "[143/1000]  train loss: 2.9737,valid loss: 18.8260\n",
      "[144/1000]  train loss: 3.0721,valid loss: 18.7800\n",
      "[145/1000]  train loss: 3.0290,valid loss: 19.0358\n",
      "[146/1000]  train loss: 3.0427,valid loss: 18.9631\n",
      "[147/1000]  train loss: 3.0717,valid loss: 19.1165\n",
      "[148/1000]  train loss: 3.1237,valid loss: 19.1996\n",
      "[149/1000]  train loss: 3.2293,valid loss: 18.9311\n",
      "[150/1000]  train loss: 3.1170,valid loss: 18.9407\n",
      "[151/1000]  train loss: 3.0776,valid loss: 18.8384\n",
      "[152/1000]  train loss: 3.0327,valid loss: 18.8473\n",
      "[153/1000]  train loss: 3.0848,valid loss: 18.6458\n",
      "[154/1000]  train loss: 3.0491,valid loss: 18.9341\n",
      "[155/1000]  train loss: 3.0606,valid loss: 19.0542\n",
      "[156/1000]  train loss: 3.0050,valid loss: 18.8055\n",
      "[157/1000]  train loss: 3.0559,valid loss: 18.8204\n",
      "[158/1000]  train loss: 3.0158,valid loss: 18.8059\n",
      "[159/1000]  train loss: 3.1681,valid loss: 19.3694\n",
      "[160/1000]  train loss: 3.1145,valid loss: 19.0653\n",
      "[161/1000]  train loss: 3.0821,valid loss: 19.4100\n",
      "[162/1000]  train loss: 3.0705,valid loss: 19.1283\n",
      "[163/1000]  train loss: 3.0518,valid loss: 19.1856\n",
      "[164/1000]  train loss: 2.9658,valid loss: 19.0539\n",
      "[165/1000]  train loss: 2.9852,valid loss: 18.9977\n",
      "[166/1000]  train loss: 3.0219,valid loss: 19.1988\n",
      "[167/1000]  train loss: 3.0028,valid loss: 18.6249\n",
      "[168/1000]  train loss: 3.0142,valid loss: 19.0612\n",
      "[169/1000]  train loss: 3.0801,valid loss: 19.4371\n",
      "[170/1000]  train loss: 3.0471,valid loss: 18.9646\n",
      "[171/1000]  train loss: 3.0018,valid loss: 18.8987\n",
      "[172/1000]  train loss: 3.0241,valid loss: 18.9447\n",
      "[173/1000]  train loss: 2.9987,valid loss: 19.2532\n",
      "[174/1000]  train loss: 2.9988,valid loss: 18.9447\n",
      "[175/1000]  train loss: 3.0031,valid loss: 18.9192\n",
      "[176/1000]  train loss: 3.1000,valid loss: 18.7491\n",
      "[177/1000]  train loss: 3.1277,valid loss: 18.8432\n",
      "[178/1000]  train loss: 3.0033,valid loss: 19.1970\n",
      "[179/1000]  train loss: 3.0140,valid loss: 19.1369\n",
      "[180/1000]  train loss: 2.9958,valid loss: 19.1334\n",
      "[181/1000]  train loss: 2.9721,valid loss: 18.8756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[182/1000]  train loss: 2.9880,valid loss: 19.2417\n",
      "[183/1000]  train loss: 2.9724,valid loss: 19.1345\n",
      "[184/1000]  train loss: 2.9895,valid loss: 19.0715\n",
      "[185/1000]  train loss: 2.9960,valid loss: 18.6945\n",
      "[186/1000]  train loss: 2.8705,valid loss: 19.1521\n",
      "[187/1000]  train loss: 3.0109,valid loss: 19.2679\n",
      "[188/1000]  train loss: 3.0100,valid loss: 19.3463\n",
      "[189/1000]  train loss: 2.9728,valid loss: 19.1377\n",
      "[190/1000]  train loss: 2.9469,valid loss: 19.1441\n",
      "[191/1000]  train loss: 2.9724,valid loss: 19.2521\n",
      "[192/1000]  train loss: 2.9964,valid loss: 19.2550\n",
      "[193/1000]  train loss: 2.9698,valid loss: 19.0559\n",
      "[194/1000]  train loss: 2.9744,valid loss: 19.1525\n",
      "[195/1000]  train loss: 2.9606,valid loss: 19.1869\n",
      "[196/1000]  train loss: 2.9355,valid loss: 19.0809\n",
      "[197/1000]  train loss: 2.9028,valid loss: 19.1760\n",
      "[198/1000]  train loss: 2.9767,valid loss: 18.8369\n",
      "[199/1000]  train loss: 2.9725,valid loss: 19.5057\n",
      "[200/1000]  train loss: 3.1458,valid loss: 19.3850\n",
      "[201/1000]  train loss: 2.9340,valid loss: 19.1184\n",
      "[202/1000]  train loss: 2.9493,valid loss: 18.9746\n",
      "[203/1000]  train loss: 2.9356,valid loss: 19.1367\n",
      "[204/1000]  train loss: 2.9508,valid loss: 19.2238\n",
      "[205/1000]  train loss: 2.9081,valid loss: 19.1204\n",
      "[206/1000]  train loss: 2.9224,valid loss: 19.2680\n",
      "[207/1000]  train loss: 2.8631,valid loss: 18.9978\n",
      "[208/1000]  train loss: 2.9172,valid loss: 19.0967\n",
      "[209/1000]  train loss: 2.9339,valid loss: 18.8519\n",
      "[210/1000]  train loss: 2.9294,valid loss: 19.1831\n",
      "[211/1000]  train loss: 2.8993,valid loss: 19.2192\n",
      "[212/1000]  train loss: 2.9555,valid loss: 19.3827\n",
      "[213/1000]  train loss: 2.9267,valid loss: 19.0539\n",
      "[214/1000]  train loss: 3.0056,valid loss: 19.1302\n",
      "[215/1000]  train loss: 2.9742,valid loss: 19.1745\n",
      "[216/1000]  train loss: 2.8846,valid loss: 19.0781\n",
      "[217/1000]  train loss: 2.9174,valid loss: 19.3654\n",
      "[218/1000]  train loss: 2.9189,valid loss: 19.4361\n",
      "[219/1000]  train loss: 2.9324,valid loss: 19.3857\n",
      "[220/1000]  train loss: 2.9405,valid loss: 18.9001\n",
      "[221/1000]  train loss: 2.7891,valid loss: 19.0488\n",
      "[222/1000]  train loss: 2.9409,valid loss: 19.2630\n",
      "[223/1000]  train loss: 2.9261,valid loss: 19.0740\n",
      "[224/1000]  train loss: 2.8439,valid loss: 19.1833\n",
      "[225/1000]  train loss: 2.8519,valid loss: 19.1599\n",
      "[226/1000]  train loss: 2.8950,valid loss: 19.1750\n",
      "[227/1000]  train loss: 2.8697,valid loss: 19.4218\n",
      "[228/1000]  train loss: 2.8170,valid loss: 19.3362\n",
      "[229/1000]  train loss: 2.8522,valid loss: 19.1769\n",
      "[230/1000]  train loss: 2.8706,valid loss: 19.4438\n",
      "[231/1000]  train loss: 2.8602,valid loss: 19.4462\n",
      "[232/1000]  train loss: 2.9759,valid loss: 19.9800\n",
      "[233/1000]  train loss: 3.0616,valid loss: 19.3176\n",
      "[234/1000]  train loss: 2.9225,valid loss: 19.4397\n",
      "[235/1000]  train loss: 2.8790,valid loss: 19.3614\n",
      "[236/1000]  train loss: 2.8710,valid loss: 19.1898\n",
      "[237/1000]  train loss: 2.8497,valid loss: 19.3622\n",
      "[238/1000]  train loss: 2.8662,valid loss: 19.4642\n",
      "[239/1000]  train loss: 2.8503,valid loss: 19.3224\n",
      "[240/1000]  train loss: 2.8548,valid loss: 19.0797\n",
      "[241/1000]  train loss: 2.9363,valid loss: 19.2705\n",
      "[242/1000]  train loss: 2.8925,valid loss: 19.0320\n",
      "[243/1000]  train loss: 2.8756,valid loss: 19.3218\n",
      "[244/1000]  train loss: 2.8307,valid loss: 19.6032\n",
      "[245/1000]  train loss: 2.7817,valid loss: 18.9237\n",
      "[246/1000]  train loss: 2.8470,valid loss: 19.3925\n",
      "[247/1000]  train loss: 2.9035,valid loss: 19.5588\n",
      "[248/1000]  train loss: 2.8681,valid loss: 19.2483\n",
      "[249/1000]  train loss: 2.8745,valid loss: 19.3166\n",
      "[250/1000]  train loss: 2.8114,valid loss: 19.4924\n",
      "[251/1000]  train loss: 2.8169,valid loss: 19.3399\n",
      "[252/1000]  train loss: 2.8468,valid loss: 19.3877\n",
      "[253/1000]  train loss: 2.8216,valid loss: 19.2246\n",
      "[254/1000]  train loss: 3.0814,valid loss: 19.4716\n",
      "[255/1000]  train loss: 2.8098,valid loss: 19.4390\n",
      "[256/1000]  train loss: 2.7989,valid loss: 19.2916\n",
      "[257/1000]  train loss: 2.8865,valid loss: 19.0268\n",
      "[258/1000]  train loss: 2.8477,valid loss: 19.4358\n",
      "[259/1000]  train loss: 2.8137,valid loss: 19.6825\n",
      "[260/1000]  train loss: 2.8994,valid loss: 19.4782\n",
      "[261/1000]  train loss: 2.8837,valid loss: 19.2100\n",
      "[262/1000]  train loss: 2.8882,valid loss: 19.4810\n",
      "[263/1000]  train loss: 2.9752,valid loss: 19.6681\n",
      "[264/1000]  train loss: 2.9244,valid loss: 19.7758\n",
      "[265/1000]  train loss: 2.8016,valid loss: 19.4195\n",
      "[266/1000]  train loss: 2.9394,valid loss: 19.2935\n",
      "[267/1000]  train loss: 2.8649,valid loss: 19.2332\n",
      "[268/1000]  train loss: 2.7550,valid loss: 19.4407\n",
      "[269/1000]  train loss: 2.8047,valid loss: 19.7705\n",
      "[270/1000]  train loss: 2.8205,valid loss: 19.6038\n",
      "[271/1000]  train loss: 2.8305,valid loss: 19.7149\n",
      "[272/1000]  train loss: 2.7551,valid loss: 19.3518\n",
      "[273/1000]  train loss: 2.8568,valid loss: 19.4036\n",
      "[274/1000]  train loss: 2.8127,valid loss: 19.4340\n",
      "[275/1000]  train loss: 2.7803,valid loss: 19.4218\n",
      "[276/1000]  train loss: 2.7670,valid loss: 19.5555\n",
      "[277/1000]  train loss: 2.7592,valid loss: 19.6126\n",
      "[278/1000]  train loss: 2.7949,valid loss: 19.7052\n",
      "[279/1000]  train loss: 2.8203,valid loss: 19.3773\n",
      "[280/1000]  train loss: 2.8491,valid loss: 19.3982\n",
      "[281/1000]  train loss: 2.7975,valid loss: 19.3021\n",
      "[282/1000]  train loss: 2.6157,valid loss: 19.6394\n",
      "[283/1000]  train loss: 2.7547,valid loss: 19.6684\n",
      "[284/1000]  train loss: 2.8182,valid loss: 19.7601\n",
      "[285/1000]  train loss: 2.7519,valid loss: 19.7594\n",
      "[286/1000]  train loss: 2.7310,valid loss: 19.5228\n",
      "[287/1000]  train loss: 2.7738,valid loss: 19.5736\n",
      "[288/1000]  train loss: 2.7650,valid loss: 19.4993\n",
      "[289/1000]  train loss: 2.7579,valid loss: 19.4152\n",
      "[290/1000]  train loss: 2.7407,valid loss: 19.4088\n",
      "[291/1000]  train loss: 2.7311,valid loss: 19.5527\n",
      "[292/1000]  train loss: 2.7569,valid loss: 19.6157\n",
      "[293/1000]  train loss: 2.7925,valid loss: 19.3121\n",
      "[294/1000]  train loss: 2.7027,valid loss: 19.4582\n",
      "[295/1000]  train loss: 2.7533,valid loss: 19.3800\n",
      "[296/1000]  train loss: 2.7471,valid loss: 19.4442\n",
      "[297/1000]  train loss: 2.7510,valid loss: 19.3166\n",
      "[298/1000]  train loss: 2.7461,valid loss: 19.7733\n",
      "[299/1000]  train loss: 2.8252,valid loss: 19.2026\n",
      "[300/1000]  train loss: 2.8251,valid loss: 19.4562\n",
      "[301/1000]  train loss: 2.6467,valid loss: 19.7028\n",
      "[302/1000]  train loss: 2.7567,valid loss: 19.7212\n",
      "[303/1000]  train loss: 2.7436,valid loss: 19.5615\n",
      "[304/1000]  train loss: 2.7221,valid loss: 19.4306\n",
      "[305/1000]  train loss: 2.7598,valid loss: 19.3388\n",
      "[306/1000]  train loss: 2.7131,valid loss: 19.8373\n",
      "[307/1000]  train loss: 2.8005,valid loss: 19.6271\n",
      "[308/1000]  train loss: 2.6159,valid loss: 19.6501\n",
      "[309/1000]  train loss: 2.7116,valid loss: 19.7362\n",
      "[310/1000]  train loss: 2.7604,valid loss: 19.8247\n",
      "[311/1000]  train loss: 2.7205,valid loss: 19.3963\n",
      "[312/1000]  train loss: 2.7288,valid loss: 19.5739\n",
      "[313/1000]  train loss: 2.8324,valid loss: 19.7627\n",
      "[314/1000]  train loss: 2.7637,valid loss: 19.7116\n",
      "[315/1000]  train loss: 2.7268,valid loss: 19.7235\n",
      "[316/1000]  train loss: 2.7018,valid loss: 19.6473\n",
      "[317/1000]  train loss: 2.6985,valid loss: 19.4388\n",
      "[318/1000]  train loss: 2.7364,valid loss: 19.5000\n",
      "[319/1000]  train loss: 2.6995,valid loss: 19.8904\n",
      "[320/1000]  train loss: 2.6674,valid loss: 19.6258\n",
      "[321/1000]  train loss: 2.6741,valid loss: 19.7653\n",
      "[322/1000]  train loss: 2.6902,valid loss: 20.2134\n",
      "[323/1000]  train loss: 2.7189,valid loss: 19.8781\n",
      "[324/1000]  train loss: 2.7241,valid loss: 19.8152\n",
      "[325/1000]  train loss: 2.6486,valid loss: 19.7030\n",
      "[326/1000]  train loss: 2.7637,valid loss: 19.5549\n",
      "[327/1000]  train loss: 2.6905,valid loss: 19.7109\n",
      "[328/1000]  train loss: 2.7360,valid loss: 19.3210\n",
      "[329/1000]  train loss: 2.7128,valid loss: 19.7601\n",
      "[330/1000]  train loss: 2.6968,valid loss: 19.9891\n",
      "[331/1000]  train loss: 2.5605,valid loss: 19.7162\n",
      "[332/1000]  train loss: 2.6874,valid loss: 19.9041\n",
      "[333/1000]  train loss: 2.6553,valid loss: 19.6472\n",
      "[334/1000]  train loss: 2.6724,valid loss: 19.5295\n",
      "[335/1000]  train loss: 2.6138,valid loss: 19.9981\n",
      "[336/1000]  train loss: 2.6607,valid loss: 19.7189\n",
      "[337/1000]  train loss: 2.6614,valid loss: 19.5571\n",
      "[338/1000]  train loss: 2.6687,valid loss: 20.0502\n",
      "[339/1000]  train loss: 2.7740,valid loss: 20.2861\n",
      "[340/1000]  train loss: 2.7301,valid loss: 19.7038\n",
      "[341/1000]  train loss: 2.6326,valid loss: 19.7428\n",
      "[342/1000]  train loss: 2.6478,valid loss: 19.7227\n",
      "[343/1000]  train loss: 2.6566,valid loss: 19.6420\n",
      "[344/1000]  train loss: 2.6722,valid loss: 20.0104\n",
      "[345/1000]  train loss: 2.6901,valid loss: 20.1963\n",
      "[346/1000]  train loss: 2.7380,valid loss: 19.9887\n",
      "[347/1000]  train loss: 2.6533,valid loss: 19.7261\n",
      "[348/1000]  train loss: 2.6443,valid loss: 19.7730\n",
      "[349/1000]  train loss: 2.6512,valid loss: 20.1540\n",
      "[350/1000]  train loss: 2.6443,valid loss: 19.6375\n",
      "[351/1000]  train loss: 2.6739,valid loss: 19.7018\n",
      "[352/1000]  train loss: 2.7407,valid loss: 19.4940\n",
      "[353/1000]  train loss: 2.6420,valid loss: 19.8693\n",
      "[354/1000]  train loss: 2.6181,valid loss: 19.8100\n",
      "[355/1000]  train loss: 2.6437,valid loss: 20.0558\n",
      "[356/1000]  train loss: 2.6799,valid loss: 20.0955\n",
      "[357/1000]  train loss: 2.6284,valid loss: 19.4982\n",
      "[358/1000]  train loss: 2.6322,valid loss: 20.2924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[359/1000]  train loss: 2.8700,valid loss: 19.8406\n",
      "[360/1000]  train loss: 2.7289,valid loss: 20.3658\n",
      "[361/1000]  train loss: 2.6308,valid loss: 19.8547\n",
      "[362/1000]  train loss: 2.6614,valid loss: 19.7527\n",
      "[363/1000]  train loss: 2.6008,valid loss: 20.0322\n",
      "[364/1000]  train loss: 2.5545,valid loss: 19.7372\n",
      "[365/1000]  train loss: 2.6167,valid loss: 19.6785\n",
      "[366/1000]  train loss: 2.6925,valid loss: 19.7181\n",
      "[367/1000]  train loss: 2.6776,valid loss: 19.6525\n",
      "[368/1000]  train loss: 2.5785,valid loss: 19.7701\n",
      "[369/1000]  train loss: 2.6178,valid loss: 19.7506\n",
      "[370/1000]  train loss: 2.6174,valid loss: 19.7645\n",
      "[371/1000]  train loss: 2.6278,valid loss: 20.1315\n",
      "[372/1000]  train loss: 2.6858,valid loss: 19.7699\n",
      "[373/1000]  train loss: 2.7149,valid loss: 20.1685\n",
      "[374/1000]  train loss: 2.6354,valid loss: 19.7272\n",
      "[375/1000]  train loss: 2.6668,valid loss: 19.7267\n",
      "[376/1000]  train loss: 2.6532,valid loss: 20.0194\n",
      "[377/1000]  train loss: 2.7267,valid loss: 20.3981\n",
      "[378/1000]  train loss: 2.6121,valid loss: 19.8875\n",
      "[379/1000]  train loss: 2.5995,valid loss: 20.1018\n",
      "[380/1000]  train loss: 2.6022,valid loss: 20.2112\n",
      "[381/1000]  train loss: 2.6470,valid loss: 20.2618\n",
      "[382/1000]  train loss: 2.5776,valid loss: 19.8214\n",
      "[383/1000]  train loss: 2.5363,valid loss: 19.9719\n",
      "[384/1000]  train loss: 2.5943,valid loss: 19.7903\n",
      "[385/1000]  train loss: 2.5946,valid loss: 20.1066\n",
      "[386/1000]  train loss: 2.5600,valid loss: 20.0544\n",
      "[387/1000]  train loss: 2.5674,valid loss: 19.9626\n",
      "[388/1000]  train loss: 2.6057,valid loss: 20.2086\n",
      "[389/1000]  train loss: 2.6747,valid loss: 20.4513\n",
      "[390/1000]  train loss: 2.6703,valid loss: 20.2060\n",
      "[391/1000]  train loss: 2.5739,valid loss: 19.8510\n",
      "[392/1000]  train loss: 2.5807,valid loss: 20.0506\n",
      "[393/1000]  train loss: 2.4796,valid loss: 19.9860\n",
      "[394/1000]  train loss: 2.5956,valid loss: 19.8127\n",
      "[395/1000]  train loss: 2.5618,valid loss: 20.3240\n",
      "[396/1000]  train loss: 2.6513,valid loss: 20.1100\n",
      "[397/1000]  train loss: 2.5869,valid loss: 20.3263\n",
      "[398/1000]  train loss: 2.5905,valid loss: 20.1348\n",
      "[399/1000]  train loss: 2.5700,valid loss: 19.8822\n",
      "[400/1000]  train loss: 2.5952,valid loss: 19.9022\n",
      "[401/1000]  train loss: 2.5401,valid loss: 20.1173\n",
      "[402/1000]  train loss: 2.5821,valid loss: 20.4213\n",
      "[403/1000]  train loss: 2.6340,valid loss: 19.9933\n",
      "[404/1000]  train loss: 2.7604,valid loss: 19.9937\n",
      "[405/1000]  train loss: 2.7611,valid loss: 19.8737\n",
      "[406/1000]  train loss: 2.6120,valid loss: 20.1361\n",
      "[407/1000]  train loss: 2.5426,valid loss: 19.9104\n",
      "[408/1000]  train loss: 2.5959,valid loss: 19.9319\n",
      "[409/1000]  train loss: 2.5559,valid loss: 20.1735\n",
      "[410/1000]  train loss: 2.5441,valid loss: 19.9950\n",
      "[411/1000]  train loss: 2.5142,valid loss: 20.1417\n",
      "[412/1000]  train loss: 2.5548,valid loss: 20.0147\n",
      "[413/1000]  train loss: 2.5387,valid loss: 20.4189\n",
      "[414/1000]  train loss: 2.5751,valid loss: 20.1690\n",
      "[415/1000]  train loss: 2.5289,valid loss: 20.1724\n",
      "[416/1000]  train loss: 2.5308,valid loss: 20.2430\n",
      "[417/1000]  train loss: 2.5535,valid loss: 20.2471\n",
      "[418/1000]  train loss: 2.5312,valid loss: 20.0096\n",
      "[419/1000]  train loss: 2.5525,valid loss: 20.1007\n",
      "[420/1000]  train loss: 2.5778,valid loss: 20.1966\n",
      "[421/1000]  train loss: 2.5956,valid loss: 19.9359\n",
      "[422/1000]  train loss: 2.5515,valid loss: 20.1382\n",
      "[423/1000]  train loss: 2.5224,valid loss: 20.4056\n",
      "[424/1000]  train loss: 2.5439,valid loss: 20.0711\n",
      "[425/1000]  train loss: 2.5210,valid loss: 20.5616\n",
      "[426/1000]  train loss: 2.5716,valid loss: 20.4324\n",
      "[427/1000]  train loss: 2.6834,valid loss: 20.5963\n",
      "[428/1000]  train loss: 2.5708,valid loss: 20.3179\n",
      "[429/1000]  train loss: 2.5571,valid loss: 20.5381\n",
      "[430/1000]  train loss: 2.5388,valid loss: 20.3299\n",
      "[431/1000]  train loss: 2.4891,valid loss: 20.3172\n",
      "[432/1000]  train loss: 2.6486,valid loss: 20.4977\n",
      "[433/1000]  train loss: 2.5842,valid loss: 20.1818\n",
      "[434/1000]  train loss: 2.4594,valid loss: 20.1639\n",
      "[435/1000]  train loss: 2.5481,valid loss: 20.5864\n",
      "[436/1000]  train loss: 2.4715,valid loss: 20.4988\n",
      "[437/1000]  train loss: 2.5196,valid loss: 20.3299\n",
      "[438/1000]  train loss: 2.5096,valid loss: 20.4370\n",
      "[439/1000]  train loss: 2.5336,valid loss: 20.0988\n",
      "[440/1000]  train loss: 2.4942,valid loss: 20.2454\n",
      "[441/1000]  train loss: 2.5393,valid loss: 20.6570\n",
      "[442/1000]  train loss: 2.6779,valid loss: 20.4340\n",
      "[443/1000]  train loss: 2.5335,valid loss: 20.0803\n",
      "[444/1000]  train loss: 2.5584,valid loss: 20.3816\n",
      "[445/1000]  train loss: 2.5294,valid loss: 20.3036\n",
      "[446/1000]  train loss: 2.5437,valid loss: 20.5764\n",
      "[447/1000]  train loss: 2.5336,valid loss: 20.1677\n",
      "[448/1000]  train loss: 2.4999,valid loss: 20.1723\n",
      "[449/1000]  train loss: 2.5134,valid loss: 20.1918\n",
      "[450/1000]  train loss: 2.4368,valid loss: 20.2515\n",
      "[451/1000]  train loss: 2.5076,valid loss: 20.4427\n",
      "[452/1000]  train loss: 2.4888,valid loss: 20.3289\n",
      "[453/1000]  train loss: 2.4994,valid loss: 20.3512\n",
      "[454/1000]  train loss: 2.5570,valid loss: 20.3124\n",
      "[455/1000]  train loss: 2.4405,valid loss: 20.6385\n",
      "[456/1000]  train loss: 2.4952,valid loss: 20.2681\n",
      "[457/1000]  train loss: 2.5037,valid loss: 20.5437\n",
      "[458/1000]  train loss: 2.4279,valid loss: 20.2467\n",
      "[459/1000]  train loss: 2.4775,valid loss: 20.1470\n",
      "[460/1000]  train loss: 2.5594,valid loss: 20.1369\n",
      "[461/1000]  train loss: 2.4598,valid loss: 20.2961\n",
      "[462/1000]  train loss: 2.4652,valid loss: 20.3786\n",
      "[463/1000]  train loss: 2.5104,valid loss: 20.0773\n",
      "[464/1000]  train loss: 2.6272,valid loss: 19.9254\n",
      "[465/1000]  train loss: 2.4782,valid loss: 20.4958\n",
      "[466/1000]  train loss: 2.5293,valid loss: 20.2448\n",
      "[467/1000]  train loss: 2.4897,valid loss: 20.3285\n",
      "[468/1000]  train loss: 2.4982,valid loss: 20.5093\n",
      "[469/1000]  train loss: 2.4305,valid loss: 20.1758\n",
      "[470/1000]  train loss: 2.5132,valid loss: 20.3269\n",
      "[471/1000]  train loss: 2.5763,valid loss: 19.9664\n",
      "[472/1000]  train loss: 2.4808,valid loss: 20.4711\n",
      "[473/1000]  train loss: 2.4347,valid loss: 20.3957\n",
      "[474/1000]  train loss: 2.4751,valid loss: 20.5143\n",
      "[475/1000]  train loss: 2.4824,valid loss: 20.2381\n",
      "[476/1000]  train loss: 2.4617,valid loss: 20.7313\n",
      "[477/1000]  train loss: 2.5955,valid loss: 20.6665\n",
      "[478/1000]  train loss: 2.4942,valid loss: 20.5061\n",
      "[479/1000]  train loss: 2.4774,valid loss: 20.2959\n",
      "[480/1000]  train loss: 2.4771,valid loss: 20.2925\n",
      "[481/1000]  train loss: 2.4489,valid loss: 20.2434\n",
      "[482/1000]  train loss: 2.4342,valid loss: 20.6601\n",
      "[483/1000]  train loss: 2.4471,valid loss: 20.2780\n",
      "[484/1000]  train loss: 2.4291,valid loss: 20.4229\n",
      "[485/1000]  train loss: 2.4358,valid loss: 20.3519\n",
      "[486/1000]  train loss: 2.4913,valid loss: 20.3348\n",
      "[487/1000]  train loss: 2.3751,valid loss: 20.6378\n",
      "[488/1000]  train loss: 2.5524,valid loss: 20.7525\n",
      "[489/1000]  train loss: 2.4940,valid loss: 20.4597\n",
      "[490/1000]  train loss: 2.3932,valid loss: 20.5636\n",
      "[491/1000]  train loss: 2.4274,valid loss: 20.2899\n",
      "[492/1000]  train loss: 2.4397,valid loss: 20.9546\n",
      "[493/1000]  train loss: 2.7089,valid loss: 20.8857\n",
      "[494/1000]  train loss: 2.4496,valid loss: 20.3837\n",
      "[495/1000]  train loss: 2.3974,valid loss: 20.7184\n",
      "[496/1000]  train loss: 2.4245,valid loss: 20.6201\n",
      "[497/1000]  train loss: 2.4527,valid loss: 20.5606\n",
      "[498/1000]  train loss: 2.4212,valid loss: 20.6279\n",
      "[499/1000]  train loss: 2.5382,valid loss: 20.8502\n",
      "[500/1000]  train loss: 2.5291,valid loss: 20.7066\n",
      "[501/1000]  train loss: 2.4249,valid loss: 20.5792\n",
      "[502/1000]  train loss: 2.4102,valid loss: 20.4787\n",
      "[503/1000]  train loss: 2.3746,valid loss: 20.4081\n",
      "[504/1000]  train loss: 2.3887,valid loss: 20.5429\n",
      "[505/1000]  train loss: 2.4376,valid loss: 20.8353\n",
      "[506/1000]  train loss: 2.3954,valid loss: 20.3286\n",
      "[507/1000]  train loss: 2.3968,valid loss: 20.5173\n",
      "[508/1000]  train loss: 2.4263,valid loss: 20.5487\n",
      "[509/1000]  train loss: 2.4204,valid loss: 20.7261\n",
      "[510/1000]  train loss: 2.4413,valid loss: 20.7888\n",
      "[511/1000]  train loss: 2.3921,valid loss: 20.5349\n",
      "[512/1000]  train loss: 2.4172,valid loss: 20.5427\n",
      "[513/1000]  train loss: 2.4051,valid loss: 20.3605\n",
      "[514/1000]  train loss: 2.3998,valid loss: 20.6506\n",
      "[515/1000]  train loss: 2.4035,valid loss: 20.5900\n",
      "[516/1000]  train loss: 2.3790,valid loss: 20.4079\n",
      "[517/1000]  train loss: 2.4153,valid loss: 20.2750\n",
      "[518/1000]  train loss: 2.3997,valid loss: 20.4062\n",
      "[519/1000]  train loss: 2.3773,valid loss: 20.7714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[520/1000]  train loss: 2.3892,valid loss: 20.4464\n",
      "[521/1000]  train loss: 2.4128,valid loss: 20.6798\n",
      "[522/1000]  train loss: 2.3921,valid loss: 20.4435\n",
      "[523/1000]  train loss: 2.3958,valid loss: 20.7096\n",
      "[524/1000]  train loss: 2.3890,valid loss: 20.4747\n",
      "[525/1000]  train loss: 2.3776,valid loss: 20.3636\n",
      "[526/1000]  train loss: 2.3646,valid loss: 20.9188\n",
      "[527/1000]  train loss: 2.3601,valid loss: 20.6361\n",
      "[528/1000]  train loss: 2.4439,valid loss: 20.6743\n",
      "[529/1000]  train loss: 2.3592,valid loss: 20.3677\n",
      "[530/1000]  train loss: 2.4202,valid loss: 20.6521\n",
      "[531/1000]  train loss: 2.3167,valid loss: 20.4343\n",
      "[532/1000]  train loss: 2.4294,valid loss: 20.9991\n",
      "[533/1000]  train loss: 2.4432,valid loss: 20.6696\n",
      "[534/1000]  train loss: 2.3883,valid loss: 20.8237\n",
      "[535/1000]  train loss: 2.6097,valid loss: 21.1081\n",
      "[536/1000]  train loss: 2.4209,valid loss: 20.5158\n",
      "[537/1000]  train loss: 2.4911,valid loss: 20.4574\n",
      "[538/1000]  train loss: 2.3890,valid loss: 20.5385\n",
      "[539/1000]  train loss: 2.5016,valid loss: 21.3512\n",
      "[540/1000]  train loss: 2.5127,valid loss: 20.7938\n",
      "[541/1000]  train loss: 2.3873,valid loss: 20.7605\n",
      "[542/1000]  train loss: 2.3997,valid loss: 20.2758\n",
      "[543/1000]  train loss: 2.3704,valid loss: 20.5735\n",
      "[544/1000]  train loss: 2.3490,valid loss: 20.7275\n",
      "[545/1000]  train loss: 2.3272,valid loss: 20.6075\n",
      "[546/1000]  train loss: 2.3755,valid loss: 20.7921\n",
      "[547/1000]  train loss: 2.4696,valid loss: 21.0932\n",
      "[548/1000]  train loss: 2.3946,valid loss: 20.4300\n",
      "[549/1000]  train loss: 2.3529,valid loss: 20.6716\n",
      "[550/1000]  train loss: 2.5200,valid loss: 20.2918\n",
      "[551/1000]  train loss: 2.4713,valid loss: 20.4417\n",
      "[552/1000]  train loss: 2.3497,valid loss: 20.5760\n",
      "[553/1000]  train loss: 2.3791,valid loss: 20.5650\n",
      "[554/1000]  train loss: 2.4157,valid loss: 20.7589\n",
      "[555/1000]  train loss: 2.2994,valid loss: 20.6745\n",
      "[556/1000]  train loss: 2.3416,valid loss: 20.6786\n",
      "[557/1000]  train loss: 2.3707,valid loss: 20.8025\n",
      "[558/1000]  train loss: 2.3439,valid loss: 20.5128\n",
      "[559/1000]  train loss: 2.3888,valid loss: 20.4317\n",
      "[560/1000]  train loss: 2.4353,valid loss: 20.4205\n",
      "[561/1000]  train loss: 2.3479,valid loss: 20.6030\n",
      "[562/1000]  train loss: 2.3429,valid loss: 20.7607\n",
      "[563/1000]  train loss: 2.3261,valid loss: 20.6002\n",
      "[564/1000]  train loss: 2.3201,valid loss: 20.7521\n",
      "[565/1000]  train loss: 2.3430,valid loss: 20.6971\n",
      "[566/1000]  train loss: 2.3374,valid loss: 20.5737\n",
      "[567/1000]  train loss: 2.3419,valid loss: 20.7536\n",
      "[568/1000]  train loss: 2.4426,valid loss: 20.5412\n",
      "[569/1000]  train loss: 2.3904,valid loss: 20.5654\n",
      "[570/1000]  train loss: 2.3388,valid loss: 21.0266\n",
      "[571/1000]  train loss: 2.3320,valid loss: 20.9944\n",
      "[572/1000]  train loss: 2.3265,valid loss: 20.5955\n",
      "[573/1000]  train loss: 2.3464,valid loss: 20.6687\n",
      "[574/1000]  train loss: 2.3284,valid loss: 21.1444\n",
      "[575/1000]  train loss: 2.2862,valid loss: 20.5859\n",
      "[576/1000]  train loss: 2.4066,valid loss: 20.9312\n",
      "[577/1000]  train loss: 2.3726,valid loss: 21.3475\n",
      "[578/1000]  train loss: 2.4091,valid loss: 20.8524\n",
      "[579/1000]  train loss: 2.3490,valid loss: 20.5830\n",
      "[580/1000]  train loss: 2.3262,valid loss: 20.8265\n",
      "[581/1000]  train loss: 2.3637,valid loss: 21.0837\n",
      "[582/1000]  train loss: 2.3595,valid loss: 20.5289\n",
      "[583/1000]  train loss: 2.4018,valid loss: 20.7840\n",
      "[584/1000]  train loss: 2.3757,valid loss: 20.8514\n",
      "[585/1000]  train loss: 2.2918,valid loss: 20.9535\n",
      "[586/1000]  train loss: 2.2762,valid loss: 20.8601\n",
      "[587/1000]  train loss: 2.3310,valid loss: 20.7873\n",
      "[588/1000]  train loss: 2.3229,valid loss: 21.0548\n",
      "[589/1000]  train loss: 2.3587,valid loss: 20.4536\n",
      "[590/1000]  train loss: 2.3889,valid loss: 20.8518\n",
      "[591/1000]  train loss: 2.2672,valid loss: 20.7484\n",
      "[592/1000]  train loss: 2.2954,valid loss: 20.7580\n",
      "[593/1000]  train loss: 2.3128,valid loss: 20.9708\n",
      "[594/1000]  train loss: 2.3285,valid loss: 20.9349\n",
      "[595/1000]  train loss: 2.2981,valid loss: 20.7956\n",
      "[596/1000]  train loss: 2.2167,valid loss: 20.6141\n",
      "[597/1000]  train loss: 2.3092,valid loss: 21.0763\n",
      "[598/1000]  train loss: 2.3271,valid loss: 20.6439\n",
      "[599/1000]  train loss: 2.3286,valid loss: 20.7527\n",
      "[600/1000]  train loss: 2.2885,valid loss: 20.6437\n",
      "[601/1000]  train loss: 2.2891,valid loss: 20.8853\n",
      "[602/1000]  train loss: 2.2949,valid loss: 21.0221\n",
      "[603/1000]  train loss: 2.2899,valid loss: 20.6876\n",
      "[604/1000]  train loss: 2.2680,valid loss: 21.0426\n",
      "[605/1000]  train loss: 2.2951,valid loss: 20.7223\n",
      "[606/1000]  train loss: 2.2727,valid loss: 20.8439\n",
      "[607/1000]  train loss: 2.2515,valid loss: 20.6915\n",
      "[608/1000]  train loss: 2.3290,valid loss: 20.7886\n",
      "[609/1000]  train loss: 2.3540,valid loss: 20.8804\n",
      "[610/1000]  train loss: 2.3134,valid loss: 20.9125\n",
      "[611/1000]  train loss: 2.2587,valid loss: 20.8509\n",
      "[612/1000]  train loss: 2.3047,valid loss: 20.6860\n",
      "[613/1000]  train loss: 2.4705,valid loss: 20.7424\n",
      "[614/1000]  train loss: 2.3492,valid loss: 20.7345\n",
      "[615/1000]  train loss: 2.2666,valid loss: 21.2014\n",
      "[616/1000]  train loss: 2.3439,valid loss: 20.9629\n",
      "[617/1000]  train loss: 2.2552,valid loss: 20.8364\n",
      "[618/1000]  train loss: 2.2809,valid loss: 21.1618\n",
      "[619/1000]  train loss: 2.3055,valid loss: 21.0584\n",
      "[620/1000]  train loss: 2.4526,valid loss: 21.3978\n",
      "[621/1000]  train loss: 2.2829,valid loss: 20.7834\n",
      "[622/1000]  train loss: 2.2630,valid loss: 21.1170\n",
      "[623/1000]  train loss: 2.3410,valid loss: 20.9372\n",
      "[624/1000]  train loss: 2.3054,valid loss: 21.0224\n",
      "[625/1000]  train loss: 2.3105,valid loss: 20.7031\n",
      "[626/1000]  train loss: 2.2617,valid loss: 21.0464\n",
      "[627/1000]  train loss: 2.3684,valid loss: 20.9740\n",
      "[628/1000]  train loss: 2.4205,valid loss: 20.8976\n",
      "[629/1000]  train loss: 2.2678,valid loss: 20.8903\n",
      "[630/1000]  train loss: 2.2820,valid loss: 20.9199\n",
      "[631/1000]  train loss: 2.2418,valid loss: 21.1292\n",
      "[632/1000]  train loss: 2.2558,valid loss: 21.0976\n",
      "[633/1000]  train loss: 2.2733,valid loss: 21.2226\n",
      "[634/1000]  train loss: 2.2614,valid loss: 21.0094\n",
      "[635/1000]  train loss: 2.3456,valid loss: 20.6837\n",
      "[636/1000]  train loss: 2.3480,valid loss: 21.0923\n",
      "[637/1000]  train loss: 2.2405,valid loss: 21.0131\n",
      "[638/1000]  train loss: 2.2490,valid loss: 20.9686\n",
      "[639/1000]  train loss: 2.3347,valid loss: 21.1515\n",
      "[640/1000]  train loss: 2.3479,valid loss: 21.0972\n",
      "[641/1000]  train loss: 2.2962,valid loss: 21.0479\n",
      "[642/1000]  train loss: 2.3147,valid loss: 20.9999\n",
      "[643/1000]  train loss: 2.2751,valid loss: 21.1868\n",
      "[644/1000]  train loss: 2.2400,valid loss: 21.2602\n",
      "[645/1000]  train loss: 2.2538,valid loss: 21.2339\n",
      "[646/1000]  train loss: 2.2891,valid loss: 21.4414\n",
      "[647/1000]  train loss: 2.2694,valid loss: 21.1558\n",
      "[648/1000]  train loss: 2.2163,valid loss: 21.0950\n",
      "[649/1000]  train loss: 2.2497,valid loss: 20.8753\n",
      "[650/1000]  train loss: 2.3476,valid loss: 21.2298\n",
      "[651/1000]  train loss: 2.3392,valid loss: 21.1183\n",
      "[652/1000]  train loss: 2.2370,valid loss: 20.9982\n",
      "[653/1000]  train loss: 2.2678,valid loss: 21.0613\n",
      "[654/1000]  train loss: 2.2869,valid loss: 21.3055\n",
      "[655/1000]  train loss: 2.2811,valid loss: 21.3928\n",
      "[656/1000]  train loss: 2.2565,valid loss: 20.8397\n",
      "[657/1000]  train loss: 2.2369,valid loss: 21.2026\n",
      "[658/1000]  train loss: 2.1082,valid loss: 20.9255\n",
      "[659/1000]  train loss: 2.2119,valid loss: 21.0720\n",
      "[660/1000]  train loss: 2.2996,valid loss: 21.2582\n",
      "[661/1000]  train loss: 2.2800,valid loss: 20.8794\n",
      "[662/1000]  train loss: 2.4874,valid loss: 21.1378\n",
      "[663/1000]  train loss: 2.2993,valid loss: 21.4328\n",
      "[664/1000]  train loss: 2.2304,valid loss: 21.1048\n",
      "[665/1000]  train loss: 2.2051,valid loss: 20.8943\n",
      "[666/1000]  train loss: 2.2162,valid loss: 21.1512\n",
      "[667/1000]  train loss: 2.2010,valid loss: 21.2362\n",
      "[668/1000]  train loss: 2.3473,valid loss: 21.6083\n",
      "[669/1000]  train loss: 2.2554,valid loss: 20.9340\n",
      "[670/1000]  train loss: 2.2311,valid loss: 21.0160\n",
      "[671/1000]  train loss: 2.2507,valid loss: 21.2703\n",
      "[672/1000]  train loss: 2.1867,valid loss: 21.1842\n",
      "[673/1000]  train loss: 2.2115,valid loss: 21.0827\n",
      "[674/1000]  train loss: 2.1911,valid loss: 21.1951\n",
      "[675/1000]  train loss: 2.2127,valid loss: 21.4217\n",
      "[676/1000]  train loss: 2.1941,valid loss: 21.1555\n",
      "[677/1000]  train loss: 2.2343,valid loss: 21.5063\n",
      "[678/1000]  train loss: 2.2124,valid loss: 21.0408\n",
      "[679/1000]  train loss: 2.2319,valid loss: 21.1355\n",
      "[680/1000]  train loss: 2.1555,valid loss: 21.0915\n",
      "[681/1000]  train loss: 2.2097,valid loss: 21.1223\n",
      "[682/1000]  train loss: 2.1359,valid loss: 21.2125\n",
      "[683/1000]  train loss: 2.1555,valid loss: 21.4969\n",
      "[684/1000]  train loss: 2.2278,valid loss: 21.1407\n",
      "[685/1000]  train loss: 2.1788,valid loss: 21.1779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[686/1000]  train loss: 2.2004,valid loss: 21.0554\n",
      "[687/1000]  train loss: 2.2152,valid loss: 21.8943\n",
      "[688/1000]  train loss: 2.3393,valid loss: 21.4705\n",
      "[689/1000]  train loss: 2.1299,valid loss: 21.3549\n",
      "[690/1000]  train loss: 2.1777,valid loss: 20.8482\n",
      "[691/1000]  train loss: 2.1772,valid loss: 21.5893\n",
      "[692/1000]  train loss: 2.1837,valid loss: 21.0855\n",
      "[693/1000]  train loss: 2.1704,valid loss: 21.1740\n",
      "[694/1000]  train loss: 2.1769,valid loss: 21.3657\n",
      "[695/1000]  train loss: 2.1803,valid loss: 21.2770\n",
      "[696/1000]  train loss: 2.2709,valid loss: 20.9118\n",
      "[697/1000]  train loss: 2.2669,valid loss: 21.1327\n",
      "[698/1000]  train loss: 2.1729,valid loss: 21.2697\n",
      "[699/1000]  train loss: 2.1488,valid loss: 21.4179\n",
      "[700/1000]  train loss: 2.1725,valid loss: 21.3860\n",
      "[701/1000]  train loss: 2.1844,valid loss: 21.3324\n",
      "[702/1000]  train loss: 2.2730,valid loss: 21.4076\n",
      "[703/1000]  train loss: 2.2229,valid loss: 21.6590\n",
      "[704/1000]  train loss: 2.1833,valid loss: 21.2557\n",
      "[705/1000]  train loss: 2.1963,valid loss: 21.2718\n",
      "[706/1000]  train loss: 2.1384,valid loss: 21.3916\n",
      "[707/1000]  train loss: 2.1820,valid loss: 21.3289\n",
      "[708/1000]  train loss: 2.1605,valid loss: 21.3088\n",
      "[709/1000]  train loss: 2.1829,valid loss: 21.2271\n",
      "[710/1000]  train loss: 2.2046,valid loss: 21.3996\n",
      "[711/1000]  train loss: 2.1746,valid loss: 21.1761\n",
      "[712/1000]  train loss: 2.1436,valid loss: 21.4019\n",
      "[713/1000]  train loss: 2.1673,valid loss: 21.4155\n",
      "[714/1000]  train loss: 2.2206,valid loss: 21.9725\n",
      "[715/1000]  train loss: 2.2048,valid loss: 21.4580\n",
      "[716/1000]  train loss: 2.1545,valid loss: 21.0934\n",
      "[717/1000]  train loss: 2.2846,valid loss: 21.1086\n",
      "[718/1000]  train loss: 2.1871,valid loss: 21.4011\n",
      "[719/1000]  train loss: 2.1422,valid loss: 21.0808\n",
      "[720/1000]  train loss: 2.1906,valid loss: 21.8519\n",
      "[721/1000]  train loss: 2.1754,valid loss: 21.0923\n",
      "[722/1000]  train loss: 2.1527,valid loss: 21.3763\n",
      "[723/1000]  train loss: 2.2288,valid loss: 21.7023\n",
      "[724/1000]  train loss: 2.2787,valid loss: 21.5473\n",
      "[725/1000]  train loss: 2.1115,valid loss: 21.3777\n",
      "[726/1000]  train loss: 2.1474,valid loss: 21.5921\n",
      "[727/1000]  train loss: 2.1764,valid loss: 21.6739\n",
      "[728/1000]  train loss: 2.1802,valid loss: 21.6088\n",
      "[729/1000]  train loss: 2.2009,valid loss: 21.3698\n",
      "[730/1000]  train loss: 2.2089,valid loss: 21.7042\n",
      "[731/1000]  train loss: 2.1771,valid loss: 21.4039\n",
      "[732/1000]  train loss: 2.1138,valid loss: 21.5019\n",
      "[733/1000]  train loss: 2.1398,valid loss: 21.4468\n",
      "[734/1000]  train loss: 2.1614,valid loss: 21.6632\n",
      "[735/1000]  train loss: 2.1489,valid loss: 21.4404\n",
      "[736/1000]  train loss: 2.1351,valid loss: 21.3196\n",
      "[737/1000]  train loss: 2.1260,valid loss: 21.6604\n",
      "[738/1000]  train loss: 2.1395,valid loss: 21.2070\n",
      "[739/1000]  train loss: 2.1449,valid loss: 21.4959\n",
      "[740/1000]  train loss: 2.1812,valid loss: 21.8271\n",
      "[741/1000]  train loss: 2.1812,valid loss: 21.5198\n",
      "[742/1000]  train loss: 2.3041,valid loss: 21.2467\n",
      "[743/1000]  train loss: 2.2678,valid loss: 21.1031\n",
      "[744/1000]  train loss: 2.1369,valid loss: 21.5694\n",
      "[745/1000]  train loss: 2.1218,valid loss: 21.5065\n",
      "[746/1000]  train loss: 2.1143,valid loss: 21.6281\n",
      "[747/1000]  train loss: 1.9841,valid loss: 21.2964\n",
      "[748/1000]  train loss: 2.1009,valid loss: 21.6644\n",
      "[749/1000]  train loss: 2.1299,valid loss: 21.4131\n",
      "[750/1000]  train loss: 2.1367,valid loss: 21.5258\n",
      "[751/1000]  train loss: 2.1332,valid loss: 21.7123\n",
      "[752/1000]  train loss: 2.1497,valid loss: 21.3534\n",
      "[753/1000]  train loss: 2.1803,valid loss: 21.4130\n",
      "[754/1000]  train loss: 2.1537,valid loss: 21.3602\n",
      "[755/1000]  train loss: 2.1091,valid loss: 21.5163\n",
      "[756/1000]  train loss: 2.1085,valid loss: 21.2793\n",
      "[757/1000]  train loss: 2.1032,valid loss: 21.7160\n",
      "[758/1000]  train loss: 2.1157,valid loss: 21.7288\n",
      "[759/1000]  train loss: 2.1061,valid loss: 21.3346\n",
      "[760/1000]  train loss: 2.1044,valid loss: 21.6811\n",
      "[761/1000]  train loss: 2.1145,valid loss: 21.5833\n",
      "[762/1000]  train loss: 2.1302,valid loss: 21.5077\n",
      "[763/1000]  train loss: 2.1420,valid loss: 21.7072\n",
      "[764/1000]  train loss: 2.1598,valid loss: 21.7317\n",
      "[765/1000]  train loss: 2.1638,valid loss: 21.7850\n",
      "[766/1000]  train loss: 2.1691,valid loss: 21.7247\n",
      "[767/1000]  train loss: 2.2284,valid loss: 21.6927\n",
      "[768/1000]  train loss: 2.2581,valid loss: 21.7138\n",
      "[769/1000]  train loss: 2.0867,valid loss: 21.4593\n",
      "[770/1000]  train loss: 2.1016,valid loss: 21.6739\n",
      "[771/1000]  train loss: 2.1041,valid loss: 21.5543\n",
      "[772/1000]  train loss: 2.0950,valid loss: 21.6276\n",
      "[773/1000]  train loss: 2.1444,valid loss: 21.6688\n",
      "[774/1000]  train loss: 2.1789,valid loss: 21.5332\n",
      "[775/1000]  train loss: 2.1232,valid loss: 21.6843\n",
      "[776/1000]  train loss: 2.1138,valid loss: 21.1803\n",
      "[777/1000]  train loss: 2.1372,valid loss: 21.4860\n",
      "[778/1000]  train loss: 2.0731,valid loss: 21.5522\n",
      "[779/1000]  train loss: 2.0933,valid loss: 21.6596\n",
      "[780/1000]  train loss: 2.0861,valid loss: 21.7124\n",
      "[781/1000]  train loss: 2.1056,valid loss: 21.7567\n",
      "[782/1000]  train loss: 2.1316,valid loss: 21.7902\n",
      "[783/1000]  train loss: 2.1762,valid loss: 22.1540\n",
      "[784/1000]  train loss: 2.1319,valid loss: 21.8210\n",
      "[785/1000]  train loss: 2.0872,valid loss: 21.7765\n",
      "[786/1000]  train loss: 2.0595,valid loss: 21.4872\n",
      "[787/1000]  train loss: 2.0535,valid loss: 21.5275\n",
      "[788/1000]  train loss: 2.0429,valid loss: 21.8553\n",
      "[789/1000]  train loss: 2.0946,valid loss: 21.4048\n",
      "[790/1000]  train loss: 2.0108,valid loss: 21.6756\n",
      "[791/1000]  train loss: 2.1064,valid loss: 21.4765\n",
      "[792/1000]  train loss: 2.0550,valid loss: 21.6016\n",
      "[793/1000]  train loss: 2.0584,valid loss: 21.5143\n",
      "[794/1000]  train loss: 2.0956,valid loss: 21.4976\n",
      "[795/1000]  train loss: 2.0910,valid loss: 21.5536\n",
      "[796/1000]  train loss: 2.1115,valid loss: 21.7722\n",
      "[797/1000]  train loss: 2.0623,valid loss: 21.5009\n",
      "[798/1000]  train loss: 2.0509,valid loss: 21.5227\n",
      "[799/1000]  train loss: 2.1262,valid loss: 21.7362\n",
      "[800/1000]  train loss: 2.1300,valid loss: 21.9710\n",
      "[801/1000]  train loss: 2.0747,valid loss: 21.6354\n",
      "[802/1000]  train loss: 2.0383,valid loss: 21.6961\n",
      "[803/1000]  train loss: 2.0729,valid loss: 21.5689\n",
      "[804/1000]  train loss: 2.0469,valid loss: 21.4555\n",
      "[805/1000]  train loss: 2.0654,valid loss: 21.5527\n",
      "[806/1000]  train loss: 2.0329,valid loss: 21.7959\n",
      "[807/1000]  train loss: 2.0425,valid loss: 21.4088\n",
      "[808/1000]  train loss: 2.0564,valid loss: 21.8329\n",
      "[809/1000]  train loss: 2.0826,valid loss: 21.4135\n",
      "[810/1000]  train loss: 2.1564,valid loss: 21.5735\n",
      "[811/1000]  train loss: 2.0609,valid loss: 21.9127\n",
      "[812/1000]  train loss: 1.9858,valid loss: 21.5292\n",
      "[813/1000]  train loss: 2.0349,valid loss: 21.7736\n",
      "[814/1000]  train loss: 2.0498,valid loss: 21.2727\n",
      "[815/1000]  train loss: 2.1924,valid loss: 21.2319\n",
      "[816/1000]  train loss: 2.0973,valid loss: 21.4289\n",
      "[817/1000]  train loss: 2.0501,valid loss: 21.7953\n",
      "[818/1000]  train loss: 2.0384,valid loss: 21.7290\n",
      "[819/1000]  train loss: 2.0776,valid loss: 21.7913\n",
      "[820/1000]  train loss: 2.0536,valid loss: 22.0572\n",
      "[821/1000]  train loss: 2.0896,valid loss: 21.8365\n",
      "[822/1000]  train loss: 2.0978,valid loss: 21.9346\n",
      "[823/1000]  train loss: 2.0421,valid loss: 21.4044\n",
      "[824/1000]  train loss: 1.9824,valid loss: 21.7135\n",
      "[825/1000]  train loss: 1.9743,valid loss: 21.8415\n",
      "[826/1000]  train loss: 2.0342,valid loss: 21.8247\n",
      "[827/1000]  train loss: 2.0294,valid loss: 21.7271\n",
      "[828/1000]  train loss: 2.0306,valid loss: 21.5400\n",
      "[829/1000]  train loss: 2.0048,valid loss: 21.5217\n",
      "[830/1000]  train loss: 2.0831,valid loss: 21.4935\n",
      "[831/1000]  train loss: 2.0388,valid loss: 21.8816\n",
      "[832/1000]  train loss: 2.0125,valid loss: 21.5321\n",
      "[833/1000]  train loss: 2.0284,valid loss: 21.5093\n",
      "[834/1000]  train loss: 2.0091,valid loss: 21.6503\n",
      "[835/1000]  train loss: 2.0213,valid loss: 21.7636\n",
      "[836/1000]  train loss: 2.1313,valid loss: 21.5215\n",
      "[837/1000]  train loss: 2.1368,valid loss: 21.5813\n",
      "[838/1000]  train loss: 2.0750,valid loss: 21.6445\n",
      "[839/1000]  train loss: 2.2383,valid loss: 21.5535\n",
      "[840/1000]  train loss: 2.0786,valid loss: 21.3681\n",
      "[841/1000]  train loss: 2.0430,valid loss: 21.6271\n",
      "[842/1000]  train loss: 2.0356,valid loss: 21.6129\n",
      "[843/1000]  train loss: 2.0139,valid loss: 21.8690\n",
      "[844/1000]  train loss: 2.0312,valid loss: 21.8118\n",
      "[845/1000]  train loss: 2.1026,valid loss: 22.0802\n",
      "[846/1000]  train loss: 2.0132,valid loss: 21.9580\n",
      "[847/1000]  train loss: 1.9955,valid loss: 21.5324\n",
      "[848/1000]  train loss: 2.1506,valid loss: 22.0613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[849/1000]  train loss: 2.0318,valid loss: 21.8177\n",
      "[850/1000]  train loss: 1.9960,valid loss: 22.0223\n",
      "[851/1000]  train loss: 2.0274,valid loss: 21.7848\n",
      "[852/1000]  train loss: 1.9886,valid loss: 22.0749\n",
      "[853/1000]  train loss: 1.9988,valid loss: 21.4724\n",
      "[854/1000]  train loss: 2.0626,valid loss: 22.0061\n",
      "[855/1000]  train loss: 2.0367,valid loss: 21.8161\n",
      "[856/1000]  train loss: 2.0002,valid loss: 21.5526\n",
      "[857/1000]  train loss: 2.0324,valid loss: 21.6133\n",
      "[858/1000]  train loss: 2.0952,valid loss: 21.4744\n",
      "[859/1000]  train loss: 1.9728,valid loss: 22.0322\n",
      "[860/1000]  train loss: 2.0715,valid loss: 21.5733\n",
      "[861/1000]  train loss: 1.9938,valid loss: 21.7336\n",
      "[862/1000]  train loss: 2.0107,valid loss: 21.7084\n",
      "[863/1000]  train loss: 1.9606,valid loss: 21.8799\n",
      "[864/1000]  train loss: 2.0738,valid loss: 21.6873\n",
      "[865/1000]  train loss: 2.0335,valid loss: 21.6242\n",
      "[866/1000]  train loss: 1.9333,valid loss: 21.7256\n",
      "[867/1000]  train loss: 2.0077,valid loss: 21.7020\n",
      "[868/1000]  train loss: 2.0070,valid loss: 21.7055\n",
      "[869/1000]  train loss: 1.9854,valid loss: 21.7234\n",
      "[870/1000]  train loss: 1.9932,valid loss: 21.5733\n",
      "[871/1000]  train loss: 1.9837,valid loss: 21.6670\n",
      "[872/1000]  train loss: 1.9821,valid loss: 21.7225\n",
      "[873/1000]  train loss: 1.9291,valid loss: 21.9977\n",
      "[874/1000]  train loss: 2.0252,valid loss: 21.8583\n",
      "[875/1000]  train loss: 2.1494,valid loss: 22.1481\n",
      "[876/1000]  train loss: 2.1606,valid loss: 21.8792\n",
      "[877/1000]  train loss: 2.0306,valid loss: 21.8429\n",
      "[878/1000]  train loss: 1.9790,valid loss: 21.6745\n",
      "[879/1000]  train loss: 1.9804,valid loss: 21.7864\n",
      "[880/1000]  train loss: 1.9835,valid loss: 21.7949\n",
      "[881/1000]  train loss: 1.9886,valid loss: 21.8294\n",
      "[882/1000]  train loss: 1.9961,valid loss: 22.0893\n",
      "[883/1000]  train loss: 1.9667,valid loss: 21.7450\n",
      "[884/1000]  train loss: 1.9472,valid loss: 22.4374\n",
      "[885/1000]  train loss: 1.9992,valid loss: 21.7254\n",
      "[886/1000]  train loss: 1.9848,valid loss: 21.8899\n",
      "[887/1000]  train loss: 1.9676,valid loss: 21.8297\n",
      "[888/1000]  train loss: 1.9661,valid loss: 21.9614\n",
      "[889/1000]  train loss: 1.9683,valid loss: 22.1337\n",
      "[890/1000]  train loss: 2.0368,valid loss: 22.1486\n",
      "[891/1000]  train loss: 2.0469,valid loss: 22.2245\n",
      "[892/1000]  train loss: 1.9758,valid loss: 21.9167\n",
      "[893/1000]  train loss: 1.9804,valid loss: 21.5504\n",
      "[894/1000]  train loss: 2.0063,valid loss: 21.9605\n",
      "[895/1000]  train loss: 2.0157,valid loss: 22.0124\n",
      "[896/1000]  train loss: 1.9402,valid loss: 22.1111\n",
      "[897/1000]  train loss: 1.9557,valid loss: 21.8112\n",
      "[898/1000]  train loss: 1.9923,valid loss: 21.7762\n",
      "[899/1000]  train loss: 1.9679,valid loss: 22.1938\n",
      "[900/1000]  train loss: 1.9509,valid loss: 21.8581\n",
      "[901/1000]  train loss: 1.9623,valid loss: 21.7298\n",
      "[902/1000]  train loss: 1.9655,valid loss: 22.1449\n",
      "[903/1000]  train loss: 1.9236,valid loss: 21.8329\n",
      "[904/1000]  train loss: 1.9926,valid loss: 21.8057\n",
      "[905/1000]  train loss: 1.9479,valid loss: 21.8894\n",
      "[906/1000]  train loss: 1.9317,valid loss: 22.0723\n",
      "[907/1000]  train loss: 1.9391,valid loss: 22.2216\n",
      "[908/1000]  train loss: 1.9915,valid loss: 22.2082\n",
      "[909/1000]  train loss: 2.0605,valid loss: 22.0654\n",
      "[910/1000]  train loss: 2.0027,valid loss: 22.3293\n",
      "[911/1000]  train loss: 2.1011,valid loss: 22.4766\n",
      "[912/1000]  train loss: 1.9538,valid loss: 22.0963\n",
      "[913/1000]  train loss: 1.9386,valid loss: 22.0696\n",
      "[914/1000]  train loss: 1.9475,valid loss: 21.8967\n",
      "[915/1000]  train loss: 1.9481,valid loss: 21.9222\n",
      "[916/1000]  train loss: 1.9933,valid loss: 21.7290\n",
      "[917/1000]  train loss: 1.9608,valid loss: 22.0035\n",
      "[918/1000]  train loss: 1.9364,valid loss: 21.8761\n",
      "[919/1000]  train loss: 2.1171,valid loss: 22.7036\n",
      "[920/1000]  train loss: 2.0933,valid loss: 22.0763\n",
      "[921/1000]  train loss: 1.8967,valid loss: 22.1338\n",
      "[922/1000]  train loss: 1.9795,valid loss: 21.8260\n",
      "[923/1000]  train loss: 1.9493,valid loss: 22.3587\n",
      "[924/1000]  train loss: 1.9756,valid loss: 21.9740\n",
      "[925/1000]  train loss: 1.9547,valid loss: 21.8938\n",
      "[926/1000]  train loss: 1.9684,valid loss: 22.0337\n",
      "[927/1000]  train loss: 1.8956,valid loss: 21.9907\n",
      "[928/1000]  train loss: 1.9323,valid loss: 21.8950\n",
      "[929/1000]  train loss: 1.9341,valid loss: 21.9816\n",
      "[930/1000]  train loss: 1.9320,valid loss: 22.0543\n",
      "[931/1000]  train loss: 1.9282,valid loss: 21.8434\n",
      "[932/1000]  train loss: 1.9312,valid loss: 21.8404\n",
      "[933/1000]  train loss: 1.9287,valid loss: 22.0423\n",
      "[934/1000]  train loss: 1.9258,valid loss: 21.9872\n",
      "[935/1000]  train loss: 1.9596,valid loss: 22.1510\n",
      "[936/1000]  train loss: 1.9890,valid loss: 22.3325\n",
      "[937/1000]  train loss: 2.0394,valid loss: 22.2907\n",
      "[938/1000]  train loss: 2.0534,valid loss: 22.0260\n",
      "[939/1000]  train loss: 1.9349,valid loss: 22.2397\n",
      "[940/1000]  train loss: 1.9438,valid loss: 22.2485\n",
      "[941/1000]  train loss: 1.9327,valid loss: 21.7211\n",
      "[942/1000]  train loss: 1.9860,valid loss: 21.7238\n",
      "[943/1000]  train loss: 1.9624,valid loss: 21.9314\n",
      "[944/1000]  train loss: 1.9081,valid loss: 22.1105\n",
      "[945/1000]  train loss: 1.8884,valid loss: 21.8419\n",
      "[946/1000]  train loss: 1.9440,valid loss: 22.1965\n",
      "[947/1000]  train loss: 1.9038,valid loss: 22.2373\n",
      "[948/1000]  train loss: 1.9181,valid loss: 21.8754\n",
      "[949/1000]  train loss: 1.9633,valid loss: 21.9732\n",
      "[950/1000]  train loss: 1.9006,valid loss: 22.1405\n",
      "[951/1000]  train loss: 1.9224,valid loss: 22.4825\n",
      "[952/1000]  train loss: 1.9108,valid loss: 21.9663\n",
      "[953/1000]  train loss: 1.8894,valid loss: 22.2105\n",
      "[954/1000]  train loss: 1.9977,valid loss: 22.4156\n",
      "[955/1000]  train loss: 1.8779,valid loss: 21.9533\n",
      "[956/1000]  train loss: 1.9332,valid loss: 22.4923\n",
      "[957/1000]  train loss: 2.0280,valid loss: 22.4137\n",
      "[958/1000]  train loss: 1.9821,valid loss: 22.5278\n",
      "[959/1000]  train loss: 1.9209,valid loss: 21.9542\n",
      "[960/1000]  train loss: 2.0447,valid loss: 22.7283\n",
      "[961/1000]  train loss: 2.0706,valid loss: 22.1941\n",
      "[962/1000]  train loss: 1.9219,valid loss: 22.1858\n",
      "[963/1000]  train loss: 1.8802,valid loss: 22.0257\n",
      "[964/1000]  train loss: 1.8539,valid loss: 22.1341\n",
      "[965/1000]  train loss: 1.8919,valid loss: 22.4314\n",
      "[966/1000]  train loss: 1.8632,valid loss: 21.8865\n",
      "[967/1000]  train loss: 1.8806,valid loss: 22.3621\n",
      "[968/1000]  train loss: 1.9527,valid loss: 22.0165\n",
      "[969/1000]  train loss: 2.0809,valid loss: 22.0090\n",
      "[970/1000]  train loss: 1.9429,valid loss: 22.0793\n",
      "[971/1000]  train loss: 1.8898,valid loss: 22.2631\n",
      "[972/1000]  train loss: 1.8910,valid loss: 22.2203\n",
      "[973/1000]  train loss: 1.9160,valid loss: 22.2948\n",
      "[974/1000]  train loss: 1.9449,valid loss: 22.4021\n",
      "[975/1000]  train loss: 1.8994,valid loss: 22.2210\n",
      "[976/1000]  train loss: 1.8809,valid loss: 22.1904\n",
      "[977/1000]  train loss: 1.8911,valid loss: 22.3680\n",
      "[978/1000]  train loss: 1.9168,valid loss: 22.2692\n",
      "[979/1000]  train loss: 1.8820,valid loss: 22.2335\n",
      "[980/1000]  train loss: 1.8930,valid loss: 22.2954\n",
      "[981/1000]  train loss: 1.9201,valid loss: 22.6258\n",
      "[982/1000]  train loss: 1.8730,valid loss: 22.0315\n",
      "[983/1000]  train loss: 1.8787,valid loss: 22.0986\n",
      "[984/1000]  train loss: 1.8767,valid loss: 22.0988\n",
      "[985/1000]  train loss: 1.9279,valid loss: 22.3577\n",
      "[986/1000]  train loss: 2.2255,valid loss: 21.9368\n",
      "[987/1000]  train loss: 2.1085,valid loss: 22.0811\n",
      "[988/1000]  train loss: 1.8838,valid loss: 22.0210\n",
      "[989/1000]  train loss: 1.8781,valid loss: 22.0644\n",
      "[990/1000]  train loss: 1.8531,valid loss: 22.0563\n",
      "[991/1000]  train loss: 1.9276,valid loss: 22.4231\n",
      "[992/1000]  train loss: 2.1097,valid loss: 22.6926\n",
      "[993/1000]  train loss: 2.0926,valid loss: 22.1344\n",
      "[994/1000]  train loss: 1.8905,valid loss: 22.0455\n",
      "[995/1000]  train loss: 1.8807,valid loss: 22.1226\n",
      "[996/1000]  train loss: 1.8608,valid loss: 22.1393\n",
      "[997/1000]  train loss: 1.8633,valid loss: 22.3566\n",
      "[998/1000]  train loss: 1.8827,valid loss: 22.3846\n",
      "[999/1000]  train loss: 1.8735,valid loss: 22.2990\n",
      "[1000/1000]  train loss: 1.8609,valid loss: 22.2795\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "##train(학습/훈련)\n",
    "## 두 단계 ->Train+Validation => step 별로 train => epoch 별로 검증한다.\n",
    "\n",
    "\n",
    "s=time.time() #시간을 알려주는 함수이다.\n",
    "\n",
    "for epoch in range(N_EPOCH):\n",
    "    \n",
    "    # 한 epoch에 대한 train 코드\n",
    "    \n",
    "    ########################################\n",
    "    # train - 모델을 train mode로 변경한다.\n",
    "    ########################################\n",
    "    boston_model.train() #train 모드로 변경\n",
    "    train_loss = 0.0 #현재 epoch의 train_loss를 저장할 변수\n",
    "\n",
    "    ### batch 단위로 학습 =>step\n",
    "    for X,y in boston_trainloader:\n",
    "        # 한 step에 대한 train 코드\n",
    "        # 1. X,y를 device로 옮긴다. =>모델과 동일한 device에 위치시켜야 한다.\n",
    "        X,y =X.to(device), y.to(device)\n",
    "        #2.모델 추정(예측) =>forward propagation\n",
    "        pred = boston_model(X)\n",
    "        \n",
    "        \n",
    "        #3. loss 계산,파라미터 초기화\n",
    "        loss = loss_fn(pred,y) #오차 계산 ->grad_fn \n",
    "        optimizer.zero_grad()\n",
    "        #4.back propagation ->파라미터들의 gradient값들을 계산한다.\n",
    "        loss.backward() #loss에 대한 모든 parameter들에 대한 gradient를 계산한다. ---- 변수의 grad 속성에 저장.\n",
    "        \n",
    "        \n",
    "        #5. 파라미터 업데이트 \n",
    "        optimizer.step()\n",
    "        \n",
    "        #이렇게 하면 weight,bias가 업데이트가 된다.\n",
    "        \n",
    "        \n",
    "        #6.현 step의 loss를 train_loss에 누적한다.\n",
    "        train_loss += loss.item()\n",
    "    #train_loss의 전체 평균을 계산한다. step수로 나눠서 전체 평균을 계산한다.\n",
    "    train_loss /= len(boston_trainloader) #step수로 나누기.\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    ######################################\n",
    "    # validation - 모델을 평가(eval) mode로 변경\n",
    "    #            - 검증,평가,서비스를 할 때.\n",
    "    #          -validation/test dataset으로 모델을 평가한다.\n",
    "    ######################################\n",
    "    boston_model.eval() #평가 모드로 변경\n",
    "    \n",
    "    \n",
    "    # 검증 loss를 저장할 변수\n",
    "    valid_loss= 0.0\n",
    "    #검증은 gradient 계산할 필요가 없음. forward propagation시 도함수를 구할 필요가 없다.\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_valid,y_valid in boston_testloader:\n",
    "            #1.device로 이동한다.\n",
    "            X_valid,y_valid = X_valid.to(device),y_valid.to(device)\n",
    "            \n",
    "            #2.모델을 이용해 예측\n",
    "            pred_valid = boston_model(X_valid)\n",
    "            #3. 평가 -MSE\n",
    "            valid_loss += loss_fn(pred_valid,y_valid).item()\n",
    "        \n",
    "        #반복문을 빠져 나오면, 검증셋에 대한 것이 끝난것이다.\n",
    "        # valid_loss 평균\n",
    "        valid_loss /=len(boston_testloader)\n",
    "        \n",
    "    # 현 epcoh에 대한 학습 결과 로그를 출력한다. +list에 추가.\n",
    "    print(f\"[{epoch+1}/{N_EPOCH}]  train loss: {train_loss:.4f},valid loss: {valid_loss:.4f}\")\n",
    "    train_loss_list.append(train_loss)\n",
    "    valid_loss_list.append(valid_loss)\n",
    "    \n",
    "    \n",
    "e = time.time()            \n",
    "    \n",
    "    \n",
    "#코드를 다시 한번 한 호흡에 실행을 해 보면, train loss와 valid loss가 둘 다 줄어들고 있음을 알 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "걸린 시간: 9.640349864959717 초\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"걸린 시간:\",(e-s),\"초\")\n",
    "\n",
    "\n",
    "#이렇게 데이터가 적은 예제는, 걸린 시간이 그렇게 길지 않다. 그래서 시간차를 비교하는 것이 적합하지 않을 지도 모른다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#train loss, valid loss의 epoch별 변화의 흐름을 시각화하자.\n",
    "\n",
    "plt.plot(range(1,N_EPOCH+1),train_loss_list,label=\"Train Loss\")\n",
    "plt.plot(range(1,N_EPOCH+1),valid_loss_list,label=\"Validation Loss\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#그래프를 통해 값의 추세를 알 수 있다.\n",
    "plt.xlabel(\"EPOCH\")\n",
    "plt.ylabel(\"Loss\")\n",
    "#plt.ylim(3,50)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#코드의 과정을 여러번 반복해서는 안되는게, train_loss_list에 많은 값이 들어가게 되면 고정된 epoch보다 크기가 커지므로 오류가 난다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.5718612410964123, 4.559169023657633)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#오차를 본격적으로 계산해보자.\n",
    "\n",
    "train_loss_list[-1]**(1/2), valid_loss_list[-1]**(1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 저장\n",
    "\n",
    "## 모델 전체 저장 및 불러오기\n",
    "- 모델구조, 파라미터 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"models/boston_model.pth\"\n",
    "torch.save(boston_model,save_path) #boston_model을 save_path에 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#크기는 8kb 정도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BostonModel                              [200, 1]                  --\n",
       "├─Linear: 1-1                            [200, 32]                 448\n",
       "├─Linear: 1-2                            [200, 16]                 528\n",
       "├─Linear: 1-3                            [200, 1]                  17\n",
       "==========================================================================================\n",
       "Total params: 993\n",
       "Trainable params: 993\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.20\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.08\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.09\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_boston_model = torch.load(save_path)\n",
    "torchinfo.summary(load_boston_model,(200,13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 13])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#예측 서비스\n",
    "\n",
    "new_X = torch.cat([boston_trainset[0][0],boston_testset[1][0]])\n",
    "new_X = new_X.reshape(-1,13) #위의 데이터들을 정확히 2개로 나눈다.\n",
    "new_X.shape #[2:데이터 수 13:feature 수]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3726, -0.4996, -0.7049,  3.6645, -0.4249,  0.9357,  0.6937, -0.4372,\n",
       "        -0.1622, -0.5617, -0.4846,  0.3717, -0.4110])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_trainset[0][0] #첫번째 데이터 ->x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.7193, -0.4996,  0.9989, -0.2729,  0.6528, -0.1237,  1.1033, -1.2517,\n",
       "         1.6874,  1.5421,  0.7927,  0.0832, -0.4357])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_testset[1][0] #두번째 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27.2261],\n",
      "        [27.2630]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pred_new= load_boston_model(new_X)\n",
    "print(pred_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([26.7000]), tensor([50.]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_trainset[0][1],boston_testset[1][1]\n",
    "\n",
    "#첫번째는 거의 차이가 없는데 둘쩨놈은 많이 틀렸다. 이런게 바로 이상치이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(516.9728, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(pred_new[1],boston_testset[1][1]) #이거도 데이터의 종류에 따라 값이 천차만별이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## state_dict 저장 및 로딩\n",
    "- 모델 파라미터만 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save_path2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-601b46c2e00d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#코드를 다시 한번 보도록 하기.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mload_sd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_sd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mnew_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBostonModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'save_path2' is not defined"
     ]
    }
   ],
   "source": [
    "#불러오기\n",
    "#코드를 다시 한번 보도록 하기.\n",
    "\n",
    "load_sd = torch.load(save_path2)\n",
    "type(load_sd)\n",
    "new_model = BostonModel()\n",
    "new_model.load_state_dict(load_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 분류 (Classification)\n",
    "\n",
    "## Fashion MNIST Dataset - 다중분류(Multi-Class Classification) 문제\n",
    "\n",
    "- 다중분류와 이진분류가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "10개의 범주(category)와 70,000개의 흑백 이미지로 구성된 [패션 MNIST](https://github.com/zalandoresearch/fashion-mnist) 데이터셋. \n",
    "이미지는 해상도(28x28 픽셀)가 낮고 다음처럼 개별 의류 품목을 나타낸다:\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>그림</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">패션-MNIST 샘플</a> (Zalando, MIT License).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "이미지는 28x28 크기이며 Gray scale이다. *레이블*(label)은 0에서 9까지의 정수 배열이다. 아래 표는 이미지에 있는 의류의 **클래스**(class)들이다.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>레이블</th>\n",
    "    <th>클래스</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0</td>\n",
    "    <td>T-shirt/top</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>Trousers</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>2</td>\n",
    "    <td>Pullover</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>3</td>\n",
    "    <td>Dress</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>4</td>\n",
    "    <td>Coat</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>5</td>\n",
    "    <td>Sandal</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>6</td>\n",
    "    <td>Shirt</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>7</td>\n",
    "    <td>Sneaker</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>8</td>\n",
    "    <td>Bag</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>9</td>\n",
    "    <td>Ankle boot</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#import 구문 따로 설정하기.\n",
    "#문제가 새롭게 시작될 때 마다 import를 따로 한다고 보면 편하다.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "import torchinfo\n",
    "\n",
    "\n",
    "#이렇게 import 하는 구문을 따로 두는 것이 중요하다.\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_class = np.array(['T-shirt/top', 'Trousers', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Dataset, DataLoader 생성\n",
    "#Built in Dataset\n",
    "\n",
    "\n",
    "#이 코드를 통해 download를 한다.\n",
    "fmnist_trainset = datasets.FashionMNIST(root=\"datasets\",train=True,download=True,transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testset data를 다운로드한다.\n",
    "fmnist_testset = datasets.FashionMNIST(root=\"datasets\",train=False,download=True,transform=transforms.ToTensor())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 개수: 60000\n",
      "test 개수: 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"train 개수:\",len(fmnist_trainset))\n",
    "print(\"test 개수:\",len(fmnist_testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: datasets\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n",
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: datasets\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n"
     ]
    }
   ],
   "source": [
    "print(fmnist_trainset)\n",
    "print(fmnist_testset)\n",
    "\n",
    "#정보들을 잘 보도록 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Trouser', 'Trouser', 'Pullover', 'Dress', 'T-shirt/top'],\n",
       "      dtype='<U11')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#index to class\n",
    "\n",
    "index_to_class = np.array(fmnist_trainset.classes)\n",
    "index_to_class[[1,1,2,3,0]] #이런 식으로 배열을 마음가는 대로 설정할 수 있다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T-shirt/top': 0,\n",
       " 'Trouser': 1,\n",
       " 'Pullover': 2,\n",
       " 'Dress': 3,\n",
       " 'Coat': 4,\n",
       " 'Sandal': 5,\n",
       " 'Shirt': 6,\n",
       " 'Sneaker': 7,\n",
       " 'Bag': 8,\n",
       " 'Ankle boot': 9}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_to_index = fmnist_trainset.class_to_idx\n",
    "class_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#이미지 확인\n",
    "\n",
    "idx = 10 #이 숫자를 마음대로 변경하여 값들을 살펴보자.\n",
    "x,y = fmnist_trainset[idx] #Dataset[i]:(X,y)\n",
    "\n",
    "plt.imshow(x[0],cmap='gray')\n",
    "plt.title(index_to_class[y])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y #의류의 종류를 이야기해주는....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader\n",
    "#train이느냐 test이느냐에 따라서 코드를 따로 코딩하면 된다.\n",
    "\n",
    "\n",
    "fmnist_trainloader = DataLoader(fmnist_trainset,batch_size=128,shuffle=True,drop_last=True)\n",
    "fmnist_testloader = DataLoader(fmnist_testset,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "##모델 정의\n",
    "\n",
    "class FashionMNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #입력 이미지를 받아서 처리후 리턴====입력 이미지의 feature수만큼!(28*28=784)\n",
    "        #가면 갈수록 feature 수를 줄이는 이유는, 전체 이미지 중에서 핵심적인 부분만 찾기 위해서 이렇게 하는 것이다.\n",
    "        self.lr1 = nn.Linear(784,2048) #784-> 2048\n",
    "        self.lr2 = nn.Linear(2048,1024) #2048-> 1024\n",
    "        self.lr3 = nn.Linear(1024,512) #1024->512\n",
    "        self.lr4 = nn.Linear(512,256) #512->256\n",
    "        self.lr5 = nn.Linear(256,128) #256->128\n",
    "        self.lr6 = nn.Linear(128,64) #128->64\n",
    "        \n",
    "        \n",
    "        #output----옷 종류(정답 클래스 개수)의 수에 맞춘다.\n",
    "        self.lr7 = nn.Linear(64,10) #각 클래스별 확률이 출력이 되도록 한다.\n",
    "    def forward(self,X):\n",
    "        #X -> (batch,channel,height,width) ==> (batch,all_feature)\n",
    "        #out = torch.flatten(X,start_dim=1)\n",
    "        out = nn.Flatten()(X)\n",
    "        \n",
    "        \n",
    "        out = nn.ReLU()(self.lr1(out))\n",
    "        out = nn.ReLU()(self.lr2(out))\n",
    "        out = nn.ReLU()(self.lr3(out))\n",
    "        out = nn.ReLU()(self.lr4(out))\n",
    "        out = nn.ReLU()(self.lr5(out))\n",
    "        out = nn.ReLU()(self.lr6(out))\n",
    "        ##output\n",
    "        out = self.lr7(out)\n",
    "        \n",
    "        \n",
    "        return out\n",
    "        #lr1 ~ lr7\n",
    "        ## forward 처리를 구현한다. => Linear -> ReLU()\n",
    "        #lr6까지는 처리를 해 주고, 7은 하지 말자......\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionMNIST(\n",
      "  (lr1): Linear(in_features=784, out_features=2048, bias=True)\n",
      "  (lr2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (lr3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (lr4): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (lr5): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (lr6): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (lr7): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#모델 생성 및 확인\n",
    "\n",
    "f_model = FashionMNIST()\n",
    "print(f_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "FashionMNIST                             [128, 10]                 --\n",
       "├─Linear: 1-1                            [128, 2048]               1,607,680\n",
       "├─Linear: 1-2                            [128, 1024]               2,098,176\n",
       "├─Linear: 1-3                            [128, 512]                524,800\n",
       "├─Linear: 1-4                            [128, 256]                131,328\n",
       "├─Linear: 1-5                            [128, 128]                32,896\n",
       "├─Linear: 1-6                            [128, 64]                 8,256\n",
       "├─Linear: 1-7                            [128, 10]                 650\n",
       "==========================================================================================\n",
       "Total params: 4,403,786\n",
       "Trainable params: 4,403,786\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 563.68\n",
       "==========================================================================================\n",
       "Input size (MB): 0.40\n",
       "Forward/backward pass size (MB): 4.14\n",
       "Params size (MB): 17.62\n",
       "Estimated Total Size (MB): 22.16\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#요약\n",
    "#파라미터가 얼마나 많은지, 그리고 어떻게 줄어 드는지도 잘 파악을 하자.\n",
    "torchinfo.summary(f_model,(128,1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0651, -0.0287,  0.0562, -0.0465,  0.0577,  0.0876,  0.0387, -0.0428,\n",
       "         0.1424,  0.0672], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 모델 추정결과 형태를 확인\n",
    "\n",
    "i = torch.ones((2,1,28,28)) #1*28*28 이미지 2장\n",
    "y_hat = f_model(i)\n",
    "y_hat[0] #첫번째 이미지에 대한 추론 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bag', 'Bag'], dtype='<U11')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat[0].shape\n",
    "\n",
    "\n",
    "#정답 class =>    예측결과 10중에서 가장 큰 값이 있는 index\n",
    "y_hat.argmax(axis=-1 ) #첫번째에 관한 예측: 5,두번째도 5임.\n",
    "\n",
    "index_to_class[y_hat.detach().numpy().argmax(axis=-1)] #detach 메소드로 잘 보도록 하자.\n",
    "#requires_grad=True인 Tensor를 ndarray로 변환할 때는\n",
    "## tensor.detach()를 한 다음 변환해야 한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_hat =>확률값으로 변환 ====>Softmax()\n",
    "y_hat_probabiity = nn.Softmax(dim=0)(y_hat) #다 합치면 1이다. 각 종류의 옷일 확률을 구하는 것이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_hat_probability' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-7661b03c898b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_hat_probabiity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_hat_probability\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_hat_probability' is not defined"
     ]
    }
   ],
   "source": [
    "y_hat_probabiity.max(dim=-1).values,y_hat_probability.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "###학습(train)\n",
    "\n",
    "\n",
    "LR=0.001\n",
    "N_EPOCH=200\n",
    "\n",
    "\n",
    "#모델을 device로 이동한다.\n",
    "f_model = f_model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "#loss fn ==>다중분류\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() #다중 분류용 log loss\n",
    "\n",
    "#optimizer\n",
    "optimizer = torch.optim.Adam(f_model.parameters(),lr=LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/20] train loss: 0.6420736354895127 valid loss: 0.4506871700286865 valid acc: 0.8361\n",
      "[02/20] train loss: 0.3995607113226866 valid loss: 0.3963545560836792 valid acc: 0.8563\n",
      "[03/20] train loss: 0.35227826371406895 valid loss: 0.3863133192062378 valid acc: 0.8627\n",
      "[04/20] train loss: 0.3269052625211895 valid loss: 0.35392794013023376 valid acc: 0.8726\n",
      "[05/20] train loss: 0.301582530936879 valid loss: 0.3552477955818176 valid acc: 0.8727\n",
      "[06/20] train loss: 0.28805436805272716 valid loss: 0.3504869341850281 valid acc: 0.8732\n",
      "[07/20] train loss: 0.2767181450294124 valid loss: 0.40548890829086304 valid acc: 0.8591\n",
      "[08/20] train loss: 0.2683854476891012 valid loss: 0.3577542006969452 valid acc: 0.8754\n",
      "[09/20] train loss: 0.2504744268157798 valid loss: 0.3477397859096527 valid acc: 0.8761\n",
      "[10/20] train loss: 0.24078787399981266 valid loss: 0.3279939591884613 valid acc: 0.8852\n",
      "[11/20] train loss: 0.23128276300799644 valid loss: 0.3358825147151947 valid acc: 0.8866\n",
      "[12/20] train loss: 0.21834452526691633 valid loss: 0.3269304633140564 valid acc: 0.8903\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-b5d7bd8b9a15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m## 4-2 grad 계산 - (오차) 역전파\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;31m## 4-3 파라미터 업데이트\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             )\n\u001b[1;32m--> 492\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "import time\n",
    "N_EPOCH=20\n",
    "## 각 에폭별 학습이 끝나고 모델 평가한 값을 저장.\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "valid_acc_list = []   # test set의 정확도 검증 결과 => 전체데이터 중 맞은데이터의 개수\n",
    "s = time.time()\n",
    "for epoch in range(N_EPOCH):\n",
    "    ######### train\n",
    "    f_model.train()\n",
    "    train_loss = 0.0 # 현재 epoch의 tain set의 loss\n",
    "    for X_train, y_train in fmnist_trainloader:\n",
    "        # 1. device로 옮기기. model과 같은 device로 옮긴다.\n",
    "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "        # 2. 예측 - 순전파\n",
    "        pred_train = f_model(X_train)\n",
    "        # 3. Loss 계산\n",
    "        loss = loss_fn(pred_train, y_train) # (예측, 정답)\n",
    "        # 4 모델 파라미터 업데이트\n",
    "        ## 4-1 gradient 초기화\n",
    "        optimizer.zero_grad()\n",
    "        ## 4-2 grad 계산 - (오차) 역전파\n",
    "        loss.backward()\n",
    "        ## 4-3 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "        # train loss를 누적\n",
    "        train_loss += loss.item()\n",
    "    # 1에폭 학습 종료 => train_loss의 평균을 list에 저장.\n",
    "    train_loss /= len(fmnist_trainloader)  # 누적_train_loss/step수\n",
    "    train_loss_list.append(train_loss)\n",
    "    ######### validation\n",
    "    f_model.eval()\n",
    "    valid_loss = 0.0  # 현재 epoch의 validation loss 저장할 변수\n",
    "    valid_acc = 0.0   # 현재 epoch의 validation accuracy(정확도)를 저장할 변수\n",
    "    ### 정확도: 맞은것의 개수 / 전체 개수\n",
    "    with torch.no_grad(): # 도함수 구할 필요가 없으므로 no grad context manager에서 실행.\n",
    "        for X_valid, y_valid in fmnist_testloader:\n",
    "            # 1. device로 옮기기\n",
    "            X_valid, y_valid = X_valid.to(device), y_valid.to(device)\n",
    "            # 2. 예측\n",
    "            pred_valid = f_model(X_valid) # class별 정답일 가능성을 출력 (batch, 10)\n",
    "            pred_label = pred_valid.argmax(dim=-1) # 정답 class를 조회. (pred_valid에서 가장 큰값을 가진 index)\n",
    "            # 3. 평가\n",
    "            ## 3.1 loss 계산\n",
    "            loss_valid = loss_fn(pred_valid, y_valid) ## loss_fn() batch만큼 평균을 계산.\n",
    "            valid_loss += loss_valid\n",
    "            ## 3.2 정확도 계산\n",
    "            valid_acc += torch.sum(pred_label == y_valid).item()\n",
    "        # 한 epoch에 대한 평가 완료 => valid_loss_list, valid_acc_list에 추가\n",
    "        valid_loss /= len(fmnist_testloader)        # step수로 나눠서 평균을 계산\n",
    "        valid_acc /= len(fmnist_testloader.dataset) # testset의 총 데이터 개수로 나눔.\n",
    "        valid_loss_list.append(valid_loss)\n",
    "        valid_acc_list.append(valid_acc)\n",
    "        print(f\"[{epoch+1:02d}/{N_EPOCH}] train loss: {train_loss} valid loss: {valid_loss} valid acc: {valid_acc}\")\n",
    "e = time.time()\n",
    "    \n",
    "#정확도는 뭐 한 89퍼센트 정도 나오는 것 같다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-e1949790b36d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvalid_loss_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalid_loss_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mvalid_loss_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-90-e1949790b36d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvalid_loss_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalid_loss_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mvalid_loss_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "valid_loss_list = [v.item() for v in valid_loss_list]\n",
    "valid_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_acc_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-01c550e16e85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_acc_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation Accuracy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'valid_acc_list' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#결과 시각화\n",
    "#loss는 내려갈 수록 즣은거고, valid는 올라갈 수록 좋다.\n",
    "#허나, 중요한 것은 과대적합이 나올 수도 있다는 것이다. 그래서 중간에 끊어주는 것이 중요하다.\n",
    "#그리고, loss나 정확도가 왔다리갔다리 하는 경우가 많다.\n",
    "#그 중에서 성능이 가장 좋은 놈을 찾는게 중요한데.... 그 놈을 찾는 것도 감이고 실력이다.\n",
    "#근데..... 특정 시점에서 더 이상 정확도 등이 획기적으로 개선되지 않으면 중간에 종료시키는 것도 중요하다.\n",
    "\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Malgun gothic\"\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_loss_list,label=\"train\")\n",
    "plt.plot(train_loss_list,label=\"validation\")\n",
    "plt.title(\"Epoch 별 loss 변화\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(valid_acc_list)\n",
    "plt.title(\"Validation Accuracy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 학습이 진행이 되면 어느 시점부터는 성능이 떨어지기 시작한다.\n",
    "#학습도중 성능 개선될 때마다 저장.(가장 좋은 성능의 모델을 서비스 할 수 있게한다.)\n",
    "\n",
    "\n",
    "#1. 학습 도중 성능이 개선 될 때마다 저장한다.\n",
    "#2. 더 이상 성능이 개선이 안되면 학습을 중지한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/1000] train loss: 0.6496741068032053 valid loss: 0.4531976199602779 valid acc: 0.8362\n",
      "====> 모델저장:  1 Epoch - 이전 valid_loss: inf, 현재 valid_loss: 0.4531976199602779\n",
      "[02/1000] train loss: 0.3880012427958158 valid loss: 0.4072249775068669 valid acc: 0.8586\n",
      "====> 모델저장:  2 Epoch - 이전 valid_loss: 0.4531976199602779, 현재 valid_loss: 0.4072249775068669\n",
      "[03/1000] train loss: 0.352161408521426 valid loss: 0.36911332965651644 valid acc: 0.8698\n",
      "====> 모델저장:  3 Epoch - 이전 valid_loss: 0.4072249775068669, 현재 valid_loss: 0.36911332965651644\n",
      "[04/1000] train loss: 0.321003187320426 valid loss: 0.3762322586925724 valid acc: 0.8652\n",
      "[05/1000] train loss: 0.3025762515986322 valid loss: 0.35094515847254404 valid acc: 0.8764\n",
      "====> 모델저장:  5 Epoch - 이전 valid_loss: 0.36911332965651644, 현재 valid_loss: 0.35094515847254404\n",
      "[06/1000] train loss: 0.28432955529190534 valid loss: 0.34837701633761203 valid acc: 0.8805\n",
      "====> 모델저장:  6 Epoch - 이전 valid_loss: 0.35094515847254404, 현재 valid_loss: 0.34837701633761203\n",
      "[07/1000] train loss: 0.2712523322074841 valid loss: 0.3442836813157118 valid acc: 0.879\n",
      "====> 모델저장:  7 Epoch - 이전 valid_loss: 0.34837701633761203, 현재 valid_loss: 0.3442836813157118\n",
      "[08/1000] train loss: 0.26057134926892245 valid loss: 0.34120869881744625 valid acc: 0.8849\n",
      "====> 모델저장:  8 Epoch - 이전 valid_loss: 0.3442836813157118, 현재 valid_loss: 0.34120869881744625\n",
      "[09/1000] train loss: 0.24514237944132242 valid loss: 0.3468181351317635 valid acc: 0.8787\n",
      "[10/1000] train loss: 0.24046144659957316 valid loss: 0.3381842834096921 valid acc: 0.8848\n",
      "====> 모델저장:  10 Epoch - 이전 valid_loss: 0.34120869881744625, 현재 valid_loss: 0.3381842834096921\n",
      "[11/1000] train loss: 0.2289039466339044 valid loss: 0.3322570786068711 valid acc: 0.8855\n",
      "====> 모델저장:  11 Epoch - 이전 valid_loss: 0.3381842834096921, 현재 valid_loss: 0.3322570786068711\n",
      "[12/1000] train loss: 0.21838627486593193 valid loss: 0.34056916776337204 valid acc: 0.8834\n",
      "[13/1000] train loss: 0.21135685075488356 valid loss: 0.35757308632512635 valid acc: 0.8867\n",
      "[14/1000] train loss: 0.20625789589288399 valid loss: 0.3297886515059803 valid acc: 0.8896\n",
      "====> 모델저장:  14 Epoch - 이전 valid_loss: 0.3322570786068711, 현재 valid_loss: 0.3297886515059803\n",
      "[15/1000] train loss: 0.19638435173238444 valid loss: 0.34945273154144046 valid acc: 0.8907\n",
      "[16/1000] train loss: 0.18816609726820746 valid loss: 0.3405876886099577 valid acc: 0.8955\n",
      "[17/1000] train loss: 0.18111535287501976 valid loss: 0.3677117658566825 valid acc: 0.8843\n",
      "[18/1000] train loss: 0.17889012716328487 valid loss: 0.38315039837756487 valid acc: 0.8885\n",
      "[19/1000] train loss: 0.17681389878320897 valid loss: 0.38808519084336635 valid acc: 0.8885\n",
      "=====> 19 Epoch에서 조기종료-0.3297886515059803에서 개선 안됨\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "LR = 0.001\n",
    "N_EPOCH = 1000\n",
    "# 모델을 device 로 이동.\n",
    "f_model = FashionMNIST()\n",
    "f_model = f_model.to(device)\n",
    "# loss fn -> 다중분류: nn.CrossEntropyLoss() ==> 다중 분류용 Log loss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(f_model.parameters(), lr=LR)\n",
    "########################################\n",
    "# 조기종료 + 모델 저장을 위한 변수 추가\n",
    "########################################\n",
    "###### 모델 저장을 위한변수\n",
    "# 학습 중 가장 좋은 성능 평가지표를 저장. 현 epoch의 지표가 이 변수값보다 좋으면 저장\n",
    "# 평가지표: validation loss\n",
    "best_score = torch.inf\n",
    "save_model_path = \"models/fashion_mnist_best_model.pth\"\n",
    "###### 조기 종료를 위한 변수: 특정 epoch동안 성능 개선이 없으면 학습을 중단\n",
    "patience = 5 # 성능이 개선 될지를 기다릴 epoch 수. patience 번 만큼 개선이 안되면 중단.(보통 10이상 지정)\n",
    "trigger_cnt = 0 # 성능 개선을 몇번 째 기다리는 지 정할 변수. patience==trigger_cnt : 중단\n",
    "# train\n",
    "## 각 에폭별 학습이 끝나고 모델 평가한 값을 저장.\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "valid_acc_list = []   # test set의 정확도 검증 결과 => 전체데이터 중 맞은데이터의 개수\n",
    "s = time.time()\n",
    "for epoch in range(N_EPOCH):\n",
    "    ######### train\n",
    "    f_model.train()\n",
    "    train_loss = 0.0 # 현재 epoch의 tain set의 loss\n",
    "    for X_train, y_train in fmnist_trainloader:\n",
    "        # 1. device로 옮기기. model과 같은 device로 옮긴다.\n",
    "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "        # 2. 예측 - 순전파\n",
    "        pred_train = f_model(X_train)\n",
    "        # 3. Loss 계산\n",
    "        loss = loss_fn(pred_train, y_train) # (예측, 정답)\n",
    "        # 4 모델 파라미터 업데이트\n",
    "        ## 4-1 gradient 초기화\n",
    "        optimizer.zero_grad()\n",
    "        ## 4-2 grad 계산 - (오차) 역전파\n",
    "        loss.backward()\n",
    "        ## 4-3 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "        # train loss를 누적\n",
    "        train_loss += loss.item()\n",
    "    # 1에폭 학습 종료 => train_loss의 평균을 list에 저장.\n",
    "    train_loss /= len(fmnist_trainloader)  # 누적_train_loss/step수\n",
    "    train_loss_list.append(train_loss)\n",
    "    ######### validation\n",
    "    f_model.eval()\n",
    "    valid_loss = 0.0  # 현재 epoch의 validation loss 저장할 변수\n",
    "    valid_acc = 0.0   # 현재 epoch의 validation accuracy(정확도)를 저장할 변수\n",
    "    ### 정확도: 맞은것의 개수 / 전체 개수\n",
    "    with torch.no_grad(): # 도함수 구할 필요가 없으므로 no grad context manager에서 실행.\n",
    "        for X_valid, y_valid in fmnist_testloader:\n",
    "            # 1. device로 옮기기\n",
    "            X_valid, y_valid = X_valid.to(device), y_valid.to(device)\n",
    "            # 2. 예측\n",
    "            pred_valid = f_model(X_valid) # class별 정답일 가능성을 출력 (batch, 10)\n",
    "            pred_label = pred_valid.argmax(dim=-1) # 정답 class를 조회. (pred_valid에서 가장 큰값을 가진 index)\n",
    "            # 3. 평가\n",
    "            ## 3.1 loss 계산\n",
    "            loss_valid = loss_fn(pred_valid, y_valid) ## loss_fn() batch만큼 평균을 계산.\n",
    "            valid_loss += loss_valid.item()\n",
    "            ## 3.2 정확도 계산\n",
    "            valid_acc += torch.sum(pred_label == y_valid).item()\n",
    "        # 한 epoch에 대한 평가 완료 => valid_loss_list, valid_acc_list에 추가\n",
    "        valid_loss /= len(fmnist_testloader)        # step수로 나눠서 평균을 계산\n",
    "        valid_acc /= len(fmnist_testloader.dataset) # testset의 총 데이터 개수로 나눔.\n",
    "        valid_loss_list.append(valid_loss)\n",
    "        valid_acc_list.append(valid_acc)\n",
    "        print(f\"[{epoch+1:02d}/{N_EPOCH}] train loss: {train_loss} valid loss: {valid_loss} valid acc: {valid_acc}\")\n",
    "    ##################################\n",
    "    # 조기종료여부, 모델 저장 처리\n",
    "    #   저장: 현 epoch valid_loss 가 best_score 보다 개선된 경우 저장(작으면 개선)\n",
    "    #################################\n",
    "    if valid_loss < best_score: # 성능이 개선된 경우.\n",
    "        #저장 로그 출력\n",
    "        print(f\"====> 모델저장:  {epoch+1} Epoch - 이전 valid_loss: {best_score}, 현재 valid_loss: {valid_loss}\")\n",
    "        # best_score교체\n",
    "        best_score = valid_loss\n",
    "        # 저장\n",
    "        torch.save(f_model, save_model_path)\n",
    "        # trigger_cnt 를 0으로 초기화\n",
    "        trigger_cnt = 0\n",
    "    else: # 성능개선이 안된경우.\n",
    "        # trigger_cnt를 1 증가\n",
    "        trigger_cnt += 1\n",
    "        if patience == trigger_cnt: # patience 만큼 대기 ==> 조기 종료\n",
    "            #로그\n",
    "            print(f\"=====> {epoch+1} Epoch에서 조기종료-{best_score}에서 개선 안됨\")\n",
    "            break\n",
    "e = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  저장된 모델을 로딩\n",
    "\n",
    "best_model = torch.load(save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test_dataloader로 평가한다.\n",
    "\n",
    "#### test_dataloader 로 평가\n",
    "best_model = best_model.to(device)\n",
    "best_model.eval()\n",
    "valid_loss = 0.0  # 현재 epoch의 validation loss 저장할 변수\n",
    "valid_acc = 0.0   # 현재 epoch의 validation accuracy(정확도)를 저장할 변수\n",
    "### 정확도: 맞은것의 개수 / 전체 개수\n",
    "with torch.no_grad(): # 도함수 구할 필요가 없으므로 no grad context manager에서 실행.\n",
    "    for X_valid, y_valid in fmnist_testloader:\n",
    "        # 1. device로 옮기기\n",
    "        X_valid, y_valid = X_valid.to(device), y_valid.to(device)\n",
    "\n",
    "        # 2. 예측\n",
    "        pred_valid = best_model(X_valid) # class별 정답일 가능성을 출력 (batch, 10)\n",
    "        pred_label = pred_valid.argmax(dim=-1) # 정답 class를 조회. (pred_valid에서 가장 큰값을 가진 index)\n",
    "\n",
    "        # 3. 평가\n",
    "        ## 3.1 loss 계산\n",
    "        loss_valid = loss_fn(pred_valid, y_valid) ## loss_fn() batch만큼 평균을 계산.\n",
    "        valid_loss += loss_valid.item()\n",
    "        ## 3.2 정확도 계산\n",
    "        valid_acc += torch.sum(pred_label == y_valid).item()\n",
    "    # 한 epoch에 대한 평가 완료 => valid_loss_list, valid_acc_list에 추가\n",
    "    valid_loss /= len(fmnist_testloader)        # step수로 나눠서 평균을 계산\n",
    "    valid_acc /= len(fmnist_testloader.dataset) # testset의 총 데이터 개수로 나눔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 위스콘신 유방암 데이터셋 - 이진분류(Binary Classification) 문제\n",
    "\n",
    "- **이진 분류 문제 처리 모델의 두가지 방법**\n",
    "    1. positive(1)일 확률을 출력하도록 구현\n",
    "        - output layer: units=1, activation='sigmoid'\n",
    "        - loss: binary_crossentropy\n",
    "    2. negative(0)일 확률과 positive(1)일 확률을 출력하도록 구현 => 다중분류 처리 방식으로 해결\n",
    "        - output layer: units=2, activation='softmax', y(정답)은 one hot encoding 처리\n",
    "        - loss: categorical_crossentropy\n",
    "        \n",
    "- 위스콘신 대학교에서 제공한 종양의 악성/양성여부 분류를 위한 데이터셋\n",
    "- Feature\n",
    "    - 종양에 대한 다양한 측정값들\n",
    "- Target의 class\n",
    "    - 0 - malignant(악성종양)\n",
    "    - 1 - benign(양성종양)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#주의! 양성이라는 것이 마냥 긍정적인 의미는 아니다. \n",
    "#왜 코로나도 음성이 좋은 거니까."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "#관련 데이터에 관한 것을 import 한다.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset, DataLoader 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(569, 30) (569,)\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "X,y = load_breast_cancer(return_X_y=True)\n",
    "print(type(X),type(y))\n",
    "print(X.shape,y.shape)\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_class = np.array([\"악성\",\"양성\"])\n",
    "class_to_index = dict(악성=0,양성=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y shape을 2차원으로 변경한다. ===> 모델 출력 shape와 맞춰준다.\n",
    "# (batch size,1)\n",
    "\n",
    "\n",
    "y = y.reshape(-1,1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 30) (143, 30) (426, 1) (143, 1)\n"
     ]
    }
   ],
   "source": [
    "### train/test set 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, #나눌 대상 X,y\n",
    "                                                   test_size=0.25, #비율을 정한다.\n",
    "                                                   stratify=y #클래스 별 비율을 맞춰서 나눈다.\n",
    "                                                   ) \n",
    "\n",
    "\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 전처리 - Feature Scaling (컬럼들의 scale을 맞춘다.)\n",
    "\n",
    "\n",
    "#StandardScaler ===> 평균:0, 표준편차 :1 을 기준으로 맞춘다.\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test) #trainset으로 fit한 scaler를 이용해 변환.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9399,  0.8737,  0.8065,  ..., -0.4851, -0.7890, -1.0267],\n",
      "        [-0.7053, -0.2145, -0.6901,  ...,  0.1044, -0.1963,  0.3647],\n",
      "        [ 0.1274, -1.2153,  0.0906,  ..., -0.1183, -0.5312, -0.3288],\n",
      "        ...,\n",
      "        [ 0.2628, -0.8633,  0.2405,  ...,  0.3449,  0.0515, -0.0314],\n",
      "        [-0.5728, -0.3548, -0.5689,  ..., -0.7793,  0.2106, -0.7217],\n",
      "        [ 0.7757, -0.1064,  0.7355,  ...,  0.4869, -0.1896, -0.6425]])\n",
      "torch.Size([143, 30])\n"
     ]
    }
   ],
   "source": [
    "#### ndarray => Tensor 변환 =====> dataset을 구성 =====> dataloader 구성\n",
    "\n",
    "\n",
    "#ndarray =>torch.Tensor\n",
    "X_train_tensor = torch.tensor(X_train_scaled,dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled,dtype=torch.float32)\n",
    "print(X_test_tensor)\n",
    "print(X_test_tensor.shape)\n",
    "y_train_tensor = torch.tensor(y_train,dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test,dtype=torch.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 생성 ==> 매모리의 tensor를 dataset으로 생성 => tensordataset\n",
    "#TensorDataset을 활용하면 더 쉽게 data를 구분할 수 있다.\n",
    "\n",
    "trainset = TensorDataset(X_train_tensor,y_train_tensor)\n",
    "testset = TensorDataset(X_test_tensor,y_test_tensor)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#dataloader\n",
    "train_loader = DataLoader(trainset, batch_size=200, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(testset, batch_size=len(testset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 클래스 정의\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426, 30)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCModel(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.lr1 = nn.Linear(30,32)\n",
    "        self.lr2 = nn.Linear(32,8)\n",
    "        #출력 layer: 이진 분류 - positive의 확률 값 한 개를 출력한다.\n",
    "        self.lr3 = nn.Linear(8,1)\n",
    "        \n",
    "        \n",
    "    def forward(self,X):\n",
    "        \n",
    "        \n",
    "        # X (입력) shape: (batchsize,30)\n",
    "        #out = self.lr1(X)\n",
    "        #out = nn.ReLU(out)\n",
    "        out = nn.ReLU()(self.lr1(X)) #이런 식으로 해도 된다.\n",
    "        out = nn.ReLU()(self.lr2(out))\n",
    "        \n",
    "        #이진분류 출력값 처리 ->Linear()는 한 개의 값을 출력한다. =>확률값으로 변경한다. ==> Sigmoid 함수를 Activation 함수로 사용한다.\n",
    "        out = self.lr3(out)\n",
    "        out = nn.Sigmoid()(out)\n",
    "        return out\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 30])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3961],\n",
       "        [0.3961],\n",
       "        [0.3961],\n",
       "        [0.3961],\n",
       "        [0.3961]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BCModel()\n",
    "tmp_x = torch.ones(5,30)\n",
    "print(tmp_x.shape)\n",
    "tmp_y = model(tmp_x)\n",
    "tmp_y\n",
    "\n",
    "#0.xxxx->1(양성)일 확률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#일정 값보다 더 큰지/작은지를 파악하는 것이 좋다.\n",
    "\n",
    "\n",
    "#tensor객체.type(타입을 지정) ===> Tensor 데이터타입을 반환.\n",
    "# bool -> int : False: 0, True :1 \n",
    "\n",
    "(tmp_y>0.5).type(torch.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1000] train loss: 0.6575140953063965, valid loss: 0.6532841324806213, valid accuracy: 0.7552447552447552\n",
      "======> 1에폭에서 모델 저장. 이전 score: inf, 현재 score: 0.6532841324806213\n",
      "[2/1000] train loss: 0.6452004909515381, valid loss: 0.6390107870101929, valid accuracy: 0.7692307692307693\n",
      "======> 2에폭에서 모델 저장. 이전 score: 0.6532841324806213, 현재 score: 0.6390107870101929\n",
      "[3/1000] train loss: 0.6319022476673126, valid loss: 0.6254581212997437, valid accuracy: 0.8041958041958042\n",
      "======> 3에폭에서 모델 저장. 이전 score: 0.6390107870101929, 현재 score: 0.6254581212997437\n",
      "[4/1000] train loss: 0.6191753149032593, valid loss: 0.6124215126037598, valid accuracy: 0.8251748251748252\n",
      "======> 4에폭에서 모델 저장. 이전 score: 0.6254581212997437, 현재 score: 0.6124215126037598\n",
      "[5/1000] train loss: 0.6060024797916412, valid loss: 0.5997019410133362, valid accuracy: 0.8461538461538461\n",
      "======> 5에폭에서 모델 저장. 이전 score: 0.6124215126037598, 현재 score: 0.5997019410133362\n",
      "[6/1000] train loss: 0.5945512652397156, valid loss: 0.5871437191963196, valid accuracy: 0.8461538461538461\n",
      "======> 6에폭에서 모델 저장. 이전 score: 0.5997019410133362, 현재 score: 0.5871437191963196\n",
      "[7/1000] train loss: 0.5812292993068695, valid loss: 0.5748099684715271, valid accuracy: 0.8531468531468531\n",
      "======> 7에폭에서 모델 저장. 이전 score: 0.5871437191963196, 현재 score: 0.5748099684715271\n",
      "[8/1000] train loss: 0.5706987977027893, valid loss: 0.5625374913215637, valid accuracy: 0.8741258741258742\n",
      "======> 8에폭에서 모델 저장. 이전 score: 0.5748099684715271, 현재 score: 0.5625374913215637\n",
      "[9/1000] train loss: 0.5554135143756866, valid loss: 0.5502921938896179, valid accuracy: 0.8951048951048951\n",
      "======> 9에폭에서 모델 저장. 이전 score: 0.5625374913215637, 현재 score: 0.5502921938896179\n",
      "[10/1000] train loss: 0.5447394549846649, valid loss: 0.5380788445472717, valid accuracy: 0.9020979020979021\n",
      "======> 10에폭에서 모델 저장. 이전 score: 0.5502921938896179, 현재 score: 0.5380788445472717\n",
      "[11/1000] train loss: 0.5326961278915405, valid loss: 0.5258543491363525, valid accuracy: 0.916083916083916\n",
      "======> 11에폭에서 모델 저장. 이전 score: 0.5380788445472717, 현재 score: 0.5258543491363525\n",
      "[12/1000] train loss: 0.5212919116020203, valid loss: 0.5136273503303528, valid accuracy: 0.916083916083916\n",
      "======> 12에폭에서 모델 저장. 이전 score: 0.5258543491363525, 현재 score: 0.5136273503303528\n",
      "[13/1000] train loss: 0.5103834867477417, valid loss: 0.5014413595199585, valid accuracy: 0.9230769230769231\n",
      "======> 13에폭에서 모델 저장. 이전 score: 0.5136273503303528, 현재 score: 0.5014413595199585\n",
      "[14/1000] train loss: 0.4958278089761734, valid loss: 0.48925402760505676, valid accuracy: 0.9230769230769231\n",
      "======> 14에폭에서 모델 저장. 이전 score: 0.5014413595199585, 현재 score: 0.48925402760505676\n",
      "[15/1000] train loss: 0.4831025004386902, valid loss: 0.477012574672699, valid accuracy: 0.9230769230769231\n",
      "======> 15에폭에서 모델 저장. 이전 score: 0.48925402760505676, 현재 score: 0.477012574672699\n",
      "[16/1000] train loss: 0.47170548141002655, valid loss: 0.4646678864955902, valid accuracy: 0.9230769230769231\n",
      "======> 16에폭에서 모델 저장. 이전 score: 0.477012574672699, 현재 score: 0.4646678864955902\n",
      "[17/1000] train loss: 0.46081599593162537, valid loss: 0.4523639678955078, valid accuracy: 0.9230769230769231\n",
      "======> 17에폭에서 모델 저장. 이전 score: 0.4646678864955902, 현재 score: 0.4523639678955078\n",
      "[18/1000] train loss: 0.4474024176597595, valid loss: 0.44002869725227356, valid accuracy: 0.9230769230769231\n",
      "======> 18에폭에서 모델 저장. 이전 score: 0.4523639678955078, 현재 score: 0.44002869725227356\n",
      "[19/1000] train loss: 0.4331027716398239, valid loss: 0.4276653230190277, valid accuracy: 0.9230769230769231\n",
      "======> 19에폭에서 모델 저장. 이전 score: 0.44002869725227356, 현재 score: 0.4276653230190277\n",
      "[20/1000] train loss: 0.42321760952472687, valid loss: 0.4153415262699127, valid accuracy: 0.9230769230769231\n",
      "======> 20에폭에서 모델 저장. 이전 score: 0.4276653230190277, 현재 score: 0.4153415262699127\n",
      "[21/1000] train loss: 0.4120214879512787, valid loss: 0.40311580896377563, valid accuracy: 0.9230769230769231\n",
      "======> 21에폭에서 모델 저장. 이전 score: 0.4153415262699127, 현재 score: 0.40311580896377563\n",
      "[22/1000] train loss: 0.3961668163537979, valid loss: 0.3910250663757324, valid accuracy: 0.9230769230769231\n",
      "======> 22에폭에서 모델 저장. 이전 score: 0.40311580896377563, 현재 score: 0.3910250663757324\n",
      "[23/1000] train loss: 0.3873357027769089, valid loss: 0.37905365228652954, valid accuracy: 0.9230769230769231\n",
      "======> 23에폭에서 모델 저장. 이전 score: 0.3910250663757324, 현재 score: 0.37905365228652954\n",
      "[24/1000] train loss: 0.37260715663433075, valid loss: 0.36722448468208313, valid accuracy: 0.9300699300699301\n",
      "======> 24에폭에서 모델 저장. 이전 score: 0.37905365228652954, 현재 score: 0.36722448468208313\n",
      "[25/1000] train loss: 0.3611724078655243, valid loss: 0.35553646087646484, valid accuracy: 0.9300699300699301\n",
      "======> 25에폭에서 모델 저장. 이전 score: 0.36722448468208313, 현재 score: 0.35553646087646484\n",
      "[26/1000] train loss: 0.35151907801628113, valid loss: 0.3441035747528076, valid accuracy: 0.9300699300699301\n",
      "======> 26에폭에서 모델 저장. 이전 score: 0.35553646087646484, 현재 score: 0.3441035747528076\n",
      "[27/1000] train loss: 0.3363332152366638, valid loss: 0.3329457938671112, valid accuracy: 0.9370629370629371\n",
      "======> 27에폭에서 모델 저장. 이전 score: 0.3441035747528076, 현재 score: 0.3329457938671112\n",
      "[28/1000] train loss: 0.3273712992668152, valid loss: 0.32204610109329224, valid accuracy: 0.9370629370629371\n",
      "======> 28에폭에서 모델 저장. 이전 score: 0.3329457938671112, 현재 score: 0.32204610109329224\n",
      "[29/1000] train loss: 0.31574489176273346, valid loss: 0.31145426630973816, valid accuracy: 0.9370629370629371\n",
      "======> 29에폭에서 모델 저장. 이전 score: 0.32204610109329224, 현재 score: 0.31145426630973816\n",
      "[30/1000] train loss: 0.3099144697189331, valid loss: 0.30119234323501587, valid accuracy: 0.9370629370629371\n",
      "======> 30에폭에서 모델 저장. 이전 score: 0.31145426630973816, 현재 score: 0.30119234323501587\n",
      "[31/1000] train loss: 0.2976679503917694, valid loss: 0.2912588119506836, valid accuracy: 0.9440559440559441\n",
      "======> 31에폭에서 모델 저장. 이전 score: 0.30119234323501587, 현재 score: 0.2912588119506836\n",
      "[32/1000] train loss: 0.285920113325119, valid loss: 0.2816547453403473, valid accuracy: 0.9440559440559441\n",
      "======> 32에폭에서 모델 저장. 이전 score: 0.2912588119506836, 현재 score: 0.2816547453403473\n",
      "[33/1000] train loss: 0.27641789615154266, valid loss: 0.2723225951194763, valid accuracy: 0.9440559440559441\n",
      "======> 33에폭에서 모델 저장. 이전 score: 0.2816547453403473, 현재 score: 0.2723225951194763\n",
      "[34/1000] train loss: 0.2704887390136719, valid loss: 0.2632768452167511, valid accuracy: 0.9440559440559441\n",
      "======> 34에폭에서 모델 저장. 이전 score: 0.2723225951194763, 현재 score: 0.2632768452167511\n",
      "[35/1000] train loss: 0.26195353269577026, valid loss: 0.2545744478702545, valid accuracy: 0.9440559440559441\n",
      "======> 35에폭에서 모델 저장. 이전 score: 0.2632768452167511, 현재 score: 0.2545744478702545\n",
      "[36/1000] train loss: 0.24840963631868362, valid loss: 0.24619485437870026, valid accuracy: 0.9440559440559441\n",
      "======> 36에폭에서 모델 저장. 이전 score: 0.2545744478702545, 현재 score: 0.24619485437870026\n",
      "[37/1000] train loss: 0.24394817650318146, valid loss: 0.23815280199050903, valid accuracy: 0.9440559440559441\n",
      "======> 37에폭에서 모델 저장. 이전 score: 0.24619485437870026, 현재 score: 0.23815280199050903\n",
      "[38/1000] train loss: 0.23424451053142548, valid loss: 0.23045629262924194, valid accuracy: 0.9440559440559441\n",
      "======> 38에폭에서 모델 저장. 이전 score: 0.23815280199050903, 현재 score: 0.23045629262924194\n",
      "[39/1000] train loss: 0.22539398074150085, valid loss: 0.2230655997991562, valid accuracy: 0.9440559440559441\n",
      "======> 39에폭에서 모델 저장. 이전 score: 0.23045629262924194, 현재 score: 0.2230655997991562\n",
      "[40/1000] train loss: 0.21787703782320023, valid loss: 0.2159857302904129, valid accuracy: 0.9440559440559441\n",
      "======> 40에폭에서 모델 저장. 이전 score: 0.2230655997991562, 현재 score: 0.2159857302904129\n",
      "[41/1000] train loss: 0.2083185613155365, valid loss: 0.2092239111661911, valid accuracy: 0.9440559440559441\n",
      "======> 41에폭에서 모델 저장. 이전 score: 0.2159857302904129, 현재 score: 0.2092239111661911\n",
      "[42/1000] train loss: 0.203307643532753, valid loss: 0.20278117060661316, valid accuracy: 0.9440559440559441\n",
      "======> 42에폭에서 모델 저장. 이전 score: 0.2092239111661911, 현재 score: 0.20278117060661316\n",
      "[43/1000] train loss: 0.20116013288497925, valid loss: 0.1966114640235901, valid accuracy: 0.9440559440559441\n",
      "======> 43에폭에서 모델 저장. 이전 score: 0.20278117060661316, 현재 score: 0.1966114640235901\n",
      "[44/1000] train loss: 0.19360694289207458, valid loss: 0.19074471294879913, valid accuracy: 0.9440559440559441\n",
      "======> 44에폭에서 모델 저장. 이전 score: 0.1966114640235901, 현재 score: 0.19074471294879913\n",
      "[45/1000] train loss: 0.18510805070400238, valid loss: 0.18514180183410645, valid accuracy: 0.9440559440559441\n",
      "======> 45에폭에서 모델 저장. 이전 score: 0.19074471294879913, 현재 score: 0.18514180183410645\n",
      "[46/1000] train loss: 0.18065424263477325, valid loss: 0.17977644503116608, valid accuracy: 0.9440559440559441\n",
      "======> 46에폭에서 모델 저장. 이전 score: 0.18514180183410645, 현재 score: 0.17977644503116608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47/1000] train loss: 0.17158369719982147, valid loss: 0.1746346801519394, valid accuracy: 0.951048951048951\n",
      "======> 47에폭에서 모델 저장. 이전 score: 0.17977644503116608, 현재 score: 0.1746346801519394\n",
      "[48/1000] train loss: 0.16992470622062683, valid loss: 0.16970349848270416, valid accuracy: 0.958041958041958\n",
      "======> 48에폭에서 모델 저장. 이전 score: 0.1746346801519394, 현재 score: 0.16970349848270416\n",
      "[49/1000] train loss: 0.163053996860981, valid loss: 0.16502784192562103, valid accuracy: 0.958041958041958\n",
      "======> 49에폭에서 모델 저장. 이전 score: 0.16970349848270416, 현재 score: 0.16502784192562103\n",
      "[50/1000] train loss: 0.15999384224414825, valid loss: 0.16055622696876526, valid accuracy: 0.958041958041958\n",
      "======> 50에폭에서 모델 저장. 이전 score: 0.16502784192562103, 현재 score: 0.16055622696876526\n",
      "[51/1000] train loss: 0.15443886816501617, valid loss: 0.15631425380706787, valid accuracy: 0.958041958041958\n",
      "======> 51에폭에서 모델 저장. 이전 score: 0.16055622696876526, 현재 score: 0.15631425380706787\n",
      "[52/1000] train loss: 0.15122900158166885, valid loss: 0.15225937962532043, valid accuracy: 0.958041958041958\n",
      "======> 52에폭에서 모델 저장. 이전 score: 0.15631425380706787, 현재 score: 0.15225937962532043\n",
      "[53/1000] train loss: 0.14627226442098618, valid loss: 0.14843042194843292, valid accuracy: 0.958041958041958\n",
      "======> 53에폭에서 모델 저장. 이전 score: 0.15225937962532043, 현재 score: 0.14843042194843292\n",
      "[54/1000] train loss: 0.13869032263755798, valid loss: 0.14478032290935516, valid accuracy: 0.958041958041958\n",
      "======> 54에폭에서 모델 저장. 이전 score: 0.14843042194843292, 현재 score: 0.14478032290935516\n",
      "[55/1000] train loss: 0.13713278621435165, valid loss: 0.14131520688533783, valid accuracy: 0.958041958041958\n",
      "======> 55에폭에서 모델 저장. 이전 score: 0.14478032290935516, 현재 score: 0.14131520688533783\n",
      "[56/1000] train loss: 0.13116908818483353, valid loss: 0.13803735375404358, valid accuracy: 0.965034965034965\n",
      "======> 56에폭에서 모델 저장. 이전 score: 0.14131520688533783, 현재 score: 0.13803735375404358\n",
      "[57/1000] train loss: 0.13080709800124168, valid loss: 0.13493427634239197, valid accuracy: 0.965034965034965\n",
      "======> 57에폭에서 모델 저장. 이전 score: 0.13803735375404358, 현재 score: 0.13493427634239197\n",
      "[58/1000] train loss: 0.12474065646529198, valid loss: 0.13197770714759827, valid accuracy: 0.965034965034965\n",
      "======> 58에폭에서 모델 저장. 이전 score: 0.13493427634239197, 현재 score: 0.13197770714759827\n",
      "[59/1000] train loss: 0.12087266147136688, valid loss: 0.12917906045913696, valid accuracy: 0.965034965034965\n",
      "======> 59에폭에서 모델 저장. 이전 score: 0.13197770714759827, 현재 score: 0.12917906045913696\n",
      "[60/1000] train loss: 0.12370436266064644, valid loss: 0.12655127048492432, valid accuracy: 0.965034965034965\n",
      "======> 60에폭에서 모델 저장. 이전 score: 0.12917906045913696, 현재 score: 0.12655127048492432\n",
      "[61/1000] train loss: 0.11970222368836403, valid loss: 0.12403425574302673, valid accuracy: 0.965034965034965\n",
      "======> 61에폭에서 모델 저장. 이전 score: 0.12655127048492432, 현재 score: 0.12403425574302673\n",
      "[62/1000] train loss: 0.11066396534442902, valid loss: 0.12164473533630371, valid accuracy: 0.965034965034965\n",
      "======> 62에폭에서 모델 저장. 이전 score: 0.12403425574302673, 현재 score: 0.12164473533630371\n",
      "[63/1000] train loss: 0.11210361495614052, valid loss: 0.11936457455158234, valid accuracy: 0.965034965034965\n",
      "======> 63에폭에서 모델 저장. 이전 score: 0.12164473533630371, 현재 score: 0.11936457455158234\n",
      "[64/1000] train loss: 0.11001936718821526, valid loss: 0.1172228753566742, valid accuracy: 0.965034965034965\n",
      "======> 64에폭에서 모델 저장. 이전 score: 0.11936457455158234, 현재 score: 0.1172228753566742\n",
      "[65/1000] train loss: 0.11100682243704796, valid loss: 0.11516530066728592, valid accuracy: 0.965034965034965\n",
      "======> 65에폭에서 모델 저장. 이전 score: 0.1172228753566742, 현재 score: 0.11516530066728592\n",
      "[66/1000] train loss: 0.10001663491129875, valid loss: 0.11318250745534897, valid accuracy: 0.965034965034965\n",
      "======> 66에폭에서 모델 저장. 이전 score: 0.11516530066728592, 현재 score: 0.11318250745534897\n",
      "[67/1000] train loss: 0.1054639145731926, valid loss: 0.11131349205970764, valid accuracy: 0.965034965034965\n",
      "======> 67에폭에서 모델 저장. 이전 score: 0.11318250745534897, 현재 score: 0.11131349205970764\n",
      "[68/1000] train loss: 0.09967485070228577, valid loss: 0.10947350412607193, valid accuracy: 0.972027972027972\n",
      "======> 68에폭에서 모델 저장. 이전 score: 0.11131349205970764, 현재 score: 0.10947350412607193\n",
      "[69/1000] train loss: 0.10022216662764549, valid loss: 0.10771816968917847, valid accuracy: 0.9790209790209791\n",
      "======> 69에폭에서 모델 저장. 이전 score: 0.10947350412607193, 현재 score: 0.10771816968917847\n",
      "[70/1000] train loss: 0.10055634751915932, valid loss: 0.10605910420417786, valid accuracy: 0.9790209790209791\n",
      "======> 70에폭에서 모델 저장. 이전 score: 0.10771816968917847, 현재 score: 0.10605910420417786\n",
      "[71/1000] train loss: 0.09202910959720612, valid loss: 0.10442794859409332, valid accuracy: 0.9790209790209791\n",
      "======> 71에폭에서 모델 저장. 이전 score: 0.10605910420417786, 현재 score: 0.10442794859409332\n",
      "[72/1000] train loss: 0.09356755763292313, valid loss: 0.10281834751367569, valid accuracy: 0.9790209790209791\n",
      "======> 72에폭에서 모델 저장. 이전 score: 0.10442794859409332, 현재 score: 0.10281834751367569\n",
      "[73/1000] train loss: 0.09348753839731216, valid loss: 0.10132042318582535, valid accuracy: 0.9790209790209791\n",
      "======> 73에폭에서 모델 저장. 이전 score: 0.10281834751367569, 현재 score: 0.10132042318582535\n",
      "[74/1000] train loss: 0.09203017503023148, valid loss: 0.09991289675235748, valid accuracy: 0.9790209790209791\n",
      "======> 74에폭에서 모델 저장. 이전 score: 0.10132042318582535, 현재 score: 0.09991289675235748\n",
      "[75/1000] train loss: 0.09160980209708214, valid loss: 0.09854757785797119, valid accuracy: 0.9790209790209791\n",
      "======> 75에폭에서 모델 저장. 이전 score: 0.09991289675235748, 현재 score: 0.09854757785797119\n",
      "[76/1000] train loss: 0.08916105702519417, valid loss: 0.09725802391767502, valid accuracy: 0.986013986013986\n",
      "======> 76에폭에서 모델 저장. 이전 score: 0.09854757785797119, 현재 score: 0.09725802391767502\n",
      "[77/1000] train loss: 0.08265789598226547, valid loss: 0.09605538100004196, valid accuracy: 0.986013986013986\n",
      "======> 77에폭에서 모델 저장. 이전 score: 0.09725802391767502, 현재 score: 0.09605538100004196\n",
      "[78/1000] train loss: 0.08481945842504501, valid loss: 0.09490759670734406, valid accuracy: 0.986013986013986\n",
      "======> 78에폭에서 모델 저장. 이전 score: 0.09605538100004196, 현재 score: 0.09490759670734406\n",
      "[79/1000] train loss: 0.08236772567033768, valid loss: 0.09379579871892929, valid accuracy: 0.986013986013986\n",
      "======> 79에폭에서 모델 저장. 이전 score: 0.09490759670734406, 현재 score: 0.09379579871892929\n",
      "[80/1000] train loss: 0.08322266861796379, valid loss: 0.09272897243499756, valid accuracy: 0.986013986013986\n",
      "======> 80에폭에서 모델 저장. 이전 score: 0.09379579871892929, 현재 score: 0.09272897243499756\n",
      "[81/1000] train loss: 0.08110456168651581, valid loss: 0.09165065735578537, valid accuracy: 0.986013986013986\n",
      "======> 81에폭에서 모델 저장. 이전 score: 0.09272897243499756, 현재 score: 0.09165065735578537\n",
      "[82/1000] train loss: 0.07541202008724213, valid loss: 0.09066328406333923, valid accuracy: 0.986013986013986\n",
      "======> 82에폭에서 모델 저장. 이전 score: 0.09165065735578537, 현재 score: 0.09066328406333923\n",
      "[83/1000] train loss: 0.07941265404224396, valid loss: 0.08972286432981491, valid accuracy: 0.986013986013986\n",
      "======> 83에폭에서 모델 저장. 이전 score: 0.09066328406333923, 현재 score: 0.08972286432981491\n",
      "[84/1000] train loss: 0.07821109145879745, valid loss: 0.08884146809577942, valid accuracy: 0.986013986013986\n",
      "======> 84에폭에서 모델 저장. 이전 score: 0.08972286432981491, 현재 score: 0.08884146809577942\n",
      "[85/1000] train loss: 0.07634446397423744, valid loss: 0.08797800540924072, valid accuracy: 0.986013986013986\n",
      "======> 85에폭에서 모델 저장. 이전 score: 0.08884146809577942, 현재 score: 0.08797800540924072\n",
      "[86/1000] train loss: 0.07483729347586632, valid loss: 0.08712389320135117, valid accuracy: 0.986013986013986\n",
      "======> 86에폭에서 모델 저장. 이전 score: 0.08797800540924072, 현재 score: 0.08712389320135117\n",
      "[87/1000] train loss: 0.07475284673273563, valid loss: 0.08631535619497299, valid accuracy: 0.986013986013986\n",
      "======> 87에폭에서 모델 저장. 이전 score: 0.08712389320135117, 현재 score: 0.08631535619497299\n",
      "[88/1000] train loss: 0.07471394911408424, valid loss: 0.08555307239294052, valid accuracy: 0.986013986013986\n",
      "======> 88에폭에서 모델 저장. 이전 score: 0.08631535619497299, 현재 score: 0.08555307239294052\n",
      "[89/1000] train loss: 0.07270745187997818, valid loss: 0.08485213667154312, valid accuracy: 0.986013986013986\n",
      "======> 89에폭에서 모델 저장. 이전 score: 0.08555307239294052, 현재 score: 0.08485213667154312\n",
      "[90/1000] train loss: 0.068208958953619, valid loss: 0.0841764509677887, valid accuracy: 0.986013986013986\n",
      "======> 90에폭에서 모델 저장. 이전 score: 0.08485213667154312, 현재 score: 0.0841764509677887\n",
      "[91/1000] train loss: 0.06770817935466766, valid loss: 0.0835704356431961, valid accuracy: 0.986013986013986\n",
      "======> 91에폭에서 모델 저장. 이전 score: 0.0841764509677887, 현재 score: 0.0835704356431961\n",
      "[92/1000] train loss: 0.06777507066726685, valid loss: 0.08298134058713913, valid accuracy: 0.986013986013986\n",
      "======> 92에폭에서 모델 저장. 이전 score: 0.0835704356431961, 현재 score: 0.08298134058713913\n",
      "[93/1000] train loss: 0.06695819832384586, valid loss: 0.08241922408342361, valid accuracy: 0.986013986013986\n",
      "======> 93에폭에서 모델 저장. 이전 score: 0.08298134058713913, 현재 score: 0.08241922408342361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[94/1000] train loss: 0.06895466893911362, valid loss: 0.08189462870359421, valid accuracy: 0.986013986013986\n",
      "======> 94에폭에서 모델 저장. 이전 score: 0.08241922408342361, 현재 score: 0.08189462870359421\n",
      "[95/1000] train loss: 0.0627828873693943, valid loss: 0.08139511942863464, valid accuracy: 0.986013986013986\n",
      "======> 95에폭에서 모델 저장. 이전 score: 0.08189462870359421, 현재 score: 0.08139511942863464\n",
      "[96/1000] train loss: 0.06655289232730865, valid loss: 0.08091694116592407, valid accuracy: 0.986013986013986\n",
      "======> 96에폭에서 모델 저장. 이전 score: 0.08139511942863464, 현재 score: 0.08091694116592407\n",
      "[97/1000] train loss: 0.06586004793643951, valid loss: 0.08047621697187424, valid accuracy: 0.986013986013986\n",
      "======> 97에폭에서 모델 저장. 이전 score: 0.08091694116592407, 현재 score: 0.08047621697187424\n",
      "[98/1000] train loss: 0.06564727239310741, valid loss: 0.08004125207662582, valid accuracy: 0.986013986013986\n",
      "======> 98에폭에서 모델 저장. 이전 score: 0.08047621697187424, 현재 score: 0.08004125207662582\n",
      "[99/1000] train loss: 0.061896150931715965, valid loss: 0.07957865297794342, valid accuracy: 0.986013986013986\n",
      "======> 99에폭에서 모델 저장. 이전 score: 0.08004125207662582, 현재 score: 0.07957865297794342\n",
      "[100/1000] train loss: 0.05901315063238144, valid loss: 0.07914850860834122, valid accuracy: 0.986013986013986\n",
      "======> 100에폭에서 모델 저장. 이전 score: 0.07957865297794342, 현재 score: 0.07914850860834122\n",
      "[101/1000] train loss: 0.06244341656565666, valid loss: 0.07873060554265976, valid accuracy: 0.986013986013986\n",
      "======> 101에폭에서 모델 저장. 이전 score: 0.07914850860834122, 현재 score: 0.07873060554265976\n",
      "[102/1000] train loss: 0.06191452778875828, valid loss: 0.07832083851099014, valid accuracy: 0.986013986013986\n",
      "======> 102에폭에서 모델 저장. 이전 score: 0.07873060554265976, 현재 score: 0.07832083851099014\n",
      "[103/1000] train loss: 0.06205269321799278, valid loss: 0.07795392721891403, valid accuracy: 0.986013986013986\n",
      "======> 103에폭에서 모델 저장. 이전 score: 0.07832083851099014, 현재 score: 0.07795392721891403\n",
      "[104/1000] train loss: 0.05730045400559902, valid loss: 0.07759574055671692, valid accuracy: 0.986013986013986\n",
      "======> 104에폭에서 모델 저장. 이전 score: 0.07795392721891403, 현재 score: 0.07759574055671692\n",
      "[105/1000] train loss: 0.06008577533066273, valid loss: 0.07728039473295212, valid accuracy: 0.986013986013986\n",
      "======> 105에폭에서 모델 저장. 이전 score: 0.07759574055671692, 현재 score: 0.07728039473295212\n",
      "[106/1000] train loss: 0.05962102860212326, valid loss: 0.07698895782232285, valid accuracy: 0.9790209790209791\n",
      "======> 106에폭에서 모델 저장. 이전 score: 0.07728039473295212, 현재 score: 0.07698895782232285\n",
      "[107/1000] train loss: 0.05996112339198589, valid loss: 0.07675612717866898, valid accuracy: 0.9790209790209791\n",
      "======> 107에폭에서 모델 저장. 이전 score: 0.07698895782232285, 현재 score: 0.07675612717866898\n",
      "[108/1000] train loss: 0.05937312729656696, valid loss: 0.07651728391647339, valid accuracy: 0.9790209790209791\n",
      "======> 108에폭에서 모델 저장. 이전 score: 0.07675612717866898, 현재 score: 0.07651728391647339\n",
      "[109/1000] train loss: 0.058554237708449364, valid loss: 0.07633676379919052, valid accuracy: 0.9790209790209791\n",
      "======> 109에폭에서 모델 저장. 이전 score: 0.07651728391647339, 현재 score: 0.07633676379919052\n",
      "[110/1000] train loss: 0.05886652134358883, valid loss: 0.07616949081420898, valid accuracy: 0.9790209790209791\n",
      "======> 110에폭에서 모델 저장. 이전 score: 0.07633676379919052, 현재 score: 0.07616949081420898\n",
      "[111/1000] train loss: 0.05591024458408356, valid loss: 0.07600542157888412, valid accuracy: 0.9790209790209791\n",
      "======> 111에폭에서 모델 저장. 이전 score: 0.07616949081420898, 현재 score: 0.07600542157888412\n",
      "[112/1000] train loss: 0.05423396825790405, valid loss: 0.07586244493722916, valid accuracy: 0.9790209790209791\n",
      "======> 112에폭에서 모델 저장. 이전 score: 0.07600542157888412, 현재 score: 0.07586244493722916\n",
      "[113/1000] train loss: 0.05172189325094223, valid loss: 0.07571686059236526, valid accuracy: 0.9790209790209791\n",
      "======> 113에폭에서 모델 저장. 이전 score: 0.07586244493722916, 현재 score: 0.07571686059236526\n",
      "[114/1000] train loss: 0.05311871878802776, valid loss: 0.07559090852737427, valid accuracy: 0.9790209790209791\n",
      "======> 114에폭에서 모델 저장. 이전 score: 0.07571686059236526, 현재 score: 0.07559090852737427\n",
      "[115/1000] train loss: 0.05069018714129925, valid loss: 0.07544999569654465, valid accuracy: 0.9790209790209791\n",
      "======> 115에폭에서 모델 저장. 이전 score: 0.07559090852737427, 현재 score: 0.07544999569654465\n",
      "[116/1000] train loss: 0.05349734611809254, valid loss: 0.0753142237663269, valid accuracy: 0.9790209790209791\n",
      "======> 116에폭에서 모델 저장. 이전 score: 0.07544999569654465, 현재 score: 0.0753142237663269\n",
      "[117/1000] train loss: 0.0549156628549099, valid loss: 0.07517234236001968, valid accuracy: 0.9790209790209791\n",
      "======> 117에폭에서 모델 저장. 이전 score: 0.0753142237663269, 현재 score: 0.07517234236001968\n",
      "[118/1000] train loss: 0.054641230031847954, valid loss: 0.07508229464292526, valid accuracy: 0.9790209790209791\n",
      "======> 118에폭에서 모델 저장. 이전 score: 0.07517234236001968, 현재 score: 0.07508229464292526\n",
      "[119/1000] train loss: 0.05283429101109505, valid loss: 0.0749790295958519, valid accuracy: 0.9790209790209791\n",
      "======> 119에폭에서 모델 저장. 이전 score: 0.07508229464292526, 현재 score: 0.0749790295958519\n",
      "[120/1000] train loss: 0.05174671858549118, valid loss: 0.07489054650068283, valid accuracy: 0.9790209790209791\n",
      "======> 120에폭에서 모델 저장. 이전 score: 0.0749790295958519, 현재 score: 0.07489054650068283\n",
      "[121/1000] train loss: 0.05209534987807274, valid loss: 0.07476430386304855, valid accuracy: 0.9790209790209791\n",
      "======> 121에폭에서 모델 저장. 이전 score: 0.07489054650068283, 현재 score: 0.07476430386304855\n",
      "[122/1000] train loss: 0.049272144213318825, valid loss: 0.07468783855438232, valid accuracy: 0.9790209790209791\n",
      "======> 122에폭에서 모델 저장. 이전 score: 0.07476430386304855, 현재 score: 0.07468783855438232\n",
      "[123/1000] train loss: 0.05036178603768349, valid loss: 0.07462921738624573, valid accuracy: 0.9790209790209791\n",
      "======> 123에폭에서 모델 저장. 이전 score: 0.07468783855438232, 현재 score: 0.07462921738624573\n",
      "[124/1000] train loss: 0.05137873440980911, valid loss: 0.07455689460039139, valid accuracy: 0.9790209790209791\n",
      "======> 124에폭에서 모델 저장. 이전 score: 0.07462921738624573, 현재 score: 0.07455689460039139\n",
      "[125/1000] train loss: 0.0511913038790226, valid loss: 0.07448383420705795, valid accuracy: 0.9790209790209791\n",
      "======> 125에폭에서 모델 저장. 이전 score: 0.07455689460039139, 현재 score: 0.07448383420705795\n",
      "[126/1000] train loss: 0.050125036388635635, valid loss: 0.07444927841424942, valid accuracy: 0.9790209790209791\n",
      "======> 126에폭에서 모델 저장. 이전 score: 0.07448383420705795, 현재 score: 0.07444927841424942\n",
      "[127/1000] train loss: 0.04897177778184414, valid loss: 0.07441923767328262, valid accuracy: 0.9790209790209791\n",
      "======> 127에폭에서 모델 저장. 이전 score: 0.07444927841424942, 현재 score: 0.07441923767328262\n",
      "[128/1000] train loss: 0.04800509847700596, valid loss: 0.07438158243894577, valid accuracy: 0.9790209790209791\n",
      "======> 128에폭에서 모델 저장. 이전 score: 0.07441923767328262, 현재 score: 0.07438158243894577\n",
      "[129/1000] train loss: 0.048562830314040184, valid loss: 0.07440703362226486, valid accuracy: 0.9790209790209791\n",
      "[130/1000] train loss: 0.04939885623753071, valid loss: 0.07440538704395294, valid accuracy: 0.9790209790209791\n",
      "[131/1000] train loss: 0.04712957702577114, valid loss: 0.07441128045320511, valid accuracy: 0.9790209790209791\n",
      "[132/1000] train loss: 0.04902133531868458, valid loss: 0.07440544664859772, valid accuracy: 0.9790209790209791\n",
      "[133/1000] train loss: 0.04688289575278759, valid loss: 0.0744030699133873, valid accuracy: 0.9790209790209791\n",
      "[134/1000] train loss: 0.04725641570985317, valid loss: 0.07441818714141846, valid accuracy: 0.9790209790209791\n",
      "[135/1000] train loss: 0.03259496111422777, valid loss: 0.07439351081848145, valid accuracy: 0.9790209790209791\n",
      "[136/1000] train loss: 0.046500325202941895, valid loss: 0.0743698701262474, valid accuracy: 0.9790209790209791\n",
      "======> 136에폭에서 모델 저장. 이전 score: 0.07438158243894577, 현재 score: 0.0743698701262474\n",
      "[137/1000] train loss: 0.04477032646536827, valid loss: 0.07436234503984451, valid accuracy: 0.9790209790209791\n",
      "======> 137에폭에서 모델 저장. 이전 score: 0.0743698701262474, 현재 score: 0.07436234503984451\n",
      "[138/1000] train loss: 0.04636702314019203, valid loss: 0.07434574514627457, valid accuracy: 0.9790209790209791\n",
      "======> 138에폭에서 모델 저장. 이전 score: 0.07436234503984451, 현재 score: 0.07434574514627457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[139/1000] train loss: 0.045743003487586975, valid loss: 0.07433395087718964, valid accuracy: 0.9790209790209791\n",
      "======> 139에폭에서 모델 저장. 이전 score: 0.07434574514627457, 현재 score: 0.07433395087718964\n",
      "[140/1000] train loss: 0.04373288340866566, valid loss: 0.07429756224155426, valid accuracy: 0.9790209790209791\n",
      "======> 140에폭에서 모델 저장. 이전 score: 0.07433395087718964, 현재 score: 0.07429756224155426\n",
      "[141/1000] train loss: 0.04374771751463413, valid loss: 0.07427441328763962, valid accuracy: 0.9790209790209791\n",
      "======> 141에폭에서 모델 저장. 이전 score: 0.07429756224155426, 현재 score: 0.07427441328763962\n",
      "[142/1000] train loss: 0.04494832642376423, valid loss: 0.07425477355718613, valid accuracy: 0.9790209790209791\n",
      "======> 142에폭에서 모델 저장. 이전 score: 0.07427441328763962, 현재 score: 0.07425477355718613\n",
      "[143/1000] train loss: 0.04226198233664036, valid loss: 0.07426010072231293, valid accuracy: 0.9790209790209791\n",
      "[144/1000] train loss: 0.043973423540592194, valid loss: 0.07429669052362442, valid accuracy: 0.9790209790209791\n",
      "[145/1000] train loss: 0.04291847161948681, valid loss: 0.07429720461368561, valid accuracy: 0.9790209790209791\n",
      "[146/1000] train loss: 0.04147753491997719, valid loss: 0.07431821525096893, valid accuracy: 0.9790209790209791\n",
      "[147/1000] train loss: 0.04369429871439934, valid loss: 0.07436742633581161, valid accuracy: 0.9790209790209791\n",
      "[148/1000] train loss: 0.04224495403468609, valid loss: 0.07436560094356537, valid accuracy: 0.9790209790209791\n",
      "[149/1000] train loss: 0.042286669835448265, valid loss: 0.07439565658569336, valid accuracy: 0.9790209790209791\n",
      "[150/1000] train loss: 0.03508570045232773, valid loss: 0.0744464322924614, valid accuracy: 0.9790209790209791\n",
      "[151/1000] train loss: 0.04272650368511677, valid loss: 0.07451531291007996, valid accuracy: 0.9790209790209791\n",
      "[152/1000] train loss: 0.0408405065536499, valid loss: 0.07456453144550323, valid accuracy: 0.9790209790209791\n",
      "[153/1000] train loss: 0.04207960143685341, valid loss: 0.07459144294261932, valid accuracy: 0.9790209790209791\n",
      "[154/1000] train loss: 0.041725799441337585, valid loss: 0.07462753355503082, valid accuracy: 0.9790209790209791\n",
      "[155/1000] train loss: 0.040900939144194126, valid loss: 0.07461261749267578, valid accuracy: 0.9790209790209791\n",
      "[156/1000] train loss: 0.040585000067949295, valid loss: 0.07459080964326859, valid accuracy: 0.9790209790209791\n",
      "[157/1000] train loss: 0.039985157549381256, valid loss: 0.07456880807876587, valid accuracy: 0.9790209790209791\n",
      "[158/1000] train loss: 0.03581319749355316, valid loss: 0.07457175105810165, valid accuracy: 0.9790209790209791\n",
      "[159/1000] train loss: 0.04008258692920208, valid loss: 0.07464341074228287, valid accuracy: 0.9790209790209791\n",
      "[160/1000] train loss: 0.03173396922647953, valid loss: 0.07472581416368484, valid accuracy: 0.9790209790209791\n",
      "[161/1000] train loss: 0.03950326144695282, valid loss: 0.07475379854440689, valid accuracy: 0.986013986013986\n",
      "[162/1000] train loss: 0.03810693696141243, valid loss: 0.0747314840555191, valid accuracy: 0.9790209790209791\n",
      "########### Early Stop: 162\n",
      "학습시간: 1.9893321990966797초\n"
     ]
    }
   ],
   "source": [
    "#모델 생성\n",
    "import time\n",
    "#하이퍼파라미터\n",
    "LR=0.001\n",
    "N_EPOCH=1000\n",
    "\n",
    "\n",
    "model = BCModel().to(device)\n",
    "#loss 함수\n",
    "loss_fn = nn.BCELoss() #binary cross entropy loss\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=LR)\n",
    "\n",
    "\n",
    "\n",
    "s=time.time()\n",
    "######################################################\n",
    "#에폭별 검증 : train loss, validation loss, validation accuracy\n",
    "#조기종료(early stop) -성능 개선이 안되면 학습을 중단한다.\n",
    "#가장 좋은 성능을 내는 에폭의 모델을 저장.\n",
    "# 조기종료/모델 저장 ==>validation loss 기준.\n",
    "\n",
    "\n",
    "\n",
    "#결과 정할 리스트\n",
    "train_loss_list,valid_loss_list,valid_acc_list =[],[],[]\n",
    "\n",
    "\n",
    "###모델 저장, 조기종료 관련 변수\n",
    "best_score = torch.inf #validation loss\n",
    "save_bcmodel_path = \"models/bc_best_model.pth\"\n",
    "\n",
    "patience = 20 #20번 정도는 기다려 보지 뭐.\n",
    "#성능이 개선 될 때까지 몇 에폭 기다릴 것인지....\n",
    "\n",
    "\n",
    "trigger_cnt = 0\n",
    "#성능이 개선 될 때까지 현재 몇 번째 기다렸는지. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(N_EPOCH):\n",
    "    \n",
    "    #train\n",
    "    model.train()\n",
    "    train_loss=0.0\n",
    "    \n",
    "    \n",
    "    #step\n",
    "    for X_train,y_train in train_loader:\n",
    "        X_train,y_train = X_train.to(device),y_train.to(device)\n",
    "        #예측\n",
    "        pred_train =model(X_train) #예측\n",
    "        loss = loss_fn(pred_train, y_train) #오차 계산\n",
    "        \n",
    "        \n",
    "        \n",
    "        #파라미터 업데이트 \n",
    "        optimizer.zero_grad() #초기화\n",
    "        loss.backward() #grad 계산\n",
    "        optimizer.step() #파라미터 update\n",
    "        \n",
    "        train_loss+=loss.item()\n",
    "    train_loss /= len(train_loader) #현재 epoch의 평균 train loss 계산.\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    ###vaildation\n",
    "    model.eval()\n",
    "    valid_loss,valid_acc =0.0,0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_valid,y_valid in test_loader:\n",
    "            X_valid,y_valid = X_valid.to(device),y_valid.to(device)\n",
    "            pred_valid = model(X_valid) #값 1개 - positive일 확률\n",
    "            pred_label = (pred_valid >= 0.5).type(torch.int32) #label ==>정확도 계산\n",
    "            \n",
    "            \n",
    "            \n",
    "            #loss\n",
    "            loss_valid = loss_fn(pred_valid,y_valid)\n",
    "            valid_loss += loss_valid.item()\n",
    "            \n",
    "            #정확도\n",
    "            valid_acc += torch.sum(pred_label == y_valid).item()\n",
    "    \n",
    "    \n",
    "    #valid 검증 결과 계산\n",
    "    valid_loss /= len(test_loader)\n",
    "    valid_acc /= len(test_loader.dataset)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f\"[{epoch+1}/{N_EPOCH}] train loss: {train_loss}, valid loss: {valid_loss}, valid accuracy: {valid_acc}\")\n",
    "    \n",
    "    #데이터를 저장한다.\n",
    "    train_loss_list.append(train_loss)\n",
    "    valid_loss_list.append(valid_loss)\n",
    "    valid_acc_list.append(valid_acc)\n",
    "    ### 모델 저장 및 조기종료 처리\n",
    "    \n",
    "    if valid_loss < best_score: # 성능 개선\n",
    "        print(f\"======> {epoch+1}에폭에서 모델 저장. 이전 score: {best_score}, 현재 score: {valid_loss}\")\n",
    "        torch.save(model,save_bcmodel_path)\n",
    "        best_score = valid_loss\n",
    "        trigger_cnt = 0\n",
    "    else: #성능개선이 되지 않았다는 이야기이다.\n",
    "        trigger_cnt += 1\n",
    "        if patience == trigger_cnt:\n",
    "            print(f\"########### Early Stop: {epoch+1}\")\n",
    "            #이렇게 하면, 언제 중단을 했는지 알 수 있다.\n",
    "            break\n",
    "\n",
    "e=time.time()\n",
    "print(f\"학습시간: {e-s}초\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#학습 결과를 그래프로 그려보자.\n",
    "\n",
    "plt.plot(train_loss_list, label=\"train\")\n",
    "plt.plot(valid_loss_list, label=\"validation\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#보다 보면, validation의 수치가 적어졌다가 확 올라간 것을 알 수 있다. \n",
    "#그러므로, 중간에 모델을 stop 하는 것이 중요하다는 것을 알 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 train 결과 확인/평가\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load(save_bcmodel_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([143, 1])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모델이 학습한 데이터는 전처리 된 것이다.(standard scaling)\n",
    "#예측(추론) 할 데이터도 같은 전처리를 해야 한다.\n",
    "\n",
    "\n",
    "\n",
    "pred_new = best_model(X_test_tensor)\n",
    "pred_new.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9980],\n",
       "        [0.9922],\n",
       "        [0.0126],\n",
       "        [0.9998],\n",
       "        [0.9999]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_new[:5] #뭐, 쉽게 말하면, 확률값 같은 것이다. positive일 확률을 구하는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#확률 ->class index\n",
    "\n",
    "pred_new_label = (pred_new >0.5).type(torch.int32)\n",
    "pred_new_label[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tensor[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
