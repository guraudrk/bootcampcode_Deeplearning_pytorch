{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c0614b",
   "metadata": {},
   "source": [
    "# Tensor 생성\n",
    "- 파이토치에서 데이터를 저장하는 자료구조\n",
    "- ndarray와 성격, 사용법이 유사하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72cb5fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.array와 torch.tensor는 구조가 동일하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d118ec",
   "metadata": {},
   "source": [
    "##  원하는 형태(shape) 텐서 생성\n",
    "- **torch.tensor(자료구조 \\[, dtype\\])**\n",
    "    - 지정한 dtype(Data type)에 맞는 Tensor객체를 생성해서 반환한다.\n",
    "    - \n",
    "## 특정 타입의 Tensor를 직접 생성\n",
    "- torch.tensor()로 생성하면서 dtype을 지정하면 아래 타입의 Tensor객체가 생성된다.\n",
    "- 원하는 Type의 Tensor클래스를 이용해 직접 생성해도 된다.\n",
    "- **torch.FloatTensor(자료구조)**\n",
    "    - float32 타입 텐서 생성\n",
    "- **torch.LongTensor(자료구조)** \n",
    "    - int64 타입 텐서생성\n",
    "- 그외\n",
    "    - BoolTensor(bool), CharTensor(int8), ShortTensor(int16), IntTensor(int32), DoubleTensor(float64)\n",
    "    \n",
    "## tensor 상태 조회\n",
    "- **tensor.shape, tensor.size(\\[축번호\\])**\n",
    "    -  tensor의 shape조회\n",
    "- **tensor.dtype, tensor.type()**\n",
    "    - tensor 원소들의 데이터타입 조회\n",
    "    - dtype은 **data type**을 type()은 tensor **객체의 클래스 타입**을 반환한다.\n",
    "- **tensor.ndim, tensor.dim()**  : tensor 차원\n",
    "- **tensor.numel()**: 전체 원소 개수\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f81163c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0+cu118'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "370e0dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([2, 2]) torch.Size([2, 2])\n",
      "축별 크기: 2 2\n",
      "type: torch.FloatTensor torch.float32\n",
      "차원크기: 2 2\n",
      "원소개수: 4\n",
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,2],[3,4]], dtype=torch.float32)\n",
    "print(\"shape:\", a.shape, a.size())\n",
    "print(\"축별 크기:\", a.shape[0], a.size(0))\n",
    "print(\"type:\", a.type(), a.dtype)\n",
    "print('차원크기:', a.dim(), a.ndim)\n",
    "print('원소개수:', a.numel())\n",
    "print(\"device:\", a.device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27f077a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(range(10))\n",
    "#이것도 np.array와 구조가 유사하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e40a7f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.int32\n",
      "torch.float64\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "#Float, Double(32, 64bit 실수)/Int, Long(32, 64 bit 정수) type Tensor\n",
    "b = torch.FloatTensor([1,3,7])  #float32\n",
    "print(b.dtype)\n",
    "c = torch.IntTensor([10,20,30]) # int64\n",
    "print(c.dtype)\n",
    "d = torch.DoubleTensor([1, 2, 3]) #torch.tensor([],dtype=torch.float64)\n",
    "print(d.dtype)\n",
    "e = torch.LongTensor([10, 20, 30, 40]) #torch.tensor([],dtype=torch.int64)\n",
    "print(e.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57bc602",
   "metadata": {},
   "source": [
    "## 특정 값으로 구성된 Tensor 생성\n",
    "- **torch.zeros(\\*size), zeros_like(텐서)**: 0으로 구성된 tensor 생성\n",
    "- **torch.ones(\\*size), ones_like(텐서)**: 1로 구성된 tensor생성\n",
    "- **torch.full(\\*size), full_like(텐서)**: 지정한 값으로 구성된 tensor생성\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "801e4ffc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(3,2,3)\n",
    "# torch.ones(2,3)\n",
    "# torch.full((3,2), fill_value=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1697cfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2],[3,4]])\n",
    "print(a.shape)\n",
    "b = torch.zeros_like(a) #a와 같은 사이즈의, a로만 이뤄진 tensor 생성.\n",
    "# b = torch.ones_like(a)\n",
    "# b = torch.full_like(a, 20)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01301e5c",
   "metadata": {},
   "source": [
    "## 동일한 간격으로 떨어진 값들로 구성된 배열생성\n",
    "- **torch.arange(start=0, end, step=1)** \n",
    "- **torch.linspace(start, end, steps,)** : steps - 원소개수\n",
    "- torch.arange나 torch.linspace는 넘파이의 그것과 구조가 완전히 동일하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9b9fd1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(10) #0에서부터 9까지!\n",
    "# torch.arange(0, 1, 0.1)\n",
    "# torch.arange(10, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afa883dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
       "        0.9000, 1.0000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(0, 10, 5) #0~10, 등분한값 5개\n",
    "torch.linspace(0, 1, 11) #0~1, 등분한값 10개\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bd2a03",
   "metadata": {},
   "source": [
    "## 빈 tensor 생성\n",
    "- **torch.empty(\\*size)**\n",
    "- 값이 아예 없을 수는 없으니, 그냥 의미 없는 값들로 채운다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9a865f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec59585",
   "metadata": {},
   "source": [
    "## 난수를 이용한 생성\n",
    "\n",
    "- **torch.rand(\\*size)**: 0 ~ 1사이 실수로 구성된 배열을 생성. 각 값은 균등분포를 따른다.\n",
    "- **torch.randn(\\*size)**: 표준정규분포(평균:0, 표준편차:1)를 따르는 실수로 구성된 배열 생성\n",
    "- **torch.randint(low=0, high, size)**: 지정한 범위의 정수로 구성된 배열 생성\n",
    "- **torch.randperm(n)**: 0 ~ n 사이의 정수를 랜덤하게 섞은 값을 원소로 가지는 배열 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "def1448b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 0, 4, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1004)  # seed 설정\n",
    "torch.rand(100,3)\n",
    "torch.randn(3,3) #3개의 숫자를 정규분포 3으로 생성\n",
    "torch.randint(1, 10, (3,3))  #범위는 1부터 10가지, 배열은 (3,3)\n",
    "torch.randperm(5) #막상 5는 포함하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9df51a",
   "metadata": {},
   "source": [
    "## tensor를 상수로 변환\n",
    "- tensor객체.item()\n",
    "    - Scalar(상수) tensor를 python 상수로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e24dc70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10) torch.int64 torch.LongTensor\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(10)\n",
    "print(a,a.dtype,a.type())\n",
    "print(a.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "340282bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.randperm(9) #index를 섞어서 데이터를 섞는다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fde801d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10)\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(10)\n",
    "print(a)\n",
    "print(a.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d62c3168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20])\n",
      "torch.Size([1])\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "b = torch.tensor([20])\n",
    "print(b)\n",
    "print(b.shape) #size는 1이다.\n",
    "print(b.item()) #원소가 하나인 배열 변환 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cefb6f7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  1,  10, 100]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "c = torch.tensor([1, 10, 100])\n",
    "print(c,c.shape)\n",
    "# print(c.item()) #원소가 여러개일 경우 Exception발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06f1ae95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-f654de0b89c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#device=\"cuda\" => VRAM에 저장한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#근데 우리는 CUDA GPU가 없으니, 실행을 시키면 에러가 난다. 그러니 에러가 나도 그러려니 하자.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"LAZY\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m         \u001b[1;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[1;31m# we need to just return without initializing in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "d = torch.tensor([10], device='cuda') #device=\"cuda\" => VRAM에 저장한다. \n",
    "#근데 우리는 CUDA GPU가 없으니, 실행을 시키면 에러가 난다. 그러니 에러가 나도 그러려니 하자.\n",
    "print(d)\n",
    "print(d.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d046c10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8709f0b",
   "metadata": {},
   "source": [
    "## ndarray 호환\n",
    "\n",
    "- ndarray를 tensor로 생성(의외로 바꾸는 경우가 상당히 많다.)\n",
    "    - **torch.tensor(ndarray)**\n",
    "    - **torch.from_numpy(ndarray)**\n",
    "- tensor를 ndarray로 변환\n",
    "    - **tensor.numpy()**\n",
    "    - tensor가 gpu에 있을 경우 cpu로 옮긴 뒤 변환해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff396b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d74e77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c1d1b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ndarray -> tensor\n",
    "arr = np.arange(1,10)\n",
    "\n",
    "\n",
    "\n",
    "#ndarray에서 tensor로 바꾸기 위해서는 아래의 2가지 방법 중 한가지를 쓰면 된다.\n",
    "torch.tensor(arr)\n",
    "torch.from_numpy(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a5fa181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5668,  0.3922,  0.1592],\n",
      "        [-1.3388,  0.6535,  0.6062],\n",
      "        [ 0.0410, -0.5964,  1.1524]])\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.56676835,  0.3922311 ,  0.15919687],\n",
       "       [-1.3387808 ,  0.6534819 ,  0.6062082 ],\n",
       "       [ 0.04101773, -0.596369  ,  1.152389  ]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor -> ndarray\n",
    "t = torch.randn(3,3) #random으로 생성하는 것.\n",
    "print(t)\n",
    "b=t.numpy()\n",
    "print(type(b))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d85bf50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4826, -0.5480],\n",
       "        [ 0.4243, -0.2416]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.randn(2,2)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28c9d47a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.48258954, -0.5479588 ],\n",
       "       [ 0.42434272, -0.24164915]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#VRAM에 있는 Tensor 객체는 ndarray로 변환이 안된다.\n",
    "#VRAM의 Tensor를 ram으로 이동시킨 뒤에 변환을 할 수 있다.\n",
    "#t2.numpy()    #이 코드로 실행을 시키면 에러가 난다. \n",
    "t2.to(\"cpu\").numpy() #이 코드는 텐서를 cpu로 옮기라고 하는 코드이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8494a6a",
   "metadata": {},
   "source": [
    "## Tensor gpu/cpu 메모리로 옮기기\n",
    "\n",
    "- pytorch는 데이터셋인 tensor를 cpu메모리와 gpu 메모리로 서로 옮길 수 있다.\n",
    "    - 데이터에 대한 연산처리를 어디서 하느냐에 따라 메모리를 선택한다.\n",
    "    - 장치는 문자열로 설정한다.\n",
    "        - CPU 사용: \"cpu\"\n",
    "        - nvida GPU: \"cuda\"\n",
    "        - Apple m1: \"mps\"\n",
    "            - pytorch 1.12 부터 지원\n",
    "- 옮기기\n",
    "    - tensor 생성시 `device` 파라미터를 이용해 설정\n",
    "    - `tensor.to(device)`를 이용해 설정\n",
    "- 현재 실행환경에서 어떤 장비를 사용할 수 있는지 확인\n",
    "    - nvidia gpu 사용가능확인\n",
    "        - `torch.cuda.is_available()` - nvida gpu 사용가능 여부\n",
    "        - `torch.backends.mps.is_available()` - M1 사용가능 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cc6387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf69bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "92ffadca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-cc358558bb4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# t2 = t.to(\"cpu\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# t2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([1, 2, 3], dtype=torch.float32, device=device)\n",
    "t\n",
    "\n",
    "# t2 = t.to(\"cpu\")\n",
    "# t2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10f4911",
   "metadata": {},
   "source": [
    "# 원소 조회및 변경 - indexing/slicing\n",
    "\n",
    "- 대부분 Numpy 와 동일\n",
    "    - **slicing에서 step을 <u>음수로 지정할 수 없다.</u>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2300078c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -8,  -4,   1,  -6,  -3,  -2,   5,  -9,   2,  -9,  -9,   4,   0,   0,\n",
       "          4,  -7,  -7,  -1,   7,  -7,  -2,   9,   5,  -1,  -2,   0,   8,   3,\n",
       "          6,  -1, -10,  -2,  -4,  -6,  -2,   0,   3,   2,   8,   9,  -4,   4,\n",
       "         -1,  -5,  -3,  -3,  -7,   3,  -5,   6,  -7,   1,  -9,   0,   3,   5,\n",
       "         -6,  -6,   4,  -6,   5,   0,   5,   1,  -1,  -9,  -6,  -9,   2,   3,\n",
       "          4,   0,  -7, -10,  -1,   5,   1,   8,   0,   9,   0,   7,   4,   6,\n",
       "         -8,   5,   0,   2,  -4,  -5,   9,  -2, -10,   0,  -7,   9,   8,  -9,\n",
       "         -6,  -6])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randint(-10, 10, (100, ))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a28537ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-8), tensor([-4, -2, -6]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0], t[[1, 5, -1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73763041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-9, -9, -2, -6, -4])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:5]\n",
    "t[10:15]\n",
    "t[90:]\n",
    "t[3:30:3]\n",
    "# t[10:1:-2]  #에러\n",
    "t[1:10:2].flip(dims=(0,)) #reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81965586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 5, 2, 4, 4, 7, 9, 5, 8, 3, 6, 3, 2, 8, 9, 4, 3, 6, 1, 3, 5, 4, 5, 5,\n",
       "        1, 2, 3, 4, 5, 1, 8, 9, 7, 4, 6, 5, 2, 9, 9, 8])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# boolean index-----참인 것만 출력한다.\n",
    "t[t > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "03a01027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([100,  -4,   1,  -6,  -3,  -2,   5,  -9,   2,  -9,  -9,   4,   0,   0,\n",
       "          4,  -7,  -7,  -1,   7,  -7,  -2,   9,   5,  -1,  -2,   0,   8,   3,\n",
       "          6,  -1, -10,  -2,  -4,  -6,  -2,   0,   3,   2,   8,   9,  -4,   4,\n",
       "         -1,  -5,  -3,  -3,  -7,   3,  -5,   6,  -7,   1,  -9,   0,   3,   5,\n",
       "         -6,  -6,   4,  -6,   5,   0,   5,   1,  -1,  -9,  -6,  -9,   2,   3,\n",
       "          4,   0,  -7, -10,  -1,   5,   1,   8,   0,   9,   0,   7,   4,   6,\n",
       "         -8,   5,   0,   2,  -4,  -5,   9,  -2, -10,   0,  -7,   9,   8,  -9,\n",
       "         -6,  -6])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 변경 ---tensor도 값을 변경할 수 있다는 것만 기억하자.\n",
    "t[0] = 100\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "49419f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(1, 10).reshape(3,3)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "15855bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[[1,0], [2,2]]  #(1,2), (0,2)의 값을 알아보자! 값을 조회하는 방법이 살짝 다를 수 있다. 그러니 혼동 주의.\n",
    "\n",
    "t[0,2] #0행 2열의 값을 살펴보자! 그러면 값이 3이 나오는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02f14d8",
   "metadata": {},
   "source": [
    "# Reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221e70f8",
   "metadata": {},
   "source": [
    "## shape 변경\n",
    "- tensor객체.reshape(\\*shape) / view(\\*shape) 이용\n",
    "    - 변환 후 값을 변경하면 원본 배열의 값도 같이 바뀐다.\n",
    " > tensor.clone(): tensor를 복제한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "71f88268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12]) torch.Size([3, 4]) torch.Size([3, 2, 2]) torch.Size([3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "a=torch.rand(12) #(12, )\n",
    "a2 = a.reshape(3,4) #3*4\n",
    "a3 = a.reshape((3,2,2)) #3*2*2\n",
    "a4 = a.reshape((3,2,-1))  #한 개 axis는 -1로 설정가능하고 그럼 계산해서 알아서 설정해 준다. 숫자를 알아서 설정해준다는 거지.\n",
    "print(a.shape, a2.size(), a3.shape, a4.shape)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6744974f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12]) torch.Size([3, 4]) torch.Size([3, 2, 2]) torch.Size([3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "#shape 대신 view를 썼는데, 두 메소드 중 어떤 것을 써도 효과는 동일하다.\n",
    "a5 = a.view(3,4) \n",
    "a6 = a.view((3,2,2))\n",
    "a7 = a.view((3,2,-1))  #한개 axis는 -1로 설정가능\n",
    "print(a.shape, a5.size(), a6.shape, a7.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8ace33cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12.1000, 15.1000,  0.4531,  0.1033,  0.2978,  0.7002,  0.0829,  0.5981,\n",
      "         0.8110,  0.0473,  0.2827,  0.5073])\n"
     ]
    }
   ],
   "source": [
    "a5[0, 0] = 12.1\n",
    "a2[0, 1] = 15.1\n",
    "\n",
    "print(a) \n",
    "#a5,a2의 값을 바뀌었는데 원본의 값이 바뀌었다. 그래서.... 이거는 얇은 복사라고 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "88561ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4249, 0.9438, 0.4531, 0.1033, 0.2978, 0.7002, 0.0829, 0.5981, 0.8110,\n",
      "        0.0473, 0.2827, 0.5073])\n"
     ]
    }
   ],
   "source": [
    "# tensor복사: clone() 메소드\n",
    "r = a.clone().reshape(3,4)\n",
    "r[0,0] = 100.1\n",
    "print(a)\n",
    "#r은 단순한 clone일 뿐이기 때문에, a의 값이 바뀌지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8c2fa4",
   "metadata": {},
   "source": [
    "## dummy 축 늘리기\n",
    "\n",
    "- None을 이용 (numpy의 newaxis 대신 None을 사용한다.)\n",
    "- unsqueeze(dim=축번호)(squeeze는 줄이는 거니까. 반대지롱!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "356a078c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "torch.Size([1, 2, 2]) torch.Size([1, 2, 2])\n",
      "torch.Size([2, 2, 1]) torch.Size([2, 2, 1])\n",
      "torch.Size([2, 1, 2, 1]) torch.Size([2, 1, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([[10,20],[10,20]])\n",
    "print(a.shape) #원본 shape는 (2,2)이다. 이를 잘 파악해두자.\n",
    "\n",
    "a1, a2 = a[None, :], a.unsqueeze(dim=0) #늘어나는 곳에 none을 준다. \n",
    "print(a1.shape, a2.shape)\n",
    "\n",
    "a3, a4 = a[:, :, None], a.unsqueeze(dim=-1)  #끝을 늘린다. dim을 통해 축번호를 설정한다.\n",
    "print(a3.shape, a4.shape)\n",
    "\n",
    "a5, a6 = a3[:,None,:,:], a3.unsqueeze(dim=1) #중간에 늘리는 것을 쏙 끼우는 느낌이다.\n",
    "print(a5.shape, a6.shape)\n",
    "\n",
    "\n",
    "#특이한 것은, unsqueeze와 None을 같이 사용한다는 것이다.\n",
    "#unsqueeze와 dim 함수를 적절하게 쓰는 것이 좋다. 그래야 더 간단하게 할 수 있으니."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9947b6ac",
   "metadata": {},
   "source": [
    "## dummy 축 제거\n",
    "- squeeze(\\[dim=축번호\\]) 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4e40b9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 3, 1])\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3, 1])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "t = torch.rand(3, 1, 3, 1)\n",
    "print(t.shape)\n",
    "\n",
    "r1 = t.squeeze()  #모두 제거\n",
    "print(r1.shape)\n",
    "\n",
    "r2 = t.squeeze(dim=1) # 특정 axis 제거\n",
    "print(r2.shape)\n",
    "\n",
    "r3 = t.squeeze(dim=[1,3]) # 여러 axis의 dummy 축 제거\n",
    "print(r3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3018717",
   "metadata": {},
   "source": [
    "## tensor 합치기\n",
    "torch.cat([tensorA, tensorB, ...], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7564c2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4],\n",
      "        [5, 6, 7, 8, 9]])\n",
      "tensor([[10, 11, 12, 13, 14],\n",
      "        [15, 16, 17, 18, 19]])\n",
      "tensor([[20, 21, 22, 23, 24],\n",
      "        [25, 26, 27, 28, 29]])\n",
      "tensor([[10, 11, 12],\n",
      "        [13, 14, 15],\n",
      "        [16, 17, 18]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(10).reshape(2,5) #0-9\n",
    "b = torch.arange(10,20).reshape(2,5) #10-19\n",
    "c = torch.arange(20,30).reshape(2,5) #20-29\n",
    "d = torch.arange(10,19).reshape(3,3) #10-18\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a633d695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14],\n",
       "        [15, 16, 17, 18, 19]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([a, b], dim=0) #합칠건데, 0번을 기준으로 합칠 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6ee08217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14],\n",
       "        [15, 16, 17, 18, 19],\n",
       "        [20, 21, 22, 23, 24],\n",
       "        [25, 26, 27, 28, 29]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([a, b, c], dim=0) #합칠건데, 0번을 기준으로 합칠 것이다. 그러면 2가 3번 '더해져서' (6,5)가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "edb2f33a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4, 10, 11, 12, 13, 14],\n",
       "        [ 5,  6,  7,  8,  9, 15, 16, 17, 18, 19]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([a, b], axis=1) # dim 대신 axis사용가능\n",
    "#가로로 그대로 늘어난다. 모양을 보면 알 수 있을 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "25f7277a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4, 10, 11, 12, 13, 14, 20, 21, 22, 23, 24],\n",
       "        [ 5,  6,  7,  8,  9, 15, 16, 17, 18, 19, 25, 26, 27, 28, 29]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([a, b, c], axis=1) #-1은 마지막축이다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "82477d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4, 10, 11, 12, 13, 14],\n",
       "        [ 5,  6,  7,  8,  9, 15, 16, 17, 18, 19]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " torch.cat([a, b], axis=-1) #음수는 뒤에서부터!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f441999c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 5 but got size 3 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-4f4e38dd8832>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#Error가 난 이유는 합치려는 기준 축 이외의 축 size가 다르기 때문이다. (2,5)와 (3,3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 5 but got size 3 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    " torch.cat([a, d])  #Error가 난 이유는 합치려는 기준 축 이외의 축 size가 다르기 때문이다. (2,5)와 (3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aaa81f",
   "metadata": {},
   "source": [
    "## 값의 위치(index) 변경\n",
    "- tensor 원소의 축별 index의 위치를 바꾼다.(간단히 생각하면 편하다.)\n",
    "- `tensor.transpose(axis1, axis2)` \n",
    "    - 두 축의 자리만 변경 할 수 있다.\n",
    "- `tensor.permute(axis1, axis2, axis3, ..)`\n",
    "    - 두 개 이상의 축 자리를 변경한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "22ad2f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3]) torch.Size([3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n[0,0]=>[0,0] #0,0은 축을 바꿔도 변함이 없다.\\n[0,1]=>[1,0] #0,1은 서로의 위치를 바꾼다.\\n\\n\\n'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= torch.arange(12).reshape(4,3)\n",
    "b= a.transpose(1,0) #index의 1축을 0축으로, 0축을 1축을 바꾼다.\n",
    "print(a.shape,b.shape)\n",
    "#변동\n",
    "\n",
    "\"\"\"\n",
    "[0,0]=>[0,0] #0,0은 축을 바꿔도 변함이 없다.\n",
    "[0,1]=>[1,0] #0,1은 서로의 위치를 바꾼다.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c621ab2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "torch.Size([2, 4, 3])\n",
      "torch.Size([4, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "print(X.shape)\n",
    "X\n",
    "\n",
    "y = X.transpose(1, 2) #1번의 위치와 2번의 위치를 바꾼다.\n",
    "print(y.shape)\n",
    "\n",
    "z = X.permute(2, 0, 1) #2번 축을 0번 위치로, 0번축을 1번 위치로, 1번축을 2번 위치로 보낸다.\n",
    "print(z.shape)\n",
    "\n",
    "#transpose와 permute의 차이를 잘 보자. 그리고, 어떤 때에 뭘 써야 할지 잘 파악해두자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e31c8141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "55775c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  4,  8],\n",
       "         [ 1,  5,  9],\n",
       "         [ 2,  6, 10],\n",
       "         [ 3,  7, 11]],\n",
       "\n",
       "        [[12, 16, 20],\n",
       "         [13, 17, 21],\n",
       "         [14, 18, 22],\n",
       "         [15, 19, 23]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9f9c77ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  4,  8],\n",
       "         [12, 16, 20]],\n",
       "\n",
       "        [[ 1,  5,  9],\n",
       "         [13, 17, 21]],\n",
       "\n",
       "        [[ 2,  6, 10],\n",
       "         [14, 18, 22]],\n",
       "\n",
       "        [[ 3,  7, 11],\n",
       "         [15, 19, 23]]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z #x,y,z를 잘 파악해보자. 데이터가 미세하게 다르다는 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfe8a88",
   "metadata": {},
   "source": [
    "# tensor 연산 및 주요 함수(곱하기, 나누기 등을 할 수 있다.)\n",
    "\n",
    "## element-wise 연산\n",
    "- tensor와 상수 연산시, tensor와 tensor간 연산시 원소별로 처리한다.\n",
    "- 행렬곱 연산을 제외하고 tensor간 연산시 피연산지 tensor간에 shape이 같아야 한다.\n",
    "    - shape이 다를 경우 조건이 맞으면 broadcasting을 한 뒤에 연산한다. (size가 다른 축의 경우 한개의 피연산자 size가 1일 경우 복사하여 shape을 맞춘다.)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "85d3d7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4],\n",
      "        [5, 6, 7, 8, 9]])\n",
      "tensor([[10, 11, 12, 13, 14],\n",
      "        [15, 16, 17, 18, 19]])\n",
      "tensor([50, 51, 52, 53, 54])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.arange(10).reshape(2,5) \n",
    "b = torch.arange(10,20).reshape(2,5)\n",
    "c = torch.arange(50, 55)\n",
    "\n",
    "\n",
    "#그냥 뭐.... 쭉 잘 봐라잉.\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "24f4f435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[100, 101, 102, 103, 104],\n",
      "        [105, 106, 107, 108, 109]])\n"
     ]
    }
   ],
   "source": [
    "print(a + 100) #모든 원소에 100을 더한다.\n",
    "# print(a - 100)\n",
    "# print(a < 5) #이거는 boolean이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1a5ed369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10, 12, 14, 16, 18],\n",
      "        [20, 22, 24, 26, 28]])\n"
     ]
    }
   ],
   "source": [
    "print(a + b)\n",
    "# print(a == b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "15256a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50, 52, 54, 56, 58],\n",
      "        [55, 57, 59, 61, 63]])\n"
     ]
    }
   ],
   "source": [
    "# broadcasting\n",
    "#데이터가 부족한 쪽에서, 데이터를 복제해서 dummy축을 만든다.\n",
    "print(a + c) #결과를 보면, c측에서 한 줄을 또 만들어서 결과를 출력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729874b2",
   "metadata": {},
   "source": [
    "## 주요 연산함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6fc6ac19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4, -3, -2],\n",
      "        [-1,  0,  1],\n",
      "        [ 2,  3,  4]])\n",
      "tensor([[4, 3, 2],\n",
      "        [1, 0, 1],\n",
      "        [2, 3, 4]])\n",
      "tensor([[2.0000, 1.7321, 1.4142],\n",
      "        [1.0000, 0.0000, 1.0000],\n",
      "        [1.4142, 1.7321, 2.0000]])\n",
      "tensor([[1.3863, 1.0986, 0.6931],\n",
      "        [0.0000,   -inf, 0.0000],\n",
      "        [0.6931, 1.0986, 1.3863]])\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "x=torch.arange(-4, 5).reshape(3,3)\n",
    "print(x)\n",
    "print(torch.abs(x)) #절대값\n",
    "print(torch.sqrt(torch.abs(x))) #제곱근      #e**4 와 같이 하면 4제곱이 된다.\n",
    "# print(torch.exp(x))  # torch.e**x\n",
    "print(torch.log(torch.abs(x))) #밑이 e인 로그를 계산한다. 자연로그를 계산하는거지.\n",
    "# print(torch.log(torch.exp(torch.tensor(1)))) # torch.log() 밑이 e인 로그계산\n",
    "print(torch.log10(torch.tensor(10)))         # torch.log10() 밑이 10인 로그계산(계산 결과는 당연히 1이 나온다.)\n",
    "print(torch.log2(torch.tensor(2)))           # torch.log2() 밑이 2인 로그계산(이거두!!)\n",
    "# print(\"=====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "026192bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.2288, -1.7263, -2.4652],\n",
      "        [-0.9719,  0.6420,  1.7219],\n",
      "        [ 1.4282,  3.5005,  3.5567]])\n",
      "tensor([[-3., -2., -2.],\n",
      "        [-1.,  1.,  2.],\n",
      "        [ 1.,  4.,  4.]])\n",
      "tensor([[-3.2300, -1.7300, -2.4700],\n",
      "        [-0.9700,  0.6400,  1.7200],\n",
      "        [ 1.4300,  3.5000,  3.5600]])\n",
      "tensor([[-4., -2., -3.],\n",
      "        [-1.,  0.,  1.],\n",
      "        [ 1.,  3.,  3.]])\n",
      "tensor([[-3., -1., -2.],\n",
      "        [-0.,  1.,  2.],\n",
      "        [ 2.,  4.,  4.]])\n"
     ]
    }
   ],
   "source": [
    "y = x + torch.randn((3,3))\n",
    "print(y)\n",
    "print(torch.round(y)) # 반올림 (하면 정수로 바뀐다.)\n",
    "print(torch.round(y, decimals=2)) # 소수점 둘째자리까지. 그 이하에서 반올림!\n",
    "print(torch.floor(y)) # 내림\n",
    "print(torch.ceil(y)) # 올림\n",
    "\n",
    "\n",
    "#이런 함수들을 가지고, 숫자가 너무 길게 나올 때, 숫자를 적절하게 cut하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358832d4",
   "metadata": {},
   "source": [
    "### 행렬곱\n",
    "- `@` 연산자 또는 `torch.matmul(tensor1, tensor2)` 함수 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ac6cfff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "eb349ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2]), torch.Size([2, 2]))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tip: 다른 설명이 없다면, tensor는 floattensor이다.\n",
    "\n",
    "x = torch.FloatTensor([[1, 2],\n",
    "                       [3, 4],\n",
    "                       [5, 6]\n",
    "                      ])\n",
    "\n",
    "y = torch.FloatTensor([[1, 2],\n",
    "                       [1, 2],\n",
    "                      ])\n",
    "x.size(), y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4283adae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2]) torch.Size([3, 2])\n",
      "tensor([[ 3.,  6.],\n",
      "        [ 7., 14.],\n",
      "        [11., 22.]])\n",
      "tensor([[ 3.,  6.],\n",
      "        [ 7., 14.],\n",
      "        [11., 22.]])\n"
     ]
    }
   ],
   "source": [
    "z1 = x @ y\n",
    "z2 = torch.matmul(x, y)\n",
    "print(z1.shape, z2.shape)\n",
    "print(z1)\n",
    "print(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "004d5f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 5])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch 행렬곱(Batch matrix muliplication) - bmm()\n",
    "# x, y가 가지는 3개의 2차원 배열 간에 행렬곱을 처리한다.\n",
    "import torch\n",
    "x = torch.FloatTensor(3,4,2)\n",
    "y = torch.FloatTensor(3,2,5)\n",
    "z = torch.bmm(x, y)\n",
    "z.shape\n",
    "\n",
    "\n",
    "#이 부분은 심화적인 부분에서 많이 쓰일 것 같으므로 일단 알아만 두자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c85a8b6",
   "metadata": {},
   "source": [
    "## torch.nan, torch.inf\n",
    "- nan: Not a Number, 주로 결측치를 표현한다.\n",
    "- inf: infinit 무한. \n",
    "    - torch.inf: 양의 무한\n",
    "    - -torch.inf: 음의 무한\n",
    "- torch.isnan(tensor)\n",
    "    - 원소별 결측치 확인\n",
    "- torch.isinf(tensor)    \n",
    "    - 원소별 inf 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7a051744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n",
      "True False\n"
     ]
    }
   ],
   "source": [
    "print(torch.inf > 10, torch.inf < 10)\n",
    "print(-torch.inf < 10, -torch.inf > 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d722e6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(nan)\n"
     ]
    }
   ],
   "source": [
    "print(torch.log(torch.tensor(-1))) # nan \n",
    "# print(torch.isnan(torch.tensor([1,2,torch.nan,3,4])))  # nan 여부 확인\n",
    "# print(torch.isinf(torch.tensor([1,2,3,4,torch.inf])))  # inf 여부 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2214bd73",
   "metadata": {},
   "source": [
    "## 기술통계함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "11a0d448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6004,  1.1224, -0.4028,  0.6106],\n",
      "        [-0.7580, -0.9758, -1.4271, -1.1446],\n",
      "        [-1.0939, -0.4980,  2.3813,  0.0757]])\n"
     ]
    }
   ],
   "source": [
    "X=torch.randn(3,4)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7b696371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-3.7104)\n",
      "tensor([-0.2701, -4.3055,  0.8652])\n",
      "tensor([[-0.2701],\n",
      "        [-4.3055],\n",
      "        [ 0.8652]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(X))\n",
    "print(torch.sum(X, dim=1)) #지정한 dim이나 axis의 index가 다른 값끼리 계선한다.\n",
    "print(torch.sum(X, dim=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "99c06f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.3092)\n",
      "tensor([-0.0675, -1.0764,  0.2163])\n",
      "tensor([[-0.0675],\n",
      "        [-1.0764],\n",
      "        [ 0.2163]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.mean(X)) #이건 평균\n",
    "print(torch.mean(X, dim=1))\n",
    "print(torch.mean(X, dim=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "db8d8f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1756)\n",
      "tensor(1.3821)\n",
      "tensor([0.1798, 1.2094, 3.8841, 0.8093])\n",
      "tensor([[0.1798, 1.2094, 3.8841, 0.8093]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.std(X)) # standard deviation 표준 편차\n",
    "print(torch.var(X)) # variance :분산\n",
    "print(torch.var(X, dim=0))\n",
    "print (torch.var(X,dim=0,keepdims=True))#차원을 유지해주면서 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "51ecc064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2701],\n",
      "        [-4.3055],\n",
      "        [ 0.8652]])\n",
      "tensor([[-0.0675],\n",
      "        [-1.0764],\n",
      "        [ 0.2163]])\n",
      "tensor(1.1756)\n"
     ]
    }
   ],
   "source": [
    "print(X.sum(dim=1, keepdims=True))\n",
    "print(X.mean(dim=1, keepdims=True))\n",
    "print(X.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8316012a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6004,  1.1224, -0.4028,  0.6106],\n",
       "        [-0.7580, -0.9758, -1.4271, -1.1446],\n",
       "        [-1.0939, -0.4980,  2.3813,  0.0757]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X #각각의 값을 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "afe123ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3813)\n",
      "torch.return_types.max(\n",
      "values=tensor([-0.7580,  1.1224,  2.3813,  0.6106]),\n",
      "indices=tensor([1, 0, 2, 0]))\n"
     ]
    }
   ],
   "source": [
    "print(torch.max(X))\n",
    "print(torch.max(X, dim=0))  # return_types.max 타입객체로 반환. max값과 max값의 index를 묶어서 반환\n",
    "#각 분야에서 MAX인 것을 출력한다.\n",
    "# print(torch.max(X, dim=1))\n",
    "# print(torch.max(X, dim=1).values, torch.max(X, dim=1).indices, sep=\" || \") #indices는 '지수'라는 의미이다.\n",
    "# print(torch.max(X, dim=0, keepdims=True))  #keepdims=True : 차원(rank)를 유지 \n",
    "# # print(torch.max(X, dim=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "234df8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.6004)\n",
      "torch.return_types.min(\n",
      "values=tensor([-1.6004, -0.9758, -1.4271, -1.1446]),\n",
      "indices=tensor([0, 1, 1, 1]))\n",
      "torch.return_types.min(\n",
      "values=tensor([-1.6004, -1.4271, -1.0939]),\n",
      "indices=tensor([0, 2, 0]))\n",
      "tensor(10)\n",
      "tensor([1, 0, 2, 0])\n",
      "tensor([1, 0, 2])\n"
     ]
    }
   ],
   "source": [
    "print(torch.min(X))\n",
    "print(torch.min(X, dim=0))\n",
    "print(torch.min(X, dim=1))\n",
    "\n",
    "print(torch.argmax(X))\n",
    "print(torch.argmax(X, dim=0)) # 각 열에서 가장 큰 애가 존재하는 인덱스\n",
    "print(torch.argmax(X, dim=1)) # 각 행에서 가장 큰 애가 존재하는 인덱스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02bf59a",
   "metadata": {},
   "source": [
    "# autograd(자동미분)\n",
    "- 자동 미분을 이용해 gradient를 계산하는 pytorch system.\n",
    "- 딥러닝 모델에서 weight와 bias tensor들(Parameter)은 backpropagation(역전파)를 이용해 gradient를 구해서 loss가 줄어드는 방향으로 update를 하게된다.\n",
    "- pytorch는 이런 미분 수행을 자동으로 처리해 준다.\n",
    "    - gradient(기울기)를 구한다는 것은 미분을 한다는 것을 말한다.\n",
    "- tensor가 미분 가능하려면 `requires_grad=True` 로 설정되 있어야 한다. (default: False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8baff6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], requires_grad=True)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.], requires_grad=True) #requires_grad가 true로 설정이 되어야 한다!\n",
    "# x = torch.tensor([1.])\n",
    "# x.requires_grad = True\n",
    "print(x) #출력창에 requires_grad=True 가 안뜨면 false라는 의미이다.\n",
    "print(x.requires_grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e7cb2702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x ** 2\n",
    "print(y)\n",
    "# 계산 결과를 담은 tensor인 y는 계산 결과와 grad_fn에 어떤 계산을 했는지 정보를 담고 있다. (PowBackward0)\n",
    "# 이는 y 계산에 사용된 x가 requires_grad=True이기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1b68cf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미분 - tensor.backward() 호출\n",
    "y.backward()   # dy/dx  gradient 계산 후 결과를 x의 grad attribute에 저장한다.\n",
    "#y가 적혀 있지만, 결국에는 x값을 미분하는 것이다. 잘 모르겠다면, 수학시간에 배웠던 것들을 생각해 보자.\n",
    "#결과를 x에 저장한다는 것이 매우 신기하다.\n",
    "#backward는 역전파, forward는 순전파를 의미한다. \n",
    "#역전파는 미분을 의미한다.\n",
    "\n",
    "#미분이 더 안되는데 하면 에러가 당연히 난다. 주의!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e7977be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad\n",
    "# 도함수: y` = 2x 이고 x가 1이었으므로 grad는 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4eb57a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-120-95faf70d8eed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#윗줄까지는 프린트가 잘 되는데, 도함수가 없으니 계산을 할 수 없어 에러가 난다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             )\n\u001b[1;32m--> 492\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# requires_grad가 True로 설정되어 있지 않은 경우\n",
    "x = torch.tensor([1.])\n",
    "y = x ** 2\n",
    "print(x)\n",
    "y.backward() #윗줄까지는 프린트가 잘 되는데, 도함수가 없으니 계산을 할 수 없어 에러가 난다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "94cc4fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<PowBackward0>) True\n",
      "tensor([10.], grad_fn=<MulBackward0>)\n",
      "tensor([20.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.], requires_grad=True)\n",
    "y = x ** 2   # 미분: 2x\n",
    "z = y * 10   # 미분: 10  ===> 2x * 10\n",
    "#근데 이 구조를 잘 보면, x가 바뀌어야 다른 값들이 바뀐다.\n",
    "\n",
    "#y값에 따라서 z의 결과가 바뀐다.\n",
    "print(y,y.requires_grad)\n",
    "z.backward()#z와 y의 미분, y와 x의 미분을 서로 계산하면, z와 x의 미분을 알 수 있다. 결과적으로, 이는 x에 대한 계산이다.\n",
    "print(z)\n",
    "print(x.grad) #미분이 잘 된 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6ad085db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.], grad_fn=<AddBackward0>)\n",
      "tensor([4.])\n",
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "## 편미분(간단히 말해서, x와 y가 따로 있는데, 1개의 변수만 미분하는 것이다.)\n",
    "#x에 대해 미분을 한다 치면, y는 그냥 상수로 봐버리는 것이다.\n",
    "x=torch.tensor([1.],requires_grad=True)\n",
    "y=torch.tensor([1.],requires_grad=True)\n",
    "z= 2*x**2 + y**2\n",
    "print(z)\n",
    "z.backward() #이것도 결국 x가 바뀌는 것에 대해 값을 구하는 것이다.\n",
    "print(x.grad)  #dz/dx ===>y를 상수로 처리를 해서, 값이 4가 나온다.\n",
    "print(y.grad)  #dz/dy ====>x를 상수로 처리를 해서, 값이 2가 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f565af37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14., grad_fn=<SumBackward0>)\n",
      "tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "#함수가 다르더라도 과정은 대강 같다.\n",
    "x=torch.tensor([1., 2., 3.] ,requires_grad=True)\n",
    "y=torch.sum(x**2) # [x1**2 + x2**2 + x3**2]  \n",
    "y.backward() #[dy/dx1, dy/dx2, dy/dx3] = [2x1, 2x2, 2x3]\n",
    "\n",
    "print(y) \n",
    "print(x.grad) # 스칼라를 벡터로 미분"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a304c280",
   "metadata": {},
   "source": [
    "## torch.no_grad() \n",
    "- no_grad() 구문에서 연산을 할 경우 requires_grad=True로 설정되었다 하더라도 gradient를 update하지 않는다.\n",
    "- gradient를 계산하지 말라는 의미. 즉, 미분을 하지 말라는 의미이다.\n",
    "- 딥러닝 모델 학습이 끝나고 평가할 때는 gradient를 계산할 필요가 없기 때문에 no_grad 구문을 사용한다.(중요!!)\n",
    "- 그리고, 이걸 한다고 해서, 다른 결과가 나오는 것은 아니니, 이를 문제없이 써도 된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d3b29ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad = True) #requires_grad를 True로 해야만 미분 계산이 가능하다.\n",
    "print(x.requires_grad)\n",
    "\n",
    "y = x**2\n",
    "\n",
    "print(y.requires_grad)  # 연산이 결과 requires_grad도 True -> 그래야 미분이 가능하므로. x가 true이면 y도 true이다.\n",
    "print(x.requires_grad)\n",
    "print(y)\n",
    "\n",
    "y.backward() #뭔가 아직은 이 코드가 익숙하지는 않은데, 그래도 익숙해져라.\n",
    "print(x.grad) #연산 결과 (2)가 나온다. 이는 앞에서 했던 것이다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8616db37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "tensor(1.)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad = True)\n",
    "print(x.requires_grad)\n",
    "\n",
    "with torch.no_grad(): #다른 것은 다 동일한데, with 구문을 통해 no_grad를 적용시킨다. \n",
    "    #no_grad 안에서 y를 적용을 시키면, requires_grad가 false가 나온다.\n",
    "    print(x.requires_grad)\n",
    "    y = x**2 #torch.no_grad()로 인해서 도함수를 만들지 않는다.\n",
    "    print(y.requires_grad)# 연산이 적용될 때 requires_grad가 False가 된다.\n",
    "    print(y)\n",
    "    \n",
    "print(x.requires_grad)\n",
    "# y.backward() #no_grad를 적용한 상태로 y.backward를 적용시키려 하면 에러(exception)가 난다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3811c4b",
   "metadata": {},
   "source": [
    "## gradient 값 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dff76eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x의 gradient값: tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "y = x**2\n",
    "y.backward()\n",
    "print(\"x의 gradient값:\", x.grad ) #y.backward가 끝이 나면, x.grad의 값이 2로 update된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af26a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8170c9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x의 gradient값: tensor(8.)\n"
     ]
    }
   ],
   "source": [
    "z = x **2\n",
    "z.backward()\n",
    "print(\"x의 gradient값:\", x.grad )\n",
    "#얼레? 틀린 값이 나오네? 이는 grad값이 이미 위의 연산에 있었기 때문이다. 계산을 계속하면 값이 계속 2씩 증가할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b607fca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "y = x**2\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "332ec8eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradient초기화 grad를 tensor 재정의로 다시 정의한다. (중요한 내용-----잘 숙지할것!)\n",
    "x.grad = torch.tensor(0.)   \n",
    "z = x**2\n",
    "z.backward()\n",
    "x.grad\n",
    "#앞에서 초기화를 했기 때문에 grad의 값이 계속 2가 나온다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba648ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
