{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LinearRegression from Scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 구현할 것\n",
    "- 공부시간과 성적간의 관계를 모델링한다.\n",
    "    - **머신러닝 모델(모형)이란** 수집한 데이터를 기반으로 입력값(Feature)와 출력값(Target)간의 관계를 하나의 공식으로 정의한 함수이다. 그 공식을 찾는 과정을 **모델링**이라고 한다.\n",
    "    - 이 예제에서는 공부한 시간으로 점수를 예측하는 모델을 정의한다.\n",
    "    - 입력값과 출력값 간의 관계를 정의할 수있는 다양한 함수(공식)이 있다. 여기에서는 딥러닝과 관계가 있는 **Linear Regression** 을 사용해본다.\n",
    "\n",
    "# 데이터 확인\n",
    "- 입력데이터: 공부시간\n",
    "- 출력데이터: 성적\n",
    "\n",
    "|공부시간|점수|\n",
    "|-|-|\n",
    "|1|20|\n",
    "|2|40|\n",
    "|3|60|\n",
    "\n",
    "우리가 수집한 공부시간과 점수 데이터를 바탕으로 둘 간의 관계를 식으로 정의 할 수 있으면 **내가 몇시간 공부하면 점수를 얼마 받을 수 있는지 예측할 수 있게 된다.**   \n",
    "수집한 데이터를 기반으로 앞으로 예측할 수있는 모형을 만드는 것이 머신러닝 모델링이다. 예측을 하기 위해서는 데이터를 '쏟아 붓는다'\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습(훈련) 데이터셋 만들기\n",
    "- 모델을 학습시키기 위한 데이터셋을 구성한다.\n",
    "- 입력데이터와 출력데이터을 따로 행렬로 구성한다.\n",
    "- 같은 데이터 포인트의 입력, 출력 데이터를 같은 index에 정의한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델링\n",
    "\n",
    "## 모델 정의\n",
    "\n",
    "- Feature와 Target간의 관계를 수식으로 정의한다.\n",
    "- 여기서는 공부시간(Feature)와 점수(Target)간의 관계를 정의하는데 **선형회귀(Linear Regression) 모델** 을 가설로 세우고 모델링을 한다.\n",
    "    - 많은 머신러닝 연구자들이 다양한 종류의 데이터에 관계를 예측할 수 있는 여러 알고리즘을 연구했다.\n",
    "    - 선형회귀 모델은 입력데이터와 출력데이터가 선형관계(비례 또는 반비례 관계)일때 좋은 성능을 나타낸다.\n",
    "  \n",
    "> ### 가설\n",
    "> - 아직은 이 식이 맞는지 틀린지는 알 수없기 때문에 **이 식을 가설(hypothesis) 라고 한다.**\n",
    "> - 가설을 세우고 모델링을 한 뒤 검증을 해서 좋은 예측결과를 내면 그 가설을 최종 결과 모델로 결정한다. 예측결과가 좋지 않을 경우 새로운 가설로 모델링을 한다.\n",
    " \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 선형회귀 (Linear Regression)\n",
    "- Feature들의 가중합을 이용해 Target을 추정한다.\n",
    "- Feature에 곱해지는 가중치(weight)들은 각 Feature가 Target 얼마나 영향을 주는지 영향도가 된다.\n",
    "    - 음수일 경우는 target값을 줄이고 양수일 경우는 target값을 늘린다.\n",
    "    - 가중치가 0에 가까울 수록 target에 영향을 주지 않는 feature이고 0에서 멀수록 target에 많은 영향을 주는 target이 된다.\n",
    "- 모델 학습과정에서 가장 적절한 Feature의 가중치를 찾아야 한다.\n",
    "- 한 마디로, 랜덤한 값을 이용해서 방정식을 잘 찾아내는 것이다.\n",
    "- 관련 도서를 찾는 것도 도움이 된다!\n",
    " \n",
    "$$\n",
    "\\hat{y} = Wx + b\n",
    "$$\n",
    "<center>$\\hat{y}$: 모델추정값<br>W: 가중치<br>x: Feature<br>b: bias(편향)</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [경사하강법을 이용한 최적화](03_2_gradient_decending.ipynb)\n",
    "\n",
    "- 반드시 반드시 반드시 반드시 복습하기!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이토치로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #이거 빼먹지 말자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#훈련(train) 데이터 정의\n",
    "#입력데이터 변수명: X, 출력데이터 변수명:Y\n",
    "X_train = torch.tensor([\n",
    "    [1],[2],[3]\n",
    "],dtype=torch.float32)\n",
    "\n",
    "\n",
    "y_train  = torch.tensor([[20],[40],[60]],dtype=torch.float32)\n",
    "\n",
    "\n",
    "#순서대로 1시간 공부-점수(20)/2시간 공부-점수(40)....뭐 이렇게 구성되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1]) torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,y_train.shape)\n",
    "\n",
    "#[3,1] => [데이터개수,개별데이터의 shape] => 3개의 데이터. 각 데이터는 1개의 값을 구성된 1차원배열\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기 파라미터\n",
      "weight: tensor([[1.5410]], requires_grad=True)\n",
      "bias: tensor([-0.2934], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#함수를 하나 만들 것이다.\n",
    "#이 함수는 weight와 bias를 정의하는 것이다.\n",
    "\n",
    "\n",
    "#그 전에,seed값을 잘 적용시켜서 랜덤값을 동일하게 고정시키자.\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "#각 변수들을 정의하자.(randn을 통해 정규분포를 만족하는 놈을 출력한다.)\n",
    "weight = torch.randn(1,1,requires_grad=True) #(1:입력 feature 개수,1:출력 값의 개수)\n",
    "#requires_grad를 통해 값을 변동시킬 것을 암시한다.\n",
    "\n",
    "bias = torch.randn(1,requires_grad=True)\n",
    "#bias도 바뀌어야 하는 대상이므로 requires_grad을 True라고 한다.\n",
    "\n",
    "\n",
    "print(\"초기 파라미터\")\n",
    "print(\"weight:\", weight)\n",
    "print(\"bias:\",bias)\n",
    "\n",
    "def linear_model(X):\n",
    "    pred = X@weight+bias #계산을 진행한다. weight와 bias는 새로 만든 것이다.\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2476],\n",
       "        [2.7886],\n",
       "        [4.3296]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 예측\n",
    "\n",
    "pred_train = linear_model(X_train) #공부시간에 따른 예측점수라고 볼 수 있다.\n",
    "pred_train  #공부시간에 따른 예측 점수를 알 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#어? 근데 실제 점수(y_train)는 예측 결과와 오차가 꽤 많이 난다.\n",
    "#그래서! 오차함수라는 것을 정의한다.\n",
    "\n",
    "\n",
    "#오차함수: 모델이 추론한 값과 정답사이의 차이를 계산한다.\n",
    "#평균 제곱 오차(mean squared error:MSE) -----앞자리를 따서 MSE라고 한다.\n",
    "\n",
    "def mse_lose_fn(pred:\"예측값\",y:\"정답\"):\n",
    "    return torch.mean((pred-y)**2) #평균 제곱 오차이니깐....!\n",
    "    #torch.mean을 통해 평균 제곱 오차를 계산한다.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오차: tensor(1611.8477, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = mse_lose_fn(pred_train,y_train)\n",
    "print('오차:',loss)\n",
    "#오차를 적나라하게 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#미분계수를 계산한다.\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight의 grad:: tensor([[-173.4578]])\n",
      "bias의 grad: tensor([-74.4229])\n"
     ]
    }
   ],
   "source": [
    "print(\"weight의 grad::\",weight.grad)\n",
    "print(\"bias의 grad:\",bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update전 weight: tensor([[1.8879]], requires_grad=True)\n",
      "update후 weight: tensor([[2.0614]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "###최적화 -> 경사하강법 ====>파이토치의 함수를 사용한다.\n",
    "#torch.optim.SGD라는 함수의 구조를 잘 파악하자.\n",
    "optimizer = torch.optim.SGD([weight,bias],\n",
    "                           lr=0.001 #학습률\n",
    "                           ) #최적화할 대상 => requires_grad가 True여야 한다.\n",
    "\n",
    "#경사하강법 계산 연산을 처리한다. weight와 bias가 바뀌는 것을 잘 파악하자.\n",
    "print(\"update전 weight:\",weight)\n",
    "optimizer.step()\n",
    "print(\"update후 weight:\",weight)\n",
    "\n",
    "#계속 셀을 실행을 하면, tensor안의 값이 바뀌는 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이전: tensor(1611.8477, grad_fn=<MeanBackward0>)\n",
      "업데이트 후: tensor(4.9641, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "###업데이트 후 로스 계산\n",
    "\n",
    "#추론\n",
    "pred = linear_model(X_train)\n",
    "\n",
    "#오차 계산\n",
    "loss2 = mse_lose_fn(pred,X_train)\n",
    "\n",
    "\n",
    "print(\"이전:\",loss)\n",
    "print(\"업데이트 후:\",loss2)\n",
    "#오차가 매우 줄어들었음을 알 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습\n",
    "1. 모델을 이용해 추정한다.\n",
    "   - pred = model(input)\n",
    "1. loss를 계산한다.\n",
    "   - loss = loss_fn(pred, target)\n",
    "1. 계산된 loss를 파라미터에 대해 미분하여 계산한 gradient 값을 각 파라미터에 저장한다.\n",
    "   - loss.backward()\n",
    "1. optimizer를 이용해 파라미터를 update한다. (반복문 안에서의 로직을 처리해주는 것이 바로 optimizer이다.)\n",
    "   - optimizer.step()  \n",
    "1. 파라미터의 gradient(미분값)을 0으로 초기화한다.\n",
    "   - optimizer.zero_grad()\n",
    "- 위의 단계를 반복한다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#과정이 어렵다. 복습을 반드시 하도록 하자!!\n",
    "#몇 번 돌려야 좋은지 모르기 때문에, 많든 적든 시행착오를 겪어봐야 한다.\n",
    "\n",
    "STEPS = 100 #파라미터(WEIGHT,BIAS)를 업데이트할 횟수.\n",
    "for _ in range(STEPS):\n",
    "    #1.추론\n",
    "    pred = linear_model(X_train)\n",
    "    # 2.오차 계산\n",
    "    loss = mse_lose_fn(pred,y_train)\n",
    "    \n",
    "    #3.weight와 bias에 대한 gredient를 계산한다.\n",
    "    loss.backward()\n",
    "    \n",
    "    #4.최적화: optimizer를 이용한다. 알고리즘은 경사하강법을 쓴다. \n",
    "    optimizer.step()\n",
    "    \n",
    "    #5.계산된 gradient 값을 초기화한다. \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "\n",
    "#함수 자체는 금방 끝난다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight tensor([[12.3091]], requires_grad=True)\n",
      "bias: tensor([4.2502], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "###weight, bias 값\n",
    "#값들이 많이 업데이트 된 것을 알 수 있다.\n",
    "print(\"weight\",weight)\n",
    "print(\"bias:\",bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(163.3457, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#오차 계산\n",
    "pred3 = linear_model(X_train)\n",
    "loss3 = mse_lose_fn(pred3,y_train)\n",
    "\n",
    "print(loss3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.780676820888633"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "163.3457**(1/2) #오차가 13 정도로 많이 줄어들었음을 알 수 있다.\n",
    "#위의 과정들을 계속 하면 결과적인 오차 값이 계속해서 작아짐을 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다중 입력, 다중 출력\n",
    "- 다중입력: Feature가 여러개인 경우\n",
    "- 다중출력: Output 결과가 여러개인 경우\n",
    "\n",
    "다음 가상 데이터를 이용해 사과와 오렌지 수확량을 예측하는 선형회귀 모델을 정의한다.  \n",
    "[참조](https://www.kaggle.com/code/aakashns/pytorch-basics-linear-regression-from-scratch)\n",
    "\n",
    "\n",
    "|온도(F)|강수량(mm)|습도(%)|사과생산량(ton)|오렌지생산량|\n",
    "|-|-|-|-:|-:|\n",
    "|73|67|43|56|70|\n",
    "|91|88|64|81|101|\n",
    "|87|134|58|119|133|\n",
    "|102|43|37|22|37|\n",
    "|69|96|70|103|119|\n",
    "\n",
    "```\n",
    "사과수확량  = w11 * 온도 + w12 * 강수량 + w13 * 습도 + b1\n",
    "오렌지수확량 = w21 * 온도 + w22 * 강수량 + w23 *습도 + b2\n",
    "```\n",
    "\n",
    "- `온도`, `강수량`, `습도` 값이 사과와, 오렌지 수확량에 어느정도 영향을 주는지 가중치를 찾는다.\n",
    "    - 모델은 사과의 수확량, 오렌지의 수확량 **두개의 예측결과를 출력**해야 한다.\n",
    "    - 사과에 대해 예측하기 위한 weight 3개와 오렌지에 대해 예측하기 위한 weight 3개 이렇게 두 묶음, 총 6개의 weight를 정의하고 학습을 통해 가장 적당한 값을 찾는다.\n",
    "        - 이 묶음을 딥러닝에서는 **Node, Unit, Neuron** 이라고 한다.\n",
    "- 목적은 우리가 수집한 train 데이터셋을 이용해 **정확한 예측을 위한 weight와 bias**를 찾는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#한 마디로, 여러 개의 요소를 잘 파악해서 생산량을 예측하는 것이다.\n",
    "#보통, 우리가 추론하는 값은 1개인 경우가 많은데, 이 경우에는 2개이다. 그래도 뭐 큰 문제는 없다.\n",
    "#각 요소의 weight의 절대값이 크고 작음에 따라 수확량에 영향을 준다. 부정적인 영향이든 좋은 영향이든 상관은 없다.\n",
    "#변수가 3개이므로 가중치(w)도 3개가 필요하다. 근데 오렌지 뿐만 아니라, 사과도 있잖아? 그래서 결국은 3*2이다.\n",
    "\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "- Train data는 feature와 target를 각각 따로 2개의 행렬로 구성한다.\n",
    "- Feature의 행은 관측치(개별 데이터)를 열을 Feature(특성, 변수)를 표현한다.\n",
    "- Target은 모델이 예측할 대상으로 행은 개별 관측치, 열은 각 항목에 대한 정답으로 이 예제에서는 사과수확량과 오렌지 수확량 값을 가진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T22:30:09.686447Z",
     "iopub.status.busy": "2023-09-04T22:30:09.686279Z",
     "iopub.status.idle": "2023-09-04T22:30:09.695315Z",
     "shell.execute_reply": "2023-09-04T22:30:09.694963Z",
     "shell.execute_reply.started": "2023-09-04T22:30:09.686438Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# Input (temp, rainfall, humidity) : (5, 3)\n",
    "#위의 그래프의 데이터들과 비교해 보는 것이 좋다. 그러면 이해가 빨라.\n",
    "inputs = torch.tensor([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T22:30:17.630127Z",
     "iopub.status.busy": "2023-09-04T22:30:17.629872Z",
     "iopub.status.idle": "2023-09-04T22:30:17.632166Z",
     "shell.execute_reply": "2023-09-04T22:30:17.631853Z",
     "shell.execute_reply.started": "2023-09-04T22:30:17.630117Z"
    }
   },
   "outputs": [],
   "source": [
    "# Targets: 생산량 - (apples, oranges) - (5, 2)\n",
    "targets = torch.tensor([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Model (from scratch)\n",
    "\n",
    "### weight와 bias\n",
    "- weight 는 각 feature에 곱해지는 가중치로 target 값에 얼마나 영향을 주는지를 나타낸다. 직선의 방정식에서 기울기이다.\n",
    "- bias는 각 feature와 weight간의 가중합에 더해주는 값으로 모든 feature가 0일 경우 target의 값을 나타낸다. 직선의 방정식에서 절편이다.\n",
    "- weight와 bias는 각각 random 값을 초기값으로 가지는 matrix로 정의한다.\n",
    "- weight의 shape: (2, 3)\n",
    "- bias의 shape: (2, ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "### weight와 bias 생성\n",
    "\n",
    "weights = torch.randn(3,2,requires_grad=True) #학습을 통해 찾아야 하는 것이기 때문에 true이다.\n",
    "##[3,2]===>3은 input feature이고 2는 output feature의 개수이다. ===>[[온도weight,강수weight,습도weight],[사과,오렌지]]\n",
    "\n",
    "\n",
    "bias = torch.randn(2,requires_grad=True) #이것도 requires_grad를 True로 한다.\n",
    "#[2] =[output개수=> 사과,오렌지]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2]) torch.Size([2])\n",
      "tensor([[ 1.5410, -0.2934],\n",
      "        [-2.1788,  0.5684],\n",
      "        [-1.0845, -1.3986]], requires_grad=True)\n",
      "tensor([0.4033, 0.8380], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(weights.shape,bias.shape)\n",
    "print(weights)\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression model\n",
    "모델은 weights `w`와 inputs `x`의 내적(dot product)한 값에 bias `b`를 더하는 간단한 함수이다. \n",
    "\n",
    "$$\n",
    "\\hspace{2.5cm} X \\hspace{1.1cm} \\cdot \\hspace{1.2cm} W^T \\hspace{1.2cm}  + \\hspace{1cm} b \\hspace{2cm}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\left[ \\begin{array}{cc}\n",
    "73 & 67 & 43 \\\\\n",
    "91 & 88 & 64 \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "69 & 96 & 70\n",
    "\\end{array} \\right]\n",
    "%\n",
    "\\cdot\n",
    "%\n",
    "\\left[ \\begin{array}{cc}\n",
    "w_{11} & w_{21} \\\\\n",
    "w_{12} & w_{22} \\\\\n",
    "w_{13} & w_{23}\n",
    "\\end{array} \\right]\n",
    "%\n",
    "+\n",
    "%\n",
    "\\left[ \\begin{array}{cc}\n",
    "b_{1} & b_{2} \\\\\n",
    "b_{1} & b_{2} \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "b_{1} & b_{2} \\\\\n",
    "\\end{array} \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#위의 linear regression model이 계산을 간단하게 해 주는 식이다. \n",
    "\n",
    "\n",
    "def model(X):\n",
    "    return X @ weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -79.7173,  -42.6370],\n",
      "        [-120.5089,  -65.3522],\n",
      "        [-220.3900,  -29.6390],\n",
      "        [  23.7697,  -56.3972],\n",
      "        [-178.3483,  -62.7408]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#위의 식을 직접 계산해 준다.\n",
    "pred = model(inputs)\n",
    "print(pred)\n",
    "#size는 5,2이고, 사과와 오렌지에 대한 5개 쌍이 나온다.\n",
    "#딱 봐도 값이 틀렸다 ㅋㅋ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "모델이 예측한 값과 정답간의 차이를 비교하는 메소드. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss_fn(preds,targets):\n",
    "    squared_error = (preds-targets)**2\n",
    "    return torch.mean(squared_error) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(36193.4961, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mse_loss_fn(pred,targets)#사과생산량 오차와 오렌지 생산량 오차를 합친 값이 나온다.\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients 계산\n",
    "loss에 대한 weight와 bias의 gradients (미분계수)를 계산한다. **Pytorch의 자동미분**을 이용한다. (**graident를 구하려는 tensor는 requires_grad=True로 설정한다.**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#그냥 간단하게 loss.backward를 해 주면 된다.\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5410, -0.2934],\n",
      "        [-2.1788,  0.5684],\n",
      "        [-1.0845, -1.3986]], requires_grad=True)\n",
      "tensor([[-15400.8262, -11915.3555],\n",
      "        [-19847.4922, -13088.5000],\n",
      "        [-11609.1875,  -8220.1094]])\n"
     ]
    }
   ],
   "source": [
    "print(weights)\n",
    "print(weights.grad) #각각에 대한 grad 값이 구해졌다. 이는 loss.backward에 따라 구해졌다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.9418, grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_weight_0_0 = weights[0,0] - weights.grad[0,0]*0.001\n",
    "new_weight_0_0 #값이 새롭게 바뀐다.(이런 과정으로 값이 바뀐다고 보면 된다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4033, 0.8380], requires_grad=True)\n",
      "tensor([-191.2390, -143.3532])\n"
     ]
    }
   ],
   "source": [
    "print(bias)\n",
    "print(bias.grad) #grad를 구하니, 값이 많이 바뀌었다.\n",
    "\n",
    "#앞으로 , grad값을 100분의 1을 한 것을 계속 bias에 더해서 값을 구해갈 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 최적화\n",
    "gradient decent 알고리즘을 이용해 loss를 줄여 모델의 추론 성능을 높인다. 이를 위해 좋은 성능을 낼 수 있도록 **경사하강법(gradient decent)** 을 이용해 weight와 bias를 update한다. \n",
    "\n",
    "1. 추론하기\n",
    "2. loss 계산하기\n",
    "3. weight와 bias에 대한 gradient계산하기\n",
    "4. 계산된 gradient에 비례한 값을 학습률을 곱해 작게 만든 뒤 wegith에서 빼서 조정한다.\n",
    "5. gradient를 0으로 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step(파라미터 업데이트 횟수)수 지정\n",
    "STEPS =100 #파라미터를 100번 업데이트 하겠다는 의미이다.\n",
    "\n",
    "#학습률(learning rate)\n",
    "LR = 0.0001 #1e-4 로 해도 된다.\n",
    "\n",
    "\n",
    "#Optimizer 생성\n",
    "optimizer = torch.optim.SGD([weights,bias],lr=LR) #경사하강법처리를 한다. gradient 값을 초기화한다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 - loss:35.308128356933594\n",
      "Step: 10 - loss:29.35744285583496\n",
      "Step: 20 - loss:24.427528381347656\n",
      "Step: 30 - loss:20.341524124145508\n",
      "Step: 40 - loss:16.954574584960938\n",
      "Step: 50 - loss:14.146974563598633\n",
      "Step: 60 - loss:11.819591522216797\n",
      "Step: 70 - loss:9.890228271484375\n",
      "Step: 80 - loss:8.290839195251465\n",
      "Step: 90 - loss:6.965029716491699\n",
      "Step: 99 - loss:5.966813087463379\n"
     ]
    }
   ],
   "source": [
    "for i in range(STEPS):\n",
    "    #추론\n",
    "    preds = model(inputs)\n",
    "    #오차 계산\n",
    "    loss = mse_loss_fn(preds,targets)\n",
    "    # gradient 계산\n",
    "    loss.backward()\n",
    "    #파라미터 (weights,bias)값들을 업데이트 한다.\n",
    "    optimizer.step()\n",
    "    #파라미터의 grad 값 초기화\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #10 steps 당 loss를 출력한다.\n",
    "    \n",
    "    if i % 10 ==0 or i == (STEPS-1):\n",
    "        print(f\"Step: {i} - loss:{loss.item()}\") #tensor 객체.item() : 스칼라나 원소가 1개인 tensor의 값을 추출한다.\n",
    "        \n",
    "#이 코드들을 보고 전체적으로 몇 번을 할 지 파악할 수 있다.    \n",
    "#오차가 줄어들 수록, loss 값이 감소하는 폭이 줄어든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 전 파라미터:\n",
      "tensor([[ 1.5410, -0.2934],\n",
      "        [-2.1788,  0.5684],\n",
      "        [-1.0845, -1.3986]], requires_grad=True)\n",
      "tensor([0.4033, 0.8380], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"학습 전 파라미터:\")\n",
    "print(weights)\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 후 파라미터:\n",
      "tensor([[-0.3720, -0.2423],\n",
      "        [ 0.8995,  0.9039],\n",
      "        [ 0.5544,  0.6273]], requires_grad=True)\n",
      "tensor([0.3984, 0.8511], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"학습 후 파라미터:\")\n",
    "print(weights)\n",
    "print(bias)\n",
    "\n",
    "#여러가지 값들이 바뀐 것을 볼 수 있다.\n",
    "#결과 강수량이 더 많은 영향을 주고 있다. 강수량이 각 과일에 주는 영향은 동일하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_value = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 57.3486,  70.7001],\n",
       "        [ 81.1847,  98.4942],\n",
       "        [120.7241, 137.2814],\n",
       "        [ 21.6454,  38.2148],\n",
       "        [ 99.8916, 114.8202]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch built-in 모델을 사용해 Linear Regression 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn #모델들을 제공하는 모듈이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Linear\n",
    "Pytorch는 nn.Linear 클래스를 통해 Linear Regression 모델을 제공한다.  \n",
    "nn.Linear에 입력 feature의 개수와 출력 값의 개수를 지정하면 random 값으로 초기화한 weight와 bias들을 생성해 모델을 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0146, -0.1694,  0.1242],\n",
      "        [-0.0215, -0.1172,  0.2497]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3907, -0.3694], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "##Linear(input feature 개수, output 개수)\n",
    "model = nn.Linear(3,2) \n",
    "\n",
    "\n",
    "## weight와 bias를 조회한다.  값은 랜덤으로 만든다. \n",
    "print(model.weight)\n",
    "print(model.bias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss 함수 정의\n",
    "\n",
    "#혹은 torch.nn.functional.mse_loss() 함수를 쓰기도 한다.\n",
    "loss_fn = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer 정의\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.0001) #경사하강법을 계산해 주는 것이다. lr은 학습률이다.\n",
    "#([최적화대상-모델의 파라미터])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -7.4607,   0.9457],\n",
       "        [ -8.6709,   3.3408],\n",
       "        [-17.1500,  -3.4612],\n",
       "        [ -4.5634,   1.6366],\n",
       "        [ -8.9600,   4.3742]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##학습전에 추론 --->오차 계산\n",
    "\n",
    "#추론\n",
    "pred = model(inputs)\n",
    "pred\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets #정답이다. 위의 pred의 출력값과 잘 비교해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9116.2744, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#오차가 났으니 오차 계산을 하자.\n",
    "\n",
    "loss = loss_fn(pred,targets) #(모델예측결과,정답)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer와 Loss 함수 정의\n",
    "- torch.optim 모듈에 다양한 Optimizer 클래스가 구현되있다. 그 중에서 Adam를 사용한다.\n",
    "- torch.nn 또는 torch.nn.functional 모듈에 다양한 Loss 함수가 제공된다. 이중 mse_loss() 를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Train\n",
    "주어진 epoch 만큼 학습하는 `fit`  함수를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(num_epochs: \"몇 번 반복할지\",model:\"학습시킬모델\",loss_fn:\"오차함수\",optim:\"옵티마이저\",inputs:\"입력데이터\",targets:\"출력데이터\"):\n",
    "    #친절하게 다 정의한다.\n",
    "    for epoch in range(num_epochs):\n",
    "        #지정한 횟수만큼 반복을 한다.\n",
    "        #1. 추론\n",
    "        pred = model(inputs)\n",
    "        #2. 오차 계산\n",
    "        loss = loss_fn(pred,targets)\n",
    "        \n",
    "        #3. gradient 계산\n",
    "        loss.backward()\n",
    "        \n",
    "        #4.파라미터 업데이트\n",
    "        optim.step()\n",
    "        \n",
    "        #5.계산된 gradient값을 초기화한다.\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        ####loss(결과) 출력\n",
    "        if epoch % 10 == 0 or epoch == (num_epochs-1):\n",
    "            print(f\"{epoch}/{num_epochs}:train loss: {loss.item():.5f}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/100:train loss: 3779.40942\n",
      "10/100:train loss: 151.17966\n",
      "20/100:train loss: 36.67197\n",
      "30/100:train loss: 13.09365\n",
      "40/100:train loss: 6.30169\n",
      "50/100:train loss: 4.05732\n",
      "60/100:train loss: 3.11103\n",
      "70/100:train loss: 2.57364\n",
      "80/100:train loss: 2.19489\n",
      "90/100:train loss: 1.89894\n",
      "99/100:train loss: 1.68045\n"
     ]
    }
   ],
   "source": [
    "#실제로 함수들을 적용해보자.\n",
    "#가면 갈수록 오차가 많이 줄어드는 것을 알 수 있다.\n",
    "fit(100,model,loss_fn,optimizer,inputs,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 57.1823,  70.3610],\n",
       "        [ 81.6451,  99.8332],\n",
       "        [119.9420, 134.7774],\n",
       "        [ 21.5037,  37.5874],\n",
       "        [100.6195, 117.2753]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#학습 후 모델 추론 결과를 확인\n",
    "\n",
    "p= model(inputs)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6584752798080444"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = loss_fn(p,targets)\n",
    "l.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
